2025-02-10 23:36:12 [INFO] Question: Which are the top 5 countries by sales?
2025-02-10 23:36:12 [INFO] Running PandaAI with bamboo_llm LLM...
2025-02-10 23:36:12 [INFO] Prompt ID: 55202f5d-173f-40bf-9715-776b3ceac28e
2025-02-10 23:36:12 [INFO] Generating new code...
2025-02-10 23:36:12 [INFO] Using Prompt: <tables>

<table table_name="table_3196ce98cf3ce3223735e1d61022e9d2" dimensions="10x2">
country,revenue
United States,5000
United Kingdom,3200
France,2900
Germany,4100
Italy,2300
</table>


</tables>

You are already provided with the following functions that you can call:
<function>
def execute_sql_query(sql_query: str) -> pd.Dataframe
    """This method connects to the database, executes the sql query and returns the dataframe"""
</function>


Update this initial code:
```python
# TODO: import the required dependencies
import pandas as pd

# Write code here

# Declare result var: 
type (possible values "string", "number", "dataframe", "plot"). Examples: { "type": "string", "value": f"The highest salary is {highest_salary}." } or { "type": "number", "value": 125 } or { "type": "dataframe", "value": pd.DataFrame({...}) } or { "type": "plot", "value": "temp_chart.png" }

```



### QUERY
 Which are the top 5 countries by sales?

At the end, declare "result" variable as a dictionary of type and value.


Generate python code and return full updated code:

### Note: Use only relevant table for query and do aggregation, sorting, joins and grouby through sql query
2025-02-10 23:36:13 [INFO] An error occurred during code generation: Invalid API key
2025-02-10 23:36:13 [INFO] Stack Trace:
Traceback (most recent call last):
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/pandasai/core/code_generation/base.py", line 34, in generate_code
    code = self._context.config.llm.generate_code(prompt, self._context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/pandasai/llm/base.py", line 172, in generate_code
    response = self.call(instruction, context)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/pandasai/llm/bamboo_llm/base.py", line 21, in call
    response = self._session.post(
               ^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/pandasai/helpers/session.py", line 47, in post
    return self.make_request("POST", path, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/pandasai/helpers/session.py", line 98, in make_request
    raise PandaAIApiCallError(data["detail"])
pandasai.exceptions.PandaAIApiCallError: Invalid API key

2025-02-10 23:37:29 [INFO] Question: Which are the top 5 countries by sales?
2025-02-10 23:37:29 [INFO] Running PandaAI with bamboo_llm LLM...
2025-02-10 23:37:29 [INFO] Prompt ID: b8fc8da4-5c83-4210-a30c-a6153c414e34
2025-02-10 23:37:29 [INFO] Generating new code...
2025-02-10 23:37:29 [INFO] Using Prompt: <tables>

<table table_name="table_3196ce98cf3ce3223735e1d61022e9d2" dimensions="10x2">
country,revenue
United States,5000
United Kingdom,3200
France,2900
Germany,4100
Italy,2300
</table>


</tables>

You are already provided with the following functions that you can call:
<function>
def execute_sql_query(sql_query: str) -> pd.Dataframe
    """This method connects to the database, executes the sql query and returns the dataframe"""
</function>


Update this initial code:
```python
# TODO: import the required dependencies
import pandas as pd

# Write code here

# Declare result var: 
type (possible values "string", "number", "dataframe", "plot"). Examples: { "type": "string", "value": f"The highest salary is {highest_salary}." } or { "type": "number", "value": 125 } or { "type": "dataframe", "value": pd.DataFrame({...}) } or { "type": "plot", "value": "temp_chart.png" }

```



### QUERY
 Which are the top 5 countries by sales?

At the end, declare "result" variable as a dictionary of type and value.


Generate python code and return full updated code:

### Note: Use only relevant table for query and do aggregation, sorting, joins and grouby through sql query
2025-02-10 23:37:29 [INFO] An error occurred during code generation: Invalid API key
2025-02-10 23:37:29 [INFO] Stack Trace:
Traceback (most recent call last):
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/pandasai/core/code_generation/base.py", line 34, in generate_code
    code = self._context.config.llm.generate_code(prompt, self._context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/pandasai/llm/base.py", line 172, in generate_code
    response = self.call(instruction, context)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/pandasai/llm/bamboo_llm/base.py", line 21, in call
    response = self._session.post(
               ^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/pandasai/helpers/session.py", line 47, in post
    return self.make_request("POST", path, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/pandasai/helpers/session.py", line 98, in make_request
    raise PandaAIApiCallError(data["detail"])
pandasai.exceptions.PandaAIApiCallError: Invalid API key

2025-02-10 23:37:45 [INFO] Question: Which are the top 5 countries by sales?
2025-02-10 23:37:45 [INFO] Running PandaAI with bamboo_llm LLM...
2025-02-10 23:37:45 [INFO] Prompt ID: 2e09e4f4-9b7d-4ea2-ba81-eada2c0eda6c
2025-02-10 23:37:45 [INFO] Generating new code...
2025-02-10 23:37:45 [INFO] Using Prompt: <tables>

<table table_name="table_3196ce98cf3ce3223735e1d61022e9d2" dimensions="10x2">
country,revenue
United States,5000
United Kingdom,3200
France,2900
Germany,4100
Italy,2300
</table>


</tables>

You are already provided with the following functions that you can call:
<function>
def execute_sql_query(sql_query: str) -> pd.Dataframe
    """This method connects to the database, executes the sql query and returns the dataframe"""
</function>


Update this initial code:
```python
# TODO: import the required dependencies
import pandas as pd

# Write code here

# Declare result var: 
type (possible values "string", "number", "dataframe", "plot"). Examples: { "type": "string", "value": f"The highest salary is {highest_salary}." } or { "type": "number", "value": 125 } or { "type": "dataframe", "value": pd.DataFrame({...}) } or { "type": "plot", "value": "temp_chart.png" }

```



### QUERY
 Which are the top 5 countries by sales?

At the end, declare "result" variable as a dictionary of type and value.


Generate python code and return full updated code:

### Note: Use only relevant table for query and do aggregation, sorting, joins and grouby through sql query
2025-02-10 23:37:45 [INFO] An error occurred during code generation: Invalid API key
2025-02-10 23:37:45 [INFO] Stack Trace:
Traceback (most recent call last):
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/pandasai/core/code_generation/base.py", line 34, in generate_code
    code = self._context.config.llm.generate_code(prompt, self._context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/pandasai/llm/base.py", line 172, in generate_code
    response = self.call(instruction, context)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/pandasai/llm/bamboo_llm/base.py", line 21, in call
    response = self._session.post(
               ^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/pandasai/helpers/session.py", line 47, in post
    return self.make_request("POST", path, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/pandasai/helpers/session.py", line 98, in make_request
    raise PandaAIApiCallError(data["detail"])
pandasai.exceptions.PandaAIApiCallError: Invalid API key

2025-02-10 23:40:16 [INFO] Question: Which are the top 5 countries by sales?
2025-02-10 23:40:16 [INFO] Running PandaAI with bamboo_llm LLM...
2025-02-10 23:40:16 [INFO] Prompt ID: fbb33352-b57e-4aeb-9cef-23bcfdb56ff4
2025-02-10 23:40:16 [INFO] Generating new code...
2025-02-10 23:40:16 [INFO] Using Prompt: <tables>

<table table_name="table_3196ce98cf3ce3223735e1d61022e9d2" dimensions="10x2">
country,revenue
United States,5000
United Kingdom,3200
France,2900
Germany,4100
Italy,2300
</table>


</tables>

You are already provided with the following functions that you can call:
<function>
def execute_sql_query(sql_query: str) -> pd.Dataframe
    """This method connects to the database, executes the sql query and returns the dataframe"""
</function>


Update this initial code:
```python
# TODO: import the required dependencies
import pandas as pd

# Write code here

# Declare result var: 
type (possible values "string", "number", "dataframe", "plot"). Examples: { "type": "string", "value": f"The highest salary is {highest_salary}." } or { "type": "number", "value": 125 } or { "type": "dataframe", "value": pd.DataFrame({...}) } or { "type": "plot", "value": "temp_chart.png" }

```



### QUERY
 Which are the top 5 countries by sales?

At the end, declare "result" variable as a dictionary of type and value.


Generate python code and return full updated code:

### Note: Use only relevant table for query and do aggregation, sorting, joins and grouby through sql query
2025-02-10 23:40:16 [INFO] An error occurred during code generation: Invalid API key
2025-02-10 23:40:16 [INFO] Stack Trace:
Traceback (most recent call last):
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/pandasai/core/code_generation/base.py", line 34, in generate_code
    code = self._context.config.llm.generate_code(prompt, self._context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/pandasai/llm/base.py", line 172, in generate_code
    response = self.call(instruction, context)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/pandasai/llm/bamboo_llm/base.py", line 21, in call
    response = self._session.post(
               ^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/pandasai/helpers/session.py", line 47, in post
    return self.make_request("POST", path, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/pandasai/helpers/session.py", line 98, in make_request
    raise PandaAIApiCallError(data["detail"])
pandasai.exceptions.PandaAIApiCallError: Invalid API key

2025-02-10 23:40:22 [INFO] Question: Which are the top 5 countries by sales?
2025-02-10 23:40:23 [INFO] Running PandaAI with bamboo_llm LLM...
2025-02-10 23:40:23 [INFO] Prompt ID: 3016f21d-6e43-4fe4-9ff7-e4ee5d36cabf
2025-02-10 23:40:23 [INFO] Generating new code...
2025-02-10 23:40:23 [INFO] Using Prompt: <tables>

<table table_name="table_3196ce98cf3ce3223735e1d61022e9d2" dimensions="10x2">
country,revenue
United States,5000
United Kingdom,3200
France,2900
Germany,4100
Italy,2300
</table>


</tables>

You are already provided with the following functions that you can call:
<function>
def execute_sql_query(sql_query: str) -> pd.Dataframe
    """This method connects to the database, executes the sql query and returns the dataframe"""
</function>


Update this initial code:
```python
# TODO: import the required dependencies
import pandas as pd

# Write code here

# Declare result var: 
type (possible values "string", "number", "dataframe", "plot"). Examples: { "type": "string", "value": f"The highest salary is {highest_salary}." } or { "type": "number", "value": 125 } or { "type": "dataframe", "value": pd.DataFrame({...}) } or { "type": "plot", "value": "temp_chart.png" }

```



### QUERY
 Which are the top 5 countries by sales?

At the end, declare "result" variable as a dictionary of type and value.


Generate python code and return full updated code:

### Note: Use only relevant table for query and do aggregation, sorting, joins and grouby through sql query
2025-02-10 23:40:36 [INFO] Code Generated:
# import the required dependencies
import pandas as pd

# function to execute sql query and return dataframe
def execute_sql_query(sql_query: str) -> pd.DataFrame:
    """This method connects to the database, executes the sql query and returns the dataframe"""

# sql query to get top 5 countries by sales
sql_query = "SELECT country, revenue FROM table_3196ce98cf3ce3223735e1d61022e9d2 ORDER BY revenue DESC LIMIT 5"

# execute sql query and get result dataframe
result_df = execute_sql_query(sql_query)

# Declare result var: 
result = { "type": "dataframe", "value": result_df }
2025-02-10 23:40:36 [INFO] Validating code requirements...
2025-02-10 23:40:36 [INFO] Code validation successful.
2025-02-10 23:40:36 [INFO] Cleaning the generated code...
2025-02-10 23:40:36 [INFO] Executing code: import pandas as pd
sql_query = 'SELECT country, revenue FROM table_3196ce98cf3ce3223735e1d61022e9d2 ORDER BY revenue DESC LIMIT 5'
result_df = execute_sql_query(sql_query)
result = {'type': 'dataframe', 'value': result_df}
2025-02-10 23:40:38 [INFO] Failed to extract font properties from /System/Library/Fonts/LastResort.otf: tuple indices must be integers or slices, not str
2025-02-10 23:40:38 [INFO] Failed to extract font properties from /System/Library/Fonts/Apple Color Emoji.ttc: In FT2Font: Could not set the fontsize (invalid pixel size; error code 0x17)
2025-02-10 23:40:38 [INFO] Failed to extract font properties from /System/Library/Fonts/Supplemental/NISC18030.ttf: In FT2Font: Could not set the fontsize (invalid pixel size; error code 0x17)
2025-02-10 23:40:38 [INFO] generated new fontManager
2025-02-10 23:40:38 [INFO] Response generated successfully.
2025-02-10 23:42:23 [INFO] Question: Which are the top 5 countries by sales?
2025-02-10 23:42:23 [INFO] Running PandaAI with bamboo_llm LLM...
2025-02-10 23:42:23 [INFO] Prompt ID: 1b73cb0e-9f31-4b1d-9f50-f2aab6fd96c6
2025-02-10 23:42:23 [INFO] Generating new code...
2025-02-10 23:42:23 [INFO] Using Prompt: <tables>

<table table_name="table_a2e389f9f9b55f3c974e75c41861bd80" dimensions="37x8">
bookingDate,valueDate,bookingDateTime,valueDateTime,transactionAmount,creditorName,remittanceInformationUnstructured,internalTransactionId
2025-01-02,2025-01-02,2025-01-02T00:00:00Z,2025-01-02T00:04:58.030Z,"{'amount': '-13.12', 'currency': 'GBP'}",DVLA,DVLA-LN61PYJ  000000000056504532 DDR,fc06b58dd668a7258adaa894be8d1a3f
2025-01-02,2025-01-01,2025-01-02T00:00:00Z,2025-01-01T09:50:31.130Z,"{'amount': '-60.00', 'currency': 'GBP'}",SUMUP *NEW LOOK BA,SUMUP *NEW LOOK BA  ON 31 DEC BCC,1317cc9763a3561a0d90de0546ed7bc6
2025-01-02,2024-12-31,2025-01-02T00:00:00Z,2024-12-31T21:41:46.030Z,"{'amount': '-50.00', 'currency': 'GBP'}",,SAINSBURYS BANK Sainsburys Bank 31DEC 21.41 ATM,716c32e620feb49b61d6ebbc4b59bbfd
2024-12-27,2024-12-27,2024-12-27T00:00:00Z,2024-12-27T09:27:39.140Z,"{'amount': '-15.59', 'currency': 'GBP'}",KLARNA*ZOOM COMMUN,KLARNA*ZOOM COMMUN  ON 26 DEC BCC,45821fc8a5f29ae9ddd255b39668df8f
2024-12-27,2024-12-25,2024-12-27T00:00:00Z,2024-12-25T10:02:13.070Z,"{'amount': '-50.00', 'currency': 'GBP'}",THE HUB DENTAL PRA,THE HUB DENTAL PRA  ON 24 DEC BCC,77877a31cbe2c17bceae0660c38e1839
</table>


</tables>

You are already provided with the following functions that you can call:
<function>
def execute_sql_query(sql_query: str) -> pd.Dataframe
    """This method connects to the database, executes the sql query and returns the dataframe"""
</function>


Update this initial code:
```python
# TODO: import the required dependencies
import pandas as pd

# Write code here

# Declare result var: 
type (possible values "string", "number", "dataframe", "plot"). Examples: { "type": "string", "value": f"The highest salary is {highest_salary}." } or { "type": "number", "value": 125 } or { "type": "dataframe", "value": pd.DataFrame({...}) } or { "type": "plot", "value": "temp_chart.png" }

```



### QUERY
 Which are the top 5 countries by sales?

At the end, declare "result" variable as a dictionary of type and value.


Generate python code and return full updated code:

### Note: Use only relevant table for query and do aggregation, sorting, joins and grouby through sql query
2025-02-10 23:42:31 [INFO] An error occurred during code generation: No code found in the response
2025-02-10 23:42:31 [INFO] Stack Trace:
Traceback (most recent call last):
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/pandasai/core/code_generation/base.py", line 34, in generate_code
    code = self._context.config.llm.generate_code(prompt, self._context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/pandasai/llm/base.py", line 173, in generate_code
    return self._extract_code(response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/pandasai/llm/base.py", line 118, in _extract_code
    raise NoCodeFoundError("No code found in the response")
pandasai.exceptions.NoCodeFoundError: No code found in the response

2025-02-10 23:51:58 [INFO] Question: What are the total expenses for January 2025?
2025-02-10 23:51:58 [INFO] Running PandaAI with bamboo_llm LLM...
2025-02-10 23:51:58 [INFO] Prompt ID: ca9a8d02-a878-467f-a251-61cf75e3778c
2025-02-10 23:51:58 [INFO] Generating new code...
2025-02-10 23:51:58 [INFO] Using Prompt: <tables>

<table table_name="table_0688ccf2ea6836f1bf70c9614bbbdf58" dimensions="37x9">
bookingDate,valueDate,bookingDateTime,valueDateTime,creditorName,remittanceInformationUnstructured,internalTransactionId,currency,amount
2025-01-02,2025-01-02,2025-01-02T00:00:00Z,2025-01-02T00:04:58.030Z,DVLA,DVLA-LN61PYJ  000000000056504532 DDR,fc06b58dd668a7258adaa894be8d1a3f,GBP,-1312
2025-01-02,2025-01-01,2025-01-02T00:00:00Z,2025-01-01T09:50:31.130Z,SUMUP *NEW LOOK BA,SUMUP *NEW LOOK BA  ON 31 DEC BCC,1317cc9763a3561a0d90de0546ed7bc6,GBP,-6000
2025-01-02,2024-12-31,2025-01-02T00:00:00Z,2024-12-31T21:41:46.030Z,,SAINSBURYS BANK Sainsburys Bank 31DEC 21.41 ATM,716c32e620feb49b61d6ebbc4b59bbfd,GBP,-5000
2024-12-27,2024-12-27,2024-12-27T00:00:00Z,2024-12-27T09:27:39.140Z,KLARNA*ZOOM COMMUN,KLARNA*ZOOM COMMUN  ON 26 DEC BCC,45821fc8a5f29ae9ddd255b39668df8f,GBP,-1559
2024-12-27,2024-12-25,2024-12-27T00:00:00Z,2024-12-25T10:02:13.070Z,THE HUB DENTAL PRA,THE HUB DENTAL PRA  ON 24 DEC BCC,77877a31cbe2c17bceae0660c38e1839,GBP,-5000
</table>


</tables>

You are already provided with the following functions that you can call:
<function>
def execute_sql_query(sql_query: str) -> pd.Dataframe
    """This method connects to the database, executes the sql query and returns the dataframe"""
</function>


Update this initial code:
```python
# TODO: import the required dependencies
import pandas as pd

# Write code here

# Declare result var: 
type (possible values "string", "number", "dataframe", "plot"). Examples: { "type": "string", "value": f"The highest salary is {highest_salary}." } or { "type": "number", "value": 125 } or { "type": "dataframe", "value": pd.DataFrame({...}) } or { "type": "plot", "value": "temp_chart.png" }

```



### QUERY
 What are the total expenses for January 2025?

At the end, declare "result" variable as a dictionary of type and value.


Generate python code and return full updated code:

### Note: Use only relevant table for query and do aggregation, sorting, joins and grouby through sql query
2025-02-10 23:52:06 [INFO] Code Generated:
# Required dependencies
import pandas as pd

# SQL query to get total expenses for January 2025
sql_query = """
            SELECT SUM(amount) AS total_expenses
            FROM table_0688ccf2ea6836f1bf70c9614bbbdf58
            WHERE bookingDate BETWEEN '2025-01-01' AND '2025-01-31'
            AND amount < 0
            """

# Execute the sql query
df = execute_sql_query(sql_query)

# Extract the total expenses value
total_expenses = df['total_expenses'][0]

# Declare result var: 
result = {
    "type": "number",
    "value": total_expenses
}

result
2025-02-10 23:52:06 [INFO] Validating code requirements...
2025-02-10 23:52:06 [INFO] Code validation successful.
2025-02-10 23:52:06 [INFO] Cleaning the generated code...
2025-02-10 23:52:06 [INFO] Executing code: import pandas as pd
sql_query = """
            SELECT SUM(amount) AS total_expenses
            FROM table_0688ccf2ea6836f1bf70c9614bbbdf58
            WHERE bookingDate BETWEEN '2025-01-01' AND '2025-01-31'
            AND amount < 0
            """
df = execute_sql_query(sql_query)
total_expenses = df['total_expenses'][0]
result = {'type': 'number', 'value': total_expenses}
result
2025-02-10 23:52:06 [INFO] Response generated successfully.
2025-02-10 23:56:38 [INFO] Question: What are the total expenses for 2024?
2025-02-10 23:56:38 [INFO] Running PandaAI with bamboo_llm LLM...
2025-02-10 23:56:38 [INFO] Prompt ID: 7bffd181-e79b-4197-af3d-220bbc42ab96
2025-02-10 23:56:38 [INFO] Generating new code...
2025-02-10 23:56:38 [INFO] Using Prompt: <tables>

<table table_name="table_0a0fe8cad351f24243e003033e37063e" dimensions="37x8">
valueDate,bookingDateTime,valueDateTime,creditorName,remittanceInformationUnstructured,internalTransactionId,currency,amount
2025-01-02,2025-01-02T00:00:00Z,2025-01-02T00:04:58.030Z,DVLA,DVLA-LN61PYJ  000000000056504532 DDR,fc06b58dd668a7258adaa894be8d1a3f,GBP,-1312
2025-01-01,2025-01-02T00:00:00Z,2025-01-01T09:50:31.130Z,SUMUP *NEW LOOK BA,SUMUP *NEW LOOK BA  ON 31 DEC BCC,1317cc9763a3561a0d90de0546ed7bc6,GBP,-6000
2024-12-31,2025-01-02T00:00:00Z,2024-12-31T21:41:46.030Z,,SAINSBURYS BANK Sainsburys Bank 31DEC 21.41 ATM,716c32e620feb49b61d6ebbc4b59bbfd,GBP,-5000
2024-12-27,2024-12-27T00:00:00Z,2024-12-27T09:27:39.140Z,KLARNA*ZOOM COMMUN,KLARNA*ZOOM COMMUN  ON 26 DEC BCC,45821fc8a5f29ae9ddd255b39668df8f,GBP,-1559
2024-12-25,2024-12-27T00:00:00Z,2024-12-25T10:02:13.070Z,THE HUB DENTAL PRA,THE HUB DENTAL PRA  ON 24 DEC BCC,77877a31cbe2c17bceae0660c38e1839,GBP,-5000
</table>


</tables>

You are already provided with the following functions that you can call:
<function>
def execute_sql_query(sql_query: str) -> pd.Dataframe
    """This method connects to the database, executes the sql query and returns the dataframe"""
</function>


Update this initial code:
```python
# TODO: import the required dependencies
import pandas as pd

# Write code here

# Declare result var: 
type (possible values "string", "number", "dataframe", "plot"). Examples: { "type": "string", "value": f"The highest salary is {highest_salary}." } or { "type": "number", "value": 125 } or { "type": "dataframe", "value": pd.DataFrame({...}) } or { "type": "plot", "value": "temp_chart.png" }

```



### QUERY
 What are the total expenses for 2024?

At the end, declare "result" variable as a dictionary of type and value.


Generate python code and return full updated code:

### Note: Use only relevant table for query and do aggregation, sorting, joins and grouby through sql query
2025-02-10 23:56:47 [INFO] Code Generated:
# import the required dependencies
import pandas as pd

def execute_sql_query(sql_query: str):
    """This method connects to the database, executes the sql query and returns the dataframe"""

# Write SQL query to get total expenses for 2024
sql_query = "SELECT SUM(amount) as total_expenses FROM table_0a0fe8cad351f24243e003033e37063e WHERE valueDate LIKE '2024%' AND amount < 0"

# Execute SQL query and get the result
expenses_df = execute_sql_query(sql_query)

# Get the total expenses from the result
total_expenses = expenses_df['total_expenses'][0]

# Declare result var: 
result = { "type": "number", "value": total_expenses }
2025-02-10 23:56:48 [INFO] Validating code requirements...
2025-02-10 23:56:48 [INFO] Code validation successful.
2025-02-10 23:56:48 [INFO] Cleaning the generated code...
2025-02-10 23:56:48 [INFO] Executing code: import pandas as pd
sql_query = "SELECT SUM(amount) as total_expenses FROM table_0a0fe8cad351f24243e003033e37063e WHERE valueDate LIKE '2024%' AND amount < 0"
expenses_df = execute_sql_query(sql_query)
total_expenses = expenses_df['total_expenses'][0]
result = {'type': 'number', 'value': total_expenses}
2025-02-10 23:56:48 [INFO] Response generated successfully.
2025-02-11 01:44:40 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-11 02:34:48 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-11 02:37:09 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-11 02:38:31 [INFO] Starting transaction classification process
2025-02-11 02:38:31 [INFO] Starting to process 1 transactions
2025-02-11 02:38:31 [INFO] Initializing TransactionClassifier
2025-02-11 02:38:31 [INFO] Processing transaction 1/1
2025-02-11 02:38:31 [INFO] Processing transaction: Office supplies purchase from Staples
2025-02-11 02:38:32 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-11 02:38:32 [WARNING] Direct JSON parsing failed, attempting to extract JSON from response
2025-02-11 02:38:32 [INFO] Successfully extracted and parsed JSON from response
2025-02-11 02:38:32 [INFO] Classification successful: 6410 - Office Supplies
2025-02-11 02:38:32 [INFO] Finished processing all transactions
2025-02-11 02:38:32 [INFO] Classification process completed
2025-02-11 02:38:32 [INFO] Classification Results:
2025-02-11 02:38:32 [INFO] Description: Office supplies purchase from Staples
2025-02-11 02:38:32 [INFO] Account Code: 6410 - Office Supplies
2025-02-11 02:38:32 [INFO] Reason: This transaction is classified as an office supplies purchase because it is a routine restock of supplies from a known office supply vendor, Staples. The description and context both support this classification, indicating a typical expense for a business.
2025-02-11 02:38:32 [INFO] Confidence: 0.98
2025-02-11 02:38:32 [INFO] Starting to process 1 transactions
2025-02-11 02:38:32 [INFO] Initializing TransactionClassifier
2025-02-11 02:38:32 [INFO] Processing transaction 1/1
2025-02-11 02:38:32 [INFO] Processing transaction: Office supplies purchase from Staples
2025-02-11 02:38:32 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-11 02:38:32 [WARNING] Direct JSON parsing failed, attempting to extract JSON from response
2025-02-11 02:38:32 [INFO] Successfully extracted and parsed JSON from response
2025-02-11 02:38:32 [INFO] Classification successful: 6410-Office Supplies
2025-02-11 02:38:32 [INFO] Finished processing all transactions
2025-02-11 03:28:16 [INFO] Starting to process 37 transactions
2025-02-11 03:28:16 [INFO] Initializing TransactionClassifier
2025-02-11 03:28:16 [INFO] Processing transaction 1/37
2025-02-11 03:29:08 [INFO] Starting to process 37 transactions
2025-02-11 03:29:08 [INFO] Initializing TransactionClassifier
2025-02-11 03:29:08 [INFO] Processing transaction 1/37
2025-02-11 03:30:13 [INFO] Starting to process 37 transactions
2025-02-11 03:30:13 [INFO] Initializing TransactionClassifier
2025-02-11 03:30:13 [INFO] Processing transaction 1/37
2025-02-11 03:30:13 [INFO] Processing transaction: {'creditorName': 'DVLA', 'remittanceInformationUnstructured': 'DVLA-LN61PYJ  000000000056504532 DDR', 'currency': 'GBP', 'amount': -1312}
2025-02-11 03:30:13 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:30:13 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6491, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6491, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:30:13 [INFO] Processing transaction 2/37
2025-02-11 03:30:13 [INFO] Processing transaction: {'creditorName': 'SUMUP *NEW LOOK BA', 'remittanceInformationUnstructured': 'SUMUP *NEW LOOK BA  ON 31 DEC BCC', 'currency': 'GBP', 'amount': -6000}
2025-02-11 03:30:15 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:30:15 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6494, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6494, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:30:15 [INFO] Processing transaction 3/37
2025-02-11 03:30:15 [INFO] Processing transaction: {'creditorName': nan, 'remittanceInformationUnstructured': 'SAINSBURYS BANK Sainsburys Bank 31DEC 21.41 ATM', 'currency': 'GBP', 'amount': -5000}
2025-02-11 03:30:17 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:30:17 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6493, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6493, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:30:17 [INFO] Processing transaction 4/37
2025-02-11 03:30:17 [INFO] Processing transaction: {'creditorName': 'KLARNA*ZOOM COMMUN', 'remittanceInformationUnstructured': 'KLARNA*ZOOM COMMUN  ON 26 DEC BCC', 'currency': 'GBP', 'amount': -1559}
2025-02-11 03:30:19 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:30:19 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6494, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6494, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:30:19 [INFO] Processing transaction 5/37
2025-02-11 03:30:19 [INFO] Processing transaction: {'creditorName': 'THE HUB DENTAL PRA', 'remittanceInformationUnstructured': 'THE HUB DENTAL PRA  ON 24 DEC BCC', 'currency': 'GBP', 'amount': -5000}
2025-02-11 03:30:21 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:30:21 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6494, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6494, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:30:21 [INFO] Processing transaction 6/37
2025-02-11 03:30:21 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB166643916980508', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB166643916980508 FT', 'currency': 'GBP', 'amount': -599}
2025-02-11 03:30:23 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:30:23 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6499, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6499, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:30:23 [INFO] Processing transaction 7/37
2025-02-11 03:30:23 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB313981532278988', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB313981532278988 FT', 'currency': 'GBP', 'amount': -21320}
2025-02-11 03:30:25 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:30:25 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6500, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6500, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:30:25 [INFO] Processing transaction 8/37
2025-02-11 03:30:25 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -30000}
2025-02-11 03:30:27 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:30:27 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6499, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6499, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:30:27 [INFO] Processing transaction 9/37
2025-02-11 03:30:27 [INFO] Processing transaction: {'creditorName': 'Barclaycard', 'remittanceInformationUnstructured': 'MR MICHAEL ALI  4929158426786006 BBP', 'currency': 'GBP', 'amount': -14538}
2025-02-11 03:30:29 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:30:29 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6493, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6493, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:30:29 [INFO] Processing transaction 10/37
2025-02-11 03:30:29 [INFO] Processing transaction: {'creditorName': 'AA', 'remittanceInformationUnstructured': 'AA MEMBERSHIP  6356011528897956 DDR', 'currency': 'GBP', 'amount': -1641}
2025-02-11 03:30:31 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:30:31 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6490, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6490, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:30:31 [INFO] Processing transaction 11/37
2025-02-11 03:30:31 [INFO] Processing transaction: {'creditorName': 'FIRECRAWL.DEV USA AMOUNT IN USD 19.00', 'remittanceInformationUnstructured': 'FIRECRAWL.DEV USA AMOUNT IN USD 19.00 ON 16 DEC VISA 1.2605 FINAL GBP AMOUNT INCLUDES NON-STERLING TRANS FEE £0.45 ', 'currency': 'GBP', 'amount': -1552}
2025-02-11 03:30:33 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:30:33 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:30:33 [INFO] Processing transaction 12/37
2025-02-11 03:30:33 [INFO] Processing transaction: {'creditorName': 'Admiral', 'remittanceInformationUnstructured': 'ADMIRAL INSURANCE  P73244643010000011 DDR', 'currency': 'GBP', 'amount': -11313}
2025-02-11 03:30:35 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:30:35 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6493, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6493, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:30:35 [INFO] Processing transaction 13/37
2025-02-11 03:30:35 [INFO] Processing transaction: {'creditorName': 'HORIZONPARKING.CO.', 'remittanceInformationUnstructured': 'HORIZONPARKING.CO.  ON 13 DEC BCC', 'currency': 'GBP', 'amount': -6000}
2025-02-11 03:30:37 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:30:37 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6494, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6494, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:30:37 [INFO] Processing transaction 14/37
2025-02-11 03:30:37 [INFO] Processing transaction: {'creditorName': 'Vodafone', 'remittanceInformationUnstructured': 'VODAFONE LTD  7082364969-1001 DDR', 'currency': 'GBP', 'amount': -1091}
2025-02-11 03:30:39 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:30:39 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6491, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6491, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:30:39 [INFO] Processing transaction 15/37
2025-02-11 03:30:39 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -20000}
2025-02-11 03:30:41 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:30:41 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6499, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6499, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:30:41 [INFO] Processing transaction 16/37
2025-02-11 03:30:41 [INFO] Processing transaction: {'creditorName': 'Barclaycard', 'remittanceInformationUnstructured': 'MR MICHAEL ALI  4929158426786006 BBP', 'currency': 'GBP', 'amount': -14538}
2025-02-11 03:30:43 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:30:43 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6493, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6493, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:30:43 [INFO] Processing transaction 17/37
2025-02-11 03:30:43 [INFO] Processing transaction: {'creditorName': 'DVLA', 'remittanceInformationUnstructured': 'DVLA-LN61PYJ  000000000056504532 DDR', 'currency': 'GBP', 'amount': -1312}
2025-02-11 03:30:45 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:30:45 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6491, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6491, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:30:45 [INFO] Processing transaction 18/37
2025-02-11 03:30:45 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB836806900164200', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB836806900164200 FT', 'currency': 'GBP', 'amount': -3515}
2025-02-11 03:30:47 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:30:47 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6500, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6500, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:30:47 [INFO] Processing transaction 19/37
2025-02-11 03:30:47 [INFO] Processing transaction: {'creditorName': 'Barclaycard', 'remittanceInformationUnstructured': 'MR MICHAEL ALI  4929158426786006 BBP', 'currency': 'GBP', 'amount': -14538}
2025-02-11 03:30:49 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:30:49 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6493, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6493, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:30:49 [INFO] Processing transaction 20/37
2025-02-11 03:30:49 [INFO] Processing transaction: {'creditorName': 'HELEN MELON  WAFFLE TIME', 'remittanceInformationUnstructured': 'HELEN MELON  WAFFLE TIME FT', 'currency': 'GBP', 'amount': -1000}
2025-02-11 03:30:51 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:30:51 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6494, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6494, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:30:51 [INFO] Processing transaction 21/37
2025-02-11 03:30:51 [INFO] Processing transaction: {'creditorName': 'EURO CAR PARKS LTD', 'remittanceInformationUnstructured': 'EURO CAR PARKS LTD  ON 19 NOV BCC', 'currency': 'GBP', 'amount': -6000}
2025-02-11 03:30:53 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:30:53 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6494, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6494, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:30:53 [INFO] Processing transaction 22/37
2025-02-11 03:30:53 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -30000}
2025-02-11 03:30:55 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:30:55 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6499, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6499, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:30:55 [INFO] Processing transaction 23/37
2025-02-11 03:30:55 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB791748851198619', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB791748851198619 FT', 'currency': 'GBP', 'amount': -31410}
2025-02-11 03:30:57 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:30:57 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6500, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6500, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:30:57 [INFO] Processing transaction 24/37
2025-02-11 03:30:57 [INFO] Processing transaction: {'creditorName': 'Admiral', 'remittanceInformationUnstructured': 'ADMIRAL INSURANCE  P73244643010000010 DDR', 'currency': 'GBP', 'amount': -11313}
2025-02-11 03:30:59 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:30:59 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6493, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6493, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:30:59 [INFO] Processing transaction 25/37
2025-02-11 03:30:59 [INFO] Processing transaction: {'creditorName': 'AA', 'remittanceInformationUnstructured': 'AA MEMBERSHIP  6356011528897956 DDR', 'currency': 'GBP', 'amount': -1641}
2025-02-11 03:31:01 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:31:01 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6490, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6490, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:31:01 [INFO] Processing transaction 26/37
2025-02-11 03:31:01 [INFO] Processing transaction: {'creditorName': 'FIRECRAWL.DEV USA AMOUNT IN USD 19.00', 'remittanceInformationUnstructured': 'FIRECRAWL.DEV USA AMOUNT IN USD 19.00 ON 16 NOV VISA 1.2594 FINAL GBP AMOUNT INCLUDES NON-STERLING TRANS FEE £0.45 ', 'currency': 'GBP', 'amount': -1554}
2025-02-11 03:31:03 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:31:03 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:31:03 [INFO] Processing transaction 27/37
2025-02-11 03:31:03 [INFO] Processing transaction: {'creditorName': nan, 'remittanceInformationUnstructured': 'NOTEMACHINE Notemachine 16NOV 13.49 ATM', 'currency': 'GBP', 'amount': -6000}
2025-02-11 03:31:05 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:31:05 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6491, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6491, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:31:05 [INFO] Processing transaction 28/37
2025-02-11 03:31:05 [INFO] Processing transaction: {'creditorName': 'Barclaycard', 'remittanceInformationUnstructured': 'MR MICHAEL ALI  4929158426786006 BBP', 'currency': 'GBP', 'amount': -18002}
2025-02-11 03:31:07 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:31:07 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6493, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6493, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:31:07 [INFO] Processing transaction 29/37
2025-02-11 03:31:07 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -50000}
2025-02-11 03:31:09 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:31:09 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6499, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6499, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:31:09 [INFO] Processing transaction 30/37
2025-02-11 03:31:09 [INFO] Processing transaction: {'creditorName': 'Vodafone', 'remittanceInformationUnstructured': 'VODAFONE LTD  7082364969-1001 DDR', 'currency': 'GBP', 'amount': -1163}
2025-02-11 03:31:11 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:31:11 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6491, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6491, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:31:11 [INFO] Processing transaction 31/37
2025-02-11 03:31:11 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -30000}
2025-02-11 03:31:13 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:31:13 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6499, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6499, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:31:13 [INFO] Processing transaction 32/37
2025-02-11 03:31:13 [INFO] Processing transaction: {'creditorName': 'DVLA', 'remittanceInformationUnstructured': 'DVLA-LN61PYJ  000000000056504532 DDR', 'currency': 'GBP', 'amount': -1312}
2025-02-11 03:31:15 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:31:15 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6491, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6491, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:31:15 [INFO] Processing transaction 33/37
2025-02-11 03:31:15 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB847450840043831', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB847450840043831 FT', 'currency': 'GBP', 'amount': -599}
2025-02-11 03:31:17 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:31:17 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6499, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6499, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:31:17 [INFO] Processing transaction 34/37
2025-02-11 03:31:17 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB170732208294537', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB170732208294537 FT', 'currency': 'GBP', 'amount': -44918}
2025-02-11 03:31:19 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:31:19 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6500, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6500, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:31:19 [INFO] Processing transaction 35/37
2025-02-11 03:31:19 [INFO] Processing transaction: {'creditorName': 'AA', 'remittanceInformationUnstructured': 'AA MEMBERSHIP  6356011528897956 DDR', 'currency': 'GBP', 'amount': -1641}
2025-02-11 03:31:21 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:31:21 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6490, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6490, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:31:21 [INFO] Processing transaction 36/37
2025-02-11 03:31:21 [INFO] Processing transaction: {'creditorName': 'Admiral', 'remittanceInformationUnstructured': 'ADMIRAL INSURANCE  P73244643010000009 DDR', 'currency': 'GBP', 'amount': -11313}
2025-02-11 03:31:23 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:31:23 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6493, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6493, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:31:23 [INFO] Processing transaction 37/37
2025-02-11 03:31:23 [INFO] Processing transaction: {'creditorName': 'Vodafone', 'remittanceInformationUnstructured': 'VODAFONE LTD  7082364969-1001 DDR', 'currency': 'GBP', 'amount': -1295}
2025-02-11 03:31:25 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:31:25 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6491, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6491, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:31:25 [INFO] Finished processing all transactions
2025-02-11 03:31:25 [INFO] Starting transaction classification process
2025-02-11 03:31:25 [ERROR] Main process failed
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/591466325.py", line 184, in <module>
    classified_transactions = process_transactions(transactions)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: process_transactions() missing 1 required positional argument: 'chart_of_accounts'
2025-02-11 03:45:05 [INFO] Starting to process 37 transactions
2025-02-11 03:45:05 [INFO] Initializing TransactionClassifier
2025-02-11 03:45:05 [INFO] Processing transaction 2025-01-02 00:00:00
2025-02-11 03:45:05 [INFO] Processing transaction: {'creditorName': 'DVLA', 'remittanceInformationUnstructured': 'DVLA-LN61PYJ  000000000056504532 DDR', 'currency': 'GBP', 'amount': -1312, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:05 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:05 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:05 [INFO] Processing transaction 2025-01-02 00:00:00
2025-02-11 03:45:05 [INFO] Processing transaction: {'creditorName': 'SUMUP *NEW LOOK BA', 'remittanceInformationUnstructured': 'SUMUP *NEW LOOK BA  ON 31 DEC BCC', 'currency': 'GBP', 'amount': -6000, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:07 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:07 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:07 [INFO] Processing transaction 2025-01-02 00:00:00
2025-02-11 03:45:07 [INFO] Processing transaction: {'creditorName': nan, 'remittanceInformationUnstructured': 'SAINSBURYS BANK Sainsburys Bank 31DEC 21.41 ATM', 'currency': 'GBP', 'amount': -5000, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:09 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:09 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6518, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6518, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:09 [INFO] Processing transaction 2024-12-27 00:00:00
2025-02-11 03:45:09 [INFO] Processing transaction: {'creditorName': 'KLARNA*ZOOM COMMUN', 'remittanceInformationUnstructured': 'KLARNA*ZOOM COMMUN  ON 26 DEC BCC', 'currency': 'GBP', 'amount': -1559, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:11 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:11 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:11 [INFO] Processing transaction 2024-12-27 00:00:00
2025-02-11 03:45:11 [INFO] Processing transaction: {'creditorName': 'THE HUB DENTAL PRA', 'remittanceInformationUnstructured': 'THE HUB DENTAL PRA  ON 24 DEC BCC', 'currency': 'GBP', 'amount': -5000, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:13 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:13 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:13 [INFO] Processing transaction 2024-12-24 00:00:00
2025-02-11 03:45:13 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB166643916980508', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB166643916980508 FT', 'currency': 'GBP', 'amount': -599, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:15 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:15 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6525, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6525, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:15 [INFO] Processing transaction 2024-12-24 00:00:00
2025-02-11 03:45:15 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB313981532278988', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB313981532278988 FT', 'currency': 'GBP', 'amount': -21320, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:17 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:17 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6525, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6525, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:17 [INFO] Processing transaction 2024-12-24 00:00:00
2025-02-11 03:45:17 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -30000, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:19 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:19 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6524, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6524, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:19 [INFO] Processing transaction 2024-12-24 00:00:00
2025-02-11 03:45:19 [INFO] Processing transaction: {'creditorName': 'Barclaycard', 'remittanceInformationUnstructured': 'MR MICHAEL ALI  4929158426786006 BBP', 'currency': 'GBP', 'amount': -14538, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:21 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:21 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6518, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6518, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:21 [INFO] Processing transaction 2024-12-18 00:00:00
2025-02-11 03:45:21 [INFO] Processing transaction: {'creditorName': 'AA', 'remittanceInformationUnstructured': 'AA MEMBERSHIP  6356011528897956 DDR', 'currency': 'GBP', 'amount': -1641, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:23 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:23 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:23 [INFO] Processing transaction 2024-12-17 00:00:00
2025-02-11 03:45:23 [INFO] Processing transaction: {'creditorName': 'FIRECRAWL.DEV USA AMOUNT IN USD 19.00', 'remittanceInformationUnstructured': 'FIRECRAWL.DEV USA AMOUNT IN USD 19.00 ON 16 DEC VISA 1.2605 FINAL GBP AMOUNT INCLUDES NON-STERLING TRANS FEE £0.45 ', 'currency': 'GBP', 'amount': -1552, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:25 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:25 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6545, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6545, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:25 [INFO] Processing transaction 2024-12-17 00:00:00
2025-02-11 03:45:25 [INFO] Processing transaction: {'creditorName': 'Admiral', 'remittanceInformationUnstructured': 'ADMIRAL INSURANCE  P73244643010000011 DDR', 'currency': 'GBP', 'amount': -11313, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:27 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:27 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:27 [INFO] Processing transaction 2024-12-16 00:00:00
2025-02-11 03:45:27 [INFO] Processing transaction: {'creditorName': 'HORIZONPARKING.CO.', 'remittanceInformationUnstructured': 'HORIZONPARKING.CO.  ON 13 DEC BCC', 'currency': 'GBP', 'amount': -6000, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:29 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:29 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:29 [INFO] Processing transaction 2024-12-10 00:00:00
2025-02-11 03:45:29 [INFO] Processing transaction: {'creditorName': 'Vodafone', 'remittanceInformationUnstructured': 'VODAFONE LTD  7082364969-1001 DDR', 'currency': 'GBP', 'amount': -1091, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:31 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:31 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6517, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6517, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:31 [INFO] Processing transaction 2024-12-04 00:00:00
2025-02-11 03:45:31 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -20000, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:33 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:33 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6524, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6524, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:33 [INFO] Processing transaction 2024-12-03 00:00:00
2025-02-11 03:45:33 [INFO] Processing transaction: {'creditorName': 'Barclaycard', 'remittanceInformationUnstructured': 'MR MICHAEL ALI  4929158426786006 BBP', 'currency': 'GBP', 'amount': -14538, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:35 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:35 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6518, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6518, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:35 [INFO] Processing transaction 2024-12-02 00:00:00
2025-02-11 03:45:35 [INFO] Processing transaction: {'creditorName': 'DVLA', 'remittanceInformationUnstructured': 'DVLA-LN61PYJ  000000000056504532 DDR', 'currency': 'GBP', 'amount': -1312, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:37 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:37 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:37 [INFO] Processing transaction 2024-11-28 00:00:00
2025-02-11 03:45:37 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB836806900164200', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB836806900164200 FT', 'currency': 'GBP', 'amount': -3515, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:39 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:39 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6525, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6525, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:39 [INFO] Processing transaction 2024-11-28 00:00:00
2025-02-11 03:45:39 [INFO] Processing transaction: {'creditorName': 'Barclaycard', 'remittanceInformationUnstructured': 'MR MICHAEL ALI  4929158426786006 BBP', 'currency': 'GBP', 'amount': -14538, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:41 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:41 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6518, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6518, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:41 [INFO] Processing transaction 2024-11-26 00:00:00
2025-02-11 03:45:41 [INFO] Processing transaction: {'creditorName': 'HELEN MELON  WAFFLE TIME', 'remittanceInformationUnstructured': 'HELEN MELON  WAFFLE TIME FT', 'currency': 'GBP', 'amount': -1000, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:43 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:43 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:43 [INFO] Processing transaction 2024-11-20 00:00:00
2025-02-11 03:45:43 [INFO] Processing transaction: {'creditorName': 'EURO CAR PARKS LTD', 'remittanceInformationUnstructured': 'EURO CAR PARKS LTD  ON 19 NOV BCC', 'currency': 'GBP', 'amount': -6000, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:45 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:45 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:45 [INFO] Processing transaction 2024-11-20 00:00:00
2025-02-11 03:45:45 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -30000, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:47 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:47 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6524, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6524, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:47 [INFO] Processing transaction 2024-11-19 00:00:00
2025-02-11 03:45:47 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB791748851198619', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB791748851198619 FT', 'currency': 'GBP', 'amount': -31410, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:49 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:49 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6525, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6525, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:49 [INFO] Processing transaction 2024-11-18 00:00:00
2025-02-11 03:45:49 [INFO] Processing transaction: {'creditorName': 'Admiral', 'remittanceInformationUnstructured': 'ADMIRAL INSURANCE  P73244643010000010 DDR', 'currency': 'GBP', 'amount': -11313, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:51 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:51 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:51 [INFO] Processing transaction 2024-11-18 00:00:00
2025-02-11 03:45:51 [INFO] Processing transaction: {'creditorName': 'AA', 'remittanceInformationUnstructured': 'AA MEMBERSHIP  6356011528897956 DDR', 'currency': 'GBP', 'amount': -1641, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:53 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:53 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:53 [INFO] Processing transaction 2024-11-18 00:00:00
2025-02-11 03:45:53 [INFO] Processing transaction: {'creditorName': 'FIRECRAWL.DEV USA AMOUNT IN USD 19.00', 'remittanceInformationUnstructured': 'FIRECRAWL.DEV USA AMOUNT IN USD 19.00 ON 16 NOV VISA 1.2594 FINAL GBP AMOUNT INCLUDES NON-STERLING TRANS FEE £0.45 ', 'currency': 'GBP', 'amount': -1554, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:55 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:55 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6545, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6545, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:55 [INFO] Processing transaction 2024-11-18 00:00:00
2025-02-11 03:45:55 [INFO] Processing transaction: {'creditorName': nan, 'remittanceInformationUnstructured': 'NOTEMACHINE Notemachine 16NOV 13.49 ATM', 'currency': 'GBP', 'amount': -6000, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:57 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:57 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:57 [INFO] Processing transaction 2024-11-08 00:00:00
2025-02-11 03:45:57 [INFO] Processing transaction: {'creditorName': 'Barclaycard', 'remittanceInformationUnstructured': 'MR MICHAEL ALI  4929158426786006 BBP', 'currency': 'GBP', 'amount': -18002, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:45:59 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:45:59 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6518, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6518, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:45:59 [INFO] Processing transaction 2024-11-08 00:00:00
2025-02-11 03:45:59 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -50000, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:46:01 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:46:01 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6524, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6524, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:46:01 [INFO] Processing transaction 2024-11-07 00:00:00
2025-02-11 03:46:01 [INFO] Processing transaction: {'creditorName': 'Vodafone', 'remittanceInformationUnstructured': 'VODAFONE LTD  7082364969-1001 DDR', 'currency': 'GBP', 'amount': -1163, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:46:03 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:46:03 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6517, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6517, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:46:03 [INFO] Processing transaction 2024-11-04 00:00:00
2025-02-11 03:46:03 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -30000, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:46:05 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:46:05 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6524, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6524, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:46:05 [INFO] Processing transaction 2024-11-01 00:00:00
2025-02-11 03:46:05 [INFO] Processing transaction: {'creditorName': 'DVLA', 'remittanceInformationUnstructured': 'DVLA-LN61PYJ  000000000056504532 DDR', 'currency': 'GBP', 'amount': -1312, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:46:07 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:46:07 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:46:07 [INFO] Processing transaction 2024-10-21 00:00:00
2025-02-11 03:46:07 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB847450840043831', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB847450840043831 FT', 'currency': 'GBP', 'amount': -599, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:46:09 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:46:09 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6525, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6525, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:46:09 [INFO] Processing transaction 2024-10-21 00:00:00
2025-02-11 03:46:09 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB170732208294537', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB170732208294537 FT', 'currency': 'GBP', 'amount': -44918, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:46:11 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:46:11 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6525, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6525, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:46:11 [INFO] Processing transaction 2024-10-18 00:00:00
2025-02-11 03:46:11 [INFO] Processing transaction: {'creditorName': 'AA', 'remittanceInformationUnstructured': 'AA MEMBERSHIP  6356011528897956 DDR', 'currency': 'GBP', 'amount': -1641, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:46:13 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:46:13 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:46:13 [INFO] Processing transaction 2024-10-17 00:00:00
2025-02-11 03:46:13 [INFO] Processing transaction: {'creditorName': 'Admiral', 'remittanceInformationUnstructured': 'ADMIRAL INSURANCE  P73244643010000009 DDR', 'currency': 'GBP', 'amount': -11313, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:46:15 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:46:15 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:46:15 [INFO] Processing transaction 2024-10-09 00:00:00
2025-02-11 03:46:15 [INFO] Processing transaction: {'creditorName': 'Vodafone', 'remittanceInformationUnstructured': 'VODAFONE LTD  7082364969-1001 DDR', 'currency': 'GBP', 'amount': -1295, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:46:17 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:46:17 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6517, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3606638357.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6517, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:46:17 [INFO] Finished processing all transactions
2025-02-11 03:57:40 [INFO] Starting to process 37 transactions
2025-02-11 03:57:40 [INFO] Initializing TransactionClassifier
2025-02-11 03:57:40 [INFO] Processing transaction 2025-01-02 00:00:00
2025-02-11 03:57:40 [INFO] Processing transaction: {'creditorName': 'DVLA', 'remittanceInformationUnstructured': 'DVLA-LN61PYJ  000000000056504532 DDR', 'currency': 'GBP', 'amount': -1312, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:57:40 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:57:40 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:57:40 [INFO] Processing transaction 2025-01-02 00:00:00
2025-02-11 03:57:40 [INFO] Processing transaction: {'creditorName': 'SUMUP *NEW LOOK BA', 'remittanceInformationUnstructured': 'SUMUP *NEW LOOK BA  ON 31 DEC BCC', 'currency': 'GBP', 'amount': -6000, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:57:42 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:57:42 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:57:42 [INFO] Processing transaction 2025-01-02 00:00:00
2025-02-11 03:57:42 [INFO] Processing transaction: {'creditorName': nan, 'remittanceInformationUnstructured': 'SAINSBURYS BANK Sainsburys Bank 31DEC 21.41 ATM', 'currency': 'GBP', 'amount': -5000, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:57:44 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:57:44 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6518, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6518, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:57:44 [INFO] Processing transaction 2024-12-27 00:00:00
2025-02-11 03:57:44 [INFO] Processing transaction: {'creditorName': 'KLARNA*ZOOM COMMUN', 'remittanceInformationUnstructured': 'KLARNA*ZOOM COMMUN  ON 26 DEC BCC', 'currency': 'GBP', 'amount': -1559, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:57:46 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:57:46 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:57:46 [INFO] Processing transaction 2024-12-27 00:00:00
2025-02-11 03:57:46 [INFO] Processing transaction: {'creditorName': 'THE HUB DENTAL PRA', 'remittanceInformationUnstructured': 'THE HUB DENTAL PRA  ON 24 DEC BCC', 'currency': 'GBP', 'amount': -5000, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:57:48 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:57:48 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:57:48 [INFO] Processing transaction 2024-12-24 00:00:00
2025-02-11 03:57:48 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB166643916980508', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB166643916980508 FT', 'currency': 'GBP', 'amount': -599, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:57:50 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:57:50 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6525, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6525, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:57:50 [INFO] Processing transaction 2024-12-24 00:00:00
2025-02-11 03:57:50 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB313981532278988', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB313981532278988 FT', 'currency': 'GBP', 'amount': -21320, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:57:52 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:57:52 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6525, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6525, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:57:52 [INFO] Processing transaction 2024-12-24 00:00:00
2025-02-11 03:57:52 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -30000, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:57:54 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:57:54 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6524, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6524, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:57:54 [INFO] Processing transaction 2024-12-24 00:00:00
2025-02-11 03:57:54 [INFO] Processing transaction: {'creditorName': 'Barclaycard', 'remittanceInformationUnstructured': 'MR MICHAEL ALI  4929158426786006 BBP', 'currency': 'GBP', 'amount': -14538, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:57:56 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:57:56 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6518, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6518, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:57:56 [INFO] Processing transaction 2024-12-18 00:00:00
2025-02-11 03:57:56 [INFO] Processing transaction: {'creditorName': 'AA', 'remittanceInformationUnstructured': 'AA MEMBERSHIP  6356011528897956 DDR', 'currency': 'GBP', 'amount': -1641, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:57:58 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:57:58 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:57:58 [INFO] Processing transaction 2024-12-17 00:00:00
2025-02-11 03:57:58 [INFO] Processing transaction: {'creditorName': 'FIRECRAWL.DEV USA AMOUNT IN USD 19.00', 'remittanceInformationUnstructured': 'FIRECRAWL.DEV USA AMOUNT IN USD 19.00 ON 16 DEC VISA 1.2605 FINAL GBP AMOUNT INCLUDES NON-STERLING TRANS FEE £0.45 ', 'currency': 'GBP', 'amount': -1552, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:00 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:00 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6545, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6545, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:00 [INFO] Processing transaction 2024-12-17 00:00:00
2025-02-11 03:58:00 [INFO] Processing transaction: {'creditorName': 'Admiral', 'remittanceInformationUnstructured': 'ADMIRAL INSURANCE  P73244643010000011 DDR', 'currency': 'GBP', 'amount': -11313, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:02 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:02 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:02 [INFO] Processing transaction 2024-12-16 00:00:00
2025-02-11 03:58:02 [INFO] Processing transaction: {'creditorName': 'HORIZONPARKING.CO.', 'remittanceInformationUnstructured': 'HORIZONPARKING.CO.  ON 13 DEC BCC', 'currency': 'GBP', 'amount': -6000, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:04 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:04 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:04 [INFO] Processing transaction 2024-12-10 00:00:00
2025-02-11 03:58:04 [INFO] Processing transaction: {'creditorName': 'Vodafone', 'remittanceInformationUnstructured': 'VODAFONE LTD  7082364969-1001 DDR', 'currency': 'GBP', 'amount': -1091, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:06 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:06 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6517, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6517, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:06 [INFO] Processing transaction 2024-12-04 00:00:00
2025-02-11 03:58:06 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -20000, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:08 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:08 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6524, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6524, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:08 [INFO] Processing transaction 2024-12-03 00:00:00
2025-02-11 03:58:08 [INFO] Processing transaction: {'creditorName': 'Barclaycard', 'remittanceInformationUnstructured': 'MR MICHAEL ALI  4929158426786006 BBP', 'currency': 'GBP', 'amount': -14538, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:10 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:10 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6518, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6518, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:10 [INFO] Processing transaction 2024-12-02 00:00:00
2025-02-11 03:58:10 [INFO] Processing transaction: {'creditorName': 'DVLA', 'remittanceInformationUnstructured': 'DVLA-LN61PYJ  000000000056504532 DDR', 'currency': 'GBP', 'amount': -1312, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:12 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:12 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:12 [INFO] Processing transaction 2024-11-28 00:00:00
2025-02-11 03:58:12 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB836806900164200', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB836806900164200 FT', 'currency': 'GBP', 'amount': -3515, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:14 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:14 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6525, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6525, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:14 [INFO] Processing transaction 2024-11-28 00:00:00
2025-02-11 03:58:14 [INFO] Processing transaction: {'creditorName': 'Barclaycard', 'remittanceInformationUnstructured': 'MR MICHAEL ALI  4929158426786006 BBP', 'currency': 'GBP', 'amount': -14538, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:16 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:16 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6518, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6518, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:16 [INFO] Processing transaction 2024-11-26 00:00:00
2025-02-11 03:58:16 [INFO] Processing transaction: {'creditorName': 'HELEN MELON  WAFFLE TIME', 'remittanceInformationUnstructured': 'HELEN MELON  WAFFLE TIME FT', 'currency': 'GBP', 'amount': -1000, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:18 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:18 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:18 [INFO] Processing transaction 2024-11-20 00:00:00
2025-02-11 03:58:18 [INFO] Processing transaction: {'creditorName': 'EURO CAR PARKS LTD', 'remittanceInformationUnstructured': 'EURO CAR PARKS LTD  ON 19 NOV BCC', 'currency': 'GBP', 'amount': -6000, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:20 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:20 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:20 [INFO] Processing transaction 2024-11-20 00:00:00
2025-02-11 03:58:20 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -30000, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:22 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:22 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6524, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6524, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:22 [INFO] Processing transaction 2024-11-19 00:00:00
2025-02-11 03:58:22 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB791748851198619', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB791748851198619 FT', 'currency': 'GBP', 'amount': -31410, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:24 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:24 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6525, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6525, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:24 [INFO] Processing transaction 2024-11-18 00:00:00
2025-02-11 03:58:24 [INFO] Processing transaction: {'creditorName': 'Admiral', 'remittanceInformationUnstructured': 'ADMIRAL INSURANCE  P73244643010000010 DDR', 'currency': 'GBP', 'amount': -11313, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:26 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:26 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:26 [INFO] Processing transaction 2024-11-18 00:00:00
2025-02-11 03:58:26 [INFO] Processing transaction: {'creditorName': 'AA', 'remittanceInformationUnstructured': 'AA MEMBERSHIP  6356011528897956 DDR', 'currency': 'GBP', 'amount': -1641, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:28 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:28 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:28 [INFO] Processing transaction 2024-11-18 00:00:00
2025-02-11 03:58:28 [INFO] Processing transaction: {'creditorName': 'FIRECRAWL.DEV USA AMOUNT IN USD 19.00', 'remittanceInformationUnstructured': 'FIRECRAWL.DEV USA AMOUNT IN USD 19.00 ON 16 NOV VISA 1.2594 FINAL GBP AMOUNT INCLUDES NON-STERLING TRANS FEE £0.45 ', 'currency': 'GBP', 'amount': -1554, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:30 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:30 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6545, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6545, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:30 [INFO] Processing transaction 2024-11-18 00:00:00
2025-02-11 03:58:30 [INFO] Processing transaction: {'creditorName': nan, 'remittanceInformationUnstructured': 'NOTEMACHINE Notemachine 16NOV 13.49 ATM', 'currency': 'GBP', 'amount': -6000, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:32 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:32 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:32 [INFO] Processing transaction 2024-11-08 00:00:00
2025-02-11 03:58:32 [INFO] Processing transaction: {'creditorName': 'Barclaycard', 'remittanceInformationUnstructured': 'MR MICHAEL ALI  4929158426786006 BBP', 'currency': 'GBP', 'amount': -18002, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:34 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:34 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6518, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6518, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:34 [INFO] Processing transaction 2024-11-08 00:00:00
2025-02-11 03:58:34 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -50000, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:36 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:36 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6524, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6524, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:36 [INFO] Processing transaction 2024-11-07 00:00:00
2025-02-11 03:58:36 [INFO] Processing transaction: {'creditorName': 'Vodafone', 'remittanceInformationUnstructured': 'VODAFONE LTD  7082364969-1001 DDR', 'currency': 'GBP', 'amount': -1163, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:38 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:38 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6517, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6517, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:38 [INFO] Processing transaction 2024-11-04 00:00:00
2025-02-11 03:58:38 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -30000, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:40 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:40 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6524, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6524, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:40 [INFO] Processing transaction 2024-11-01 00:00:00
2025-02-11 03:58:40 [INFO] Processing transaction: {'creditorName': 'DVLA', 'remittanceInformationUnstructured': 'DVLA-LN61PYJ  000000000056504532 DDR', 'currency': 'GBP', 'amount': -1312, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:42 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:42 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:42 [INFO] Processing transaction 2024-10-21 00:00:00
2025-02-11 03:58:42 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB847450840043831', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB847450840043831 FT', 'currency': 'GBP', 'amount': -599, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:44 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:44 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6525, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6525, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:44 [INFO] Processing transaction 2024-10-21 00:00:00
2025-02-11 03:58:44 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB170732208294537', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB170732208294537 FT', 'currency': 'GBP', 'amount': -44918, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:46 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:46 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6525, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6525, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:46 [INFO] Processing transaction 2024-10-18 00:00:00
2025-02-11 03:58:46 [INFO] Processing transaction: {'creditorName': 'AA', 'remittanceInformationUnstructured': 'AA MEMBERSHIP  6356011528897956 DDR', 'currency': 'GBP', 'amount': -1641, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:48 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:48 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:48 [INFO] Processing transaction 2024-10-17 00:00:00
2025-02-11 03:58:48 [INFO] Processing transaction: {'creditorName': 'Admiral', 'remittanceInformationUnstructured': 'ADMIRAL INSURANCE  P73244643010000009 DDR', 'currency': 'GBP', 'amount': -11313, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:50 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:50 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6519, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:50 [INFO] Processing transaction 2024-10-09 00:00:00
2025-02-11 03:58:50 [INFO] Processing transaction: {'creditorName': 'Vodafone', 'remittanceInformationUnstructured': 'VODAFONE LTD  7082364969-1001 DDR', 'currency': 'GBP', 'amount': -1295, 'chart_of_account_agent': None, 'chart_of_account_reason': None, 'chart_of_account_confidence': None}
2025-02-11 03:58:52 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 03:58:52 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6517, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3651794097.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6517, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 03:58:52 [INFO] Finished processing all transactions
2025-02-11 04:07:45 [INFO] Starting to process 37 transactions
2025-02-11 04:07:45 [INFO] Initializing TransactionClassifier
2025-02-11 04:07:45 [INFO] Processing transaction 2025-01-02 00:00:00 at 04:07:45
2025-02-11 04:07:45 [INFO] Processing transaction: {'creditorName': 'DVLA', 'remittanceInformationUnstructured': 'DVLA-LN61PYJ  000000000056504532 DDR', 'currency': 'GBP', 'amount': -1312, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:07:46 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:07:46 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:07:46 [INFO] Processing transaction 2025-01-02 00:00:00 at 04:07:46
2025-02-11 04:07:46 [INFO] Processing transaction: {'creditorName': 'SUMUP *NEW LOOK BA', 'remittanceInformationUnstructured': 'SUMUP *NEW LOOK BA  ON 31 DEC BCC', 'currency': 'GBP', 'amount': -6000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:07:48 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:07:48 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:07:48 [INFO] Processing transaction 2025-01-02 00:00:00 at 04:07:48
2025-02-11 04:07:48 [INFO] Processing transaction: {'creditorName': nan, 'remittanceInformationUnstructured': 'SAINSBURYS BANK Sainsburys Bank 31DEC 21.41 ATM', 'currency': 'GBP', 'amount': -5000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:07:50 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:07:50 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:07:50 [INFO] Processing transaction 2024-12-27 00:00:00 at 04:07:50
2025-02-11 04:07:50 [INFO] Processing transaction: {'creditorName': 'KLARNA*ZOOM COMMUN', 'remittanceInformationUnstructured': 'KLARNA*ZOOM COMMUN  ON 26 DEC BCC', 'currency': 'GBP', 'amount': -1559, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:07:52 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:07:52 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:07:52 [INFO] Processing transaction 2024-12-27 00:00:00 at 04:07:52
2025-02-11 04:07:52 [INFO] Processing transaction: {'creditorName': 'THE HUB DENTAL PRA', 'remittanceInformationUnstructured': 'THE HUB DENTAL PRA  ON 24 DEC BCC', 'currency': 'GBP', 'amount': -5000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:07:54 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:07:54 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:07:54 [INFO] Processing transaction 2024-12-24 00:00:00 at 04:07:54
2025-02-11 04:07:54 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB166643916980508', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB166643916980508 FT', 'currency': 'GBP', 'amount': -599, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:07:56 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:07:56 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:07:56 [INFO] Processing transaction 2024-12-24 00:00:00 at 04:07:56
2025-02-11 04:07:56 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB313981532278988', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB313981532278988 FT', 'currency': 'GBP', 'amount': -21320, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:07:58 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:07:58 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:07:58 [INFO] Processing transaction 2024-12-24 00:00:00 at 04:07:58
2025-02-11 04:07:58 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -30000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:00 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:00 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:00 [INFO] Processing transaction 2024-12-24 00:00:00 at 04:08:00
2025-02-11 04:08:00 [INFO] Processing transaction: {'creditorName': 'Barclaycard', 'remittanceInformationUnstructured': 'MR MICHAEL ALI  4929158426786006 BBP', 'currency': 'GBP', 'amount': -14538, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:02 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:02 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:02 [INFO] Processing transaction 2024-12-18 00:00:00 at 04:08:02
2025-02-11 04:08:02 [INFO] Processing transaction: {'creditorName': 'AA', 'remittanceInformationUnstructured': 'AA MEMBERSHIP  6356011528897956 DDR', 'currency': 'GBP', 'amount': -1641, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:04 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:04 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6506, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6506, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:04 [INFO] Processing transaction 2024-12-17 00:00:00 at 04:08:04
2025-02-11 04:08:04 [INFO] Processing transaction: {'creditorName': 'FIRECRAWL.DEV USA AMOUNT IN USD 19.00', 'remittanceInformationUnstructured': 'FIRECRAWL.DEV USA AMOUNT IN USD 19.00 ON 16 DEC VISA 1.2605 FINAL GBP AMOUNT INCLUDES NON-STERLING TRANS FEE £0.45 ', 'currency': 'GBP', 'amount': -1552, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:06 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:06 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6535, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6535, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:06 [INFO] Processing transaction 2024-12-17 00:00:00 at 04:08:06
2025-02-11 04:08:06 [INFO] Processing transaction: {'creditorName': 'Admiral', 'remittanceInformationUnstructured': 'ADMIRAL INSURANCE  P73244643010000011 DDR', 'currency': 'GBP', 'amount': -11313, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:08 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:08 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:08 [INFO] Processing transaction 2024-12-16 00:00:00 at 04:08:08
2025-02-11 04:08:08 [INFO] Processing transaction: {'creditorName': 'HORIZONPARKING.CO.', 'remittanceInformationUnstructured': 'HORIZONPARKING.CO.  ON 13 DEC BCC', 'currency': 'GBP', 'amount': -6000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:10 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:10 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:10 [INFO] Processing transaction 2024-12-10 00:00:00 at 04:08:10
2025-02-11 04:08:10 [INFO] Processing transaction: {'creditorName': 'Vodafone', 'remittanceInformationUnstructured': 'VODAFONE LTD  7082364969-1001 DDR', 'currency': 'GBP', 'amount': -1091, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:12 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:12 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:12 [INFO] Processing transaction 2024-12-04 00:00:00 at 04:08:12
2025-02-11 04:08:12 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -20000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:14 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:14 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:14 [INFO] Processing transaction 2024-12-03 00:00:00 at 04:08:14
2025-02-11 04:08:14 [INFO] Processing transaction: {'creditorName': 'Barclaycard', 'remittanceInformationUnstructured': 'MR MICHAEL ALI  4929158426786006 BBP', 'currency': 'GBP', 'amount': -14538, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:16 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:16 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:16 [INFO] Processing transaction 2024-12-02 00:00:00 at 04:08:16
2025-02-11 04:08:16 [INFO] Processing transaction: {'creditorName': 'DVLA', 'remittanceInformationUnstructured': 'DVLA-LN61PYJ  000000000056504532 DDR', 'currency': 'GBP', 'amount': -1312, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:18 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:18 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:18 [INFO] Processing transaction 2024-11-28 00:00:00 at 04:08:18
2025-02-11 04:08:18 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB836806900164200', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB836806900164200 FT', 'currency': 'GBP', 'amount': -3515, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:20 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:20 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:20 [INFO] Processing transaction 2024-11-28 00:00:00 at 04:08:20
2025-02-11 04:08:20 [INFO] Processing transaction: {'creditorName': 'Barclaycard', 'remittanceInformationUnstructured': 'MR MICHAEL ALI  4929158426786006 BBP', 'currency': 'GBP', 'amount': -14538, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:22 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:22 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:22 [INFO] Processing transaction 2024-11-26 00:00:00 at 04:08:22
2025-02-11 04:08:22 [INFO] Processing transaction: {'creditorName': 'HELEN MELON  WAFFLE TIME', 'remittanceInformationUnstructured': 'HELEN MELON  WAFFLE TIME FT', 'currency': 'GBP', 'amount': -1000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:24 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:24 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:24 [INFO] Processing transaction 2024-11-20 00:00:00 at 04:08:24
2025-02-11 04:08:24 [INFO] Processing transaction: {'creditorName': 'EURO CAR PARKS LTD', 'remittanceInformationUnstructured': 'EURO CAR PARKS LTD  ON 19 NOV BCC', 'currency': 'GBP', 'amount': -6000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:26 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:26 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:26 [INFO] Processing transaction 2024-11-20 00:00:00 at 04:08:26
2025-02-11 04:08:26 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -30000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:28 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:28 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:28 [INFO] Processing transaction 2024-11-19 00:00:00 at 04:08:28
2025-02-11 04:08:28 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB791748851198619', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB791748851198619 FT', 'currency': 'GBP', 'amount': -31410, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:30 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:30 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:30 [INFO] Processing transaction 2024-11-18 00:00:00 at 04:08:30
2025-02-11 04:08:30 [INFO] Processing transaction: {'creditorName': 'Admiral', 'remittanceInformationUnstructured': 'ADMIRAL INSURANCE  P73244643010000010 DDR', 'currency': 'GBP', 'amount': -11313, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:32 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:32 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:32 [INFO] Processing transaction 2024-11-18 00:00:00 at 04:08:32
2025-02-11 04:08:32 [INFO] Processing transaction: {'creditorName': 'AA', 'remittanceInformationUnstructured': 'AA MEMBERSHIP  6356011528897956 DDR', 'currency': 'GBP', 'amount': -1641, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:34 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:34 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6506, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6506, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:34 [INFO] Processing transaction 2024-11-18 00:00:00 at 04:08:34
2025-02-11 04:08:34 [INFO] Processing transaction: {'creditorName': 'FIRECRAWL.DEV USA AMOUNT IN USD 19.00', 'remittanceInformationUnstructured': 'FIRECRAWL.DEV USA AMOUNT IN USD 19.00 ON 16 NOV VISA 1.2594 FINAL GBP AMOUNT INCLUDES NON-STERLING TRANS FEE £0.45 ', 'currency': 'GBP', 'amount': -1554, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:36 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:36 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6535, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6535, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:36 [INFO] Processing transaction 2024-11-18 00:00:00 at 04:08:36
2025-02-11 04:08:36 [INFO] Processing transaction: {'creditorName': nan, 'remittanceInformationUnstructured': 'NOTEMACHINE Notemachine 16NOV 13.49 ATM', 'currency': 'GBP', 'amount': -6000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:38 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:38 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:38 [INFO] Processing transaction 2024-11-08 00:00:00 at 04:08:38
2025-02-11 04:08:38 [INFO] Processing transaction: {'creditorName': 'Barclaycard', 'remittanceInformationUnstructured': 'MR MICHAEL ALI  4929158426786006 BBP', 'currency': 'GBP', 'amount': -18002, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:40 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:40 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:40 [INFO] Processing transaction 2024-11-08 00:00:00 at 04:08:40
2025-02-11 04:08:40 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -50000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:42 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:42 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:42 [INFO] Processing transaction 2024-11-07 00:00:00 at 04:08:42
2025-02-11 04:08:42 [INFO] Processing transaction: {'creditorName': 'Vodafone', 'remittanceInformationUnstructured': 'VODAFONE LTD  7082364969-1001 DDR', 'currency': 'GBP', 'amount': -1163, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:44 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:44 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:44 [INFO] Processing transaction 2024-11-04 00:00:00 at 04:08:44
2025-02-11 04:08:44 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -30000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:46 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:46 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:46 [INFO] Processing transaction 2024-11-01 00:00:00 at 04:08:46
2025-02-11 04:08:46 [INFO] Processing transaction: {'creditorName': 'DVLA', 'remittanceInformationUnstructured': 'DVLA-LN61PYJ  000000000056504532 DDR', 'currency': 'GBP', 'amount': -1312, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:48 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:48 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:48 [INFO] Processing transaction 2024-10-21 00:00:00 at 04:08:48
2025-02-11 04:08:48 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB847450840043831', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB847450840043831 FT', 'currency': 'GBP', 'amount': -599, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:50 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:50 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:50 [INFO] Processing transaction 2024-10-21 00:00:00 at 04:08:50
2025-02-11 04:08:50 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB170732208294537', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB170732208294537 FT', 'currency': 'GBP', 'amount': -44918, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:52 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:52 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:52 [INFO] Processing transaction 2024-10-18 00:00:00 at 04:08:52
2025-02-11 04:08:52 [INFO] Processing transaction: {'creditorName': 'AA', 'remittanceInformationUnstructured': 'AA MEMBERSHIP  6356011528897956 DDR', 'currency': 'GBP', 'amount': -1641, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:54 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:54 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6506, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6506, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:54 [INFO] Processing transaction 2024-10-17 00:00:00 at 04:08:54
2025-02-11 04:08:54 [INFO] Processing transaction: {'creditorName': 'Admiral', 'remittanceInformationUnstructured': 'ADMIRAL INSURANCE  P73244643010000009 DDR', 'currency': 'GBP', 'amount': -11313, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:56 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:56 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:56 [INFO] Processing transaction 2024-10-09 00:00:00 at 04:08:56
2025-02-11 04:08:56 [INFO] Processing transaction: {'creditorName': 'Vodafone', 'remittanceInformationUnstructured': 'VODAFONE LTD  7082364969-1001 DDR', 'currency': 'GBP', 'amount': -1295, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:08:58 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:08:58 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1944071982.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:08:58 [INFO] Finished processing all transactions
2025-02-11 04:09:18 [INFO] Starting to process 37 transactions
2025-02-11 04:09:18 [INFO] Initializing TransactionClassifier
2025-02-11 04:09:18 [INFO] Processing transaction 2025-01-02 00:00:00
2025-02-11 04:09:18 [INFO] Processing transaction: {'creditorName': 'DVLA', 'remittanceInformationUnstructured': 'DVLA-LN61PYJ  000000000056504532 DDR', 'currency': 'GBP', 'amount': -1312, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:09:18 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:09:18 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:09:18 [INFO] Transaction 2025-01-02 00:00:00:
2025-02-11 04:09:18 [INFO] Account Code: ERROR
2025-02-11 04:09:18 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:09:18 [INFO] --------------------------------------------------
2025-02-11 04:09:18 [INFO] Processing transaction 2025-01-02 00:00:00
2025-02-11 04:09:18 [INFO] Processing transaction: {'creditorName': 'SUMUP *NEW LOOK BA', 'remittanceInformationUnstructured': 'SUMUP *NEW LOOK BA  ON 31 DEC BCC', 'currency': 'GBP', 'amount': -6000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:09:20 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:09:20 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:09:20 [INFO] Transaction 2025-01-02 00:00:00:
2025-02-11 04:09:20 [INFO] Account Code: ERROR
2025-02-11 04:09:20 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:09:20 [INFO] --------------------------------------------------
2025-02-11 04:09:20 [INFO] Processing transaction 2025-01-02 00:00:00
2025-02-11 04:09:20 [INFO] Processing transaction: {'creditorName': nan, 'remittanceInformationUnstructured': 'SAINSBURYS BANK Sainsburys Bank 31DEC 21.41 ATM', 'currency': 'GBP', 'amount': -5000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:09:22 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:09:22 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:09:22 [INFO] Transaction 2025-01-02 00:00:00:
2025-02-11 04:09:22 [INFO] Account Code: ERROR
2025-02-11 04:09:22 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:09:22 [INFO] --------------------------------------------------
2025-02-11 04:09:22 [INFO] Processing transaction 2024-12-27 00:00:00
2025-02-11 04:09:22 [INFO] Processing transaction: {'creditorName': 'KLARNA*ZOOM COMMUN', 'remittanceInformationUnstructured': 'KLARNA*ZOOM COMMUN  ON 26 DEC BCC', 'currency': 'GBP', 'amount': -1559, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:09:24 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:09:24 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:09:24 [INFO] Transaction 2024-12-27 00:00:00:
2025-02-11 04:09:24 [INFO] Account Code: ERROR
2025-02-11 04:09:24 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:09:24 [INFO] --------------------------------------------------
2025-02-11 04:09:24 [INFO] Processing transaction 2024-12-27 00:00:00
2025-02-11 04:09:24 [INFO] Processing transaction: {'creditorName': 'THE HUB DENTAL PRA', 'remittanceInformationUnstructured': 'THE HUB DENTAL PRA  ON 24 DEC BCC', 'currency': 'GBP', 'amount': -5000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:09:26 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:09:26 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:09:26 [INFO] Transaction 2024-12-27 00:00:00:
2025-02-11 04:09:26 [INFO] Account Code: ERROR
2025-02-11 04:09:26 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:09:26 [INFO] --------------------------------------------------
2025-02-11 04:09:26 [INFO] Processing transaction 2024-12-24 00:00:00
2025-02-11 04:09:26 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB166643916980508', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB166643916980508 FT', 'currency': 'GBP', 'amount': -599, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:09:28 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:09:28 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:09:28 [INFO] Transaction 2024-12-24 00:00:00:
2025-02-11 04:09:28 [INFO] Account Code: ERROR
2025-02-11 04:09:28 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:09:28 [INFO] --------------------------------------------------
2025-02-11 04:09:28 [INFO] Processing transaction 2024-12-24 00:00:00
2025-02-11 04:09:28 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB313981532278988', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB313981532278988 FT', 'currency': 'GBP', 'amount': -21320, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:09:30 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:09:30 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:09:30 [INFO] Transaction 2024-12-24 00:00:00:
2025-02-11 04:09:30 [INFO] Account Code: ERROR
2025-02-11 04:09:30 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:09:30 [INFO] --------------------------------------------------
2025-02-11 04:09:30 [INFO] Processing transaction 2024-12-24 00:00:00
2025-02-11 04:09:30 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -30000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:09:32 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:09:32 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:09:32 [INFO] Transaction 2024-12-24 00:00:00:
2025-02-11 04:09:32 [INFO] Account Code: ERROR
2025-02-11 04:09:32 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:09:32 [INFO] --------------------------------------------------
2025-02-11 04:09:32 [INFO] Processing transaction 2024-12-24 00:00:00
2025-02-11 04:09:32 [INFO] Processing transaction: {'creditorName': 'Barclaycard', 'remittanceInformationUnstructured': 'MR MICHAEL ALI  4929158426786006 BBP', 'currency': 'GBP', 'amount': -14538, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:09:34 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:09:34 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:09:34 [INFO] Transaction 2024-12-24 00:00:00:
2025-02-11 04:09:34 [INFO] Account Code: ERROR
2025-02-11 04:09:34 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:09:34 [INFO] --------------------------------------------------
2025-02-11 04:09:34 [INFO] Processing transaction 2024-12-18 00:00:00
2025-02-11 04:09:34 [INFO] Processing transaction: {'creditorName': 'AA', 'remittanceInformationUnstructured': 'AA MEMBERSHIP  6356011528897956 DDR', 'currency': 'GBP', 'amount': -1641, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:09:36 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:09:36 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6506, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6506, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:09:36 [INFO] Transaction 2024-12-18 00:00:00:
2025-02-11 04:09:36 [INFO] Account Code: ERROR
2025-02-11 04:09:36 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:09:36 [INFO] --------------------------------------------------
2025-02-11 04:09:36 [INFO] Processing transaction 2024-12-17 00:00:00
2025-02-11 04:09:36 [INFO] Processing transaction: {'creditorName': 'FIRECRAWL.DEV USA AMOUNT IN USD 19.00', 'remittanceInformationUnstructured': 'FIRECRAWL.DEV USA AMOUNT IN USD 19.00 ON 16 DEC VISA 1.2605 FINAL GBP AMOUNT INCLUDES NON-STERLING TRANS FEE £0.45 ', 'currency': 'GBP', 'amount': -1552, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:09:38 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:09:38 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6535, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6535, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:09:38 [INFO] Transaction 2024-12-17 00:00:00:
2025-02-11 04:09:38 [INFO] Account Code: ERROR
2025-02-11 04:09:38 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:09:38 [INFO] --------------------------------------------------
2025-02-11 04:09:38 [INFO] Processing transaction 2024-12-17 00:00:00
2025-02-11 04:09:38 [INFO] Processing transaction: {'creditorName': 'Admiral', 'remittanceInformationUnstructured': 'ADMIRAL INSURANCE  P73244643010000011 DDR', 'currency': 'GBP', 'amount': -11313, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:09:40 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:09:40 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:09:40 [INFO] Transaction 2024-12-17 00:00:00:
2025-02-11 04:09:40 [INFO] Account Code: ERROR
2025-02-11 04:09:40 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:09:40 [INFO] --------------------------------------------------
2025-02-11 04:09:40 [INFO] Processing transaction 2024-12-16 00:00:00
2025-02-11 04:09:40 [INFO] Processing transaction: {'creditorName': 'HORIZONPARKING.CO.', 'remittanceInformationUnstructured': 'HORIZONPARKING.CO.  ON 13 DEC BCC', 'currency': 'GBP', 'amount': -6000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:09:42 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:09:42 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:09:42 [INFO] Transaction 2024-12-16 00:00:00:
2025-02-11 04:09:42 [INFO] Account Code: ERROR
2025-02-11 04:09:42 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:09:42 [INFO] --------------------------------------------------
2025-02-11 04:09:42 [INFO] Processing transaction 2024-12-10 00:00:00
2025-02-11 04:09:42 [INFO] Processing transaction: {'creditorName': 'Vodafone', 'remittanceInformationUnstructured': 'VODAFONE LTD  7082364969-1001 DDR', 'currency': 'GBP', 'amount': -1091, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:09:44 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:09:44 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:09:44 [INFO] Transaction 2024-12-10 00:00:00:
2025-02-11 04:09:44 [INFO] Account Code: ERROR
2025-02-11 04:09:44 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:09:44 [INFO] --------------------------------------------------
2025-02-11 04:09:44 [INFO] Processing transaction 2024-12-04 00:00:00
2025-02-11 04:09:44 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -20000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:09:46 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:09:46 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:09:46 [INFO] Transaction 2024-12-04 00:00:00:
2025-02-11 04:09:46 [INFO] Account Code: ERROR
2025-02-11 04:09:46 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:09:46 [INFO] --------------------------------------------------
2025-02-11 04:09:46 [INFO] Processing transaction 2024-12-03 00:00:00
2025-02-11 04:09:46 [INFO] Processing transaction: {'creditorName': 'Barclaycard', 'remittanceInformationUnstructured': 'MR MICHAEL ALI  4929158426786006 BBP', 'currency': 'GBP', 'amount': -14538, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:09:48 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:09:48 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:09:48 [INFO] Transaction 2024-12-03 00:00:00:
2025-02-11 04:09:48 [INFO] Account Code: ERROR
2025-02-11 04:09:48 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:09:48 [INFO] --------------------------------------------------
2025-02-11 04:09:48 [INFO] Processing transaction 2024-12-02 00:00:00
2025-02-11 04:09:48 [INFO] Processing transaction: {'creditorName': 'DVLA', 'remittanceInformationUnstructured': 'DVLA-LN61PYJ  000000000056504532 DDR', 'currency': 'GBP', 'amount': -1312, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:09:50 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:09:50 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:09:50 [INFO] Transaction 2024-12-02 00:00:00:
2025-02-11 04:09:50 [INFO] Account Code: ERROR
2025-02-11 04:09:50 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:09:50 [INFO] --------------------------------------------------
2025-02-11 04:09:50 [INFO] Processing transaction 2024-11-28 00:00:00
2025-02-11 04:09:50 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB836806900164200', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB836806900164200 FT', 'currency': 'GBP', 'amount': -3515, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:09:52 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:09:52 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:09:52 [INFO] Transaction 2024-11-28 00:00:00:
2025-02-11 04:09:52 [INFO] Account Code: ERROR
2025-02-11 04:09:52 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:09:52 [INFO] --------------------------------------------------
2025-02-11 04:09:52 [INFO] Processing transaction 2024-11-28 00:00:00
2025-02-11 04:09:52 [INFO] Processing transaction: {'creditorName': 'Barclaycard', 'remittanceInformationUnstructured': 'MR MICHAEL ALI  4929158426786006 BBP', 'currency': 'GBP', 'amount': -14538, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:09:54 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:09:54 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:09:54 [INFO] Transaction 2024-11-28 00:00:00:
2025-02-11 04:09:54 [INFO] Account Code: ERROR
2025-02-11 04:09:54 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:09:54 [INFO] --------------------------------------------------
2025-02-11 04:09:54 [INFO] Processing transaction 2024-11-26 00:00:00
2025-02-11 04:09:54 [INFO] Processing transaction: {'creditorName': 'HELEN MELON  WAFFLE TIME', 'remittanceInformationUnstructured': 'HELEN MELON  WAFFLE TIME FT', 'currency': 'GBP', 'amount': -1000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:09:56 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:09:56 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:09:56 [INFO] Transaction 2024-11-26 00:00:00:
2025-02-11 04:09:56 [INFO] Account Code: ERROR
2025-02-11 04:09:56 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:09:56 [INFO] --------------------------------------------------
2025-02-11 04:09:56 [INFO] Processing transaction 2024-11-20 00:00:00
2025-02-11 04:09:56 [INFO] Processing transaction: {'creditorName': 'EURO CAR PARKS LTD', 'remittanceInformationUnstructured': 'EURO CAR PARKS LTD  ON 19 NOV BCC', 'currency': 'GBP', 'amount': -6000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:09:58 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:09:58 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:09:58 [INFO] Transaction 2024-11-20 00:00:00:
2025-02-11 04:09:58 [INFO] Account Code: ERROR
2025-02-11 04:09:58 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:09:58 [INFO] --------------------------------------------------
2025-02-11 04:09:58 [INFO] Processing transaction 2024-11-20 00:00:00
2025-02-11 04:09:58 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -30000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:10:00 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:10:00 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:10:00 [INFO] Transaction 2024-11-20 00:00:00:
2025-02-11 04:10:00 [INFO] Account Code: ERROR
2025-02-11 04:10:00 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:10:00 [INFO] --------------------------------------------------
2025-02-11 04:10:00 [INFO] Processing transaction 2024-11-19 00:00:00
2025-02-11 04:10:00 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB791748851198619', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB791748851198619 FT', 'currency': 'GBP', 'amount': -31410, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:10:02 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:10:02 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:10:02 [INFO] Transaction 2024-11-19 00:00:00:
2025-02-11 04:10:02 [INFO] Account Code: ERROR
2025-02-11 04:10:02 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:10:02 [INFO] --------------------------------------------------
2025-02-11 04:10:02 [INFO] Processing transaction 2024-11-18 00:00:00
2025-02-11 04:10:02 [INFO] Processing transaction: {'creditorName': 'Admiral', 'remittanceInformationUnstructured': 'ADMIRAL INSURANCE  P73244643010000010 DDR', 'currency': 'GBP', 'amount': -11313, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:10:04 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:10:04 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:10:04 [INFO] Transaction 2024-11-18 00:00:00:
2025-02-11 04:10:04 [INFO] Account Code: ERROR
2025-02-11 04:10:04 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:10:04 [INFO] --------------------------------------------------
2025-02-11 04:10:04 [INFO] Processing transaction 2024-11-18 00:00:00
2025-02-11 04:10:04 [INFO] Processing transaction: {'creditorName': 'AA', 'remittanceInformationUnstructured': 'AA MEMBERSHIP  6356011528897956 DDR', 'currency': 'GBP', 'amount': -1641, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:10:06 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:10:06 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6506, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6506, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:10:06 [INFO] Transaction 2024-11-18 00:00:00:
2025-02-11 04:10:06 [INFO] Account Code: ERROR
2025-02-11 04:10:06 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:10:06 [INFO] --------------------------------------------------
2025-02-11 04:10:06 [INFO] Processing transaction 2024-11-18 00:00:00
2025-02-11 04:10:06 [INFO] Processing transaction: {'creditorName': 'FIRECRAWL.DEV USA AMOUNT IN USD 19.00', 'remittanceInformationUnstructured': 'FIRECRAWL.DEV USA AMOUNT IN USD 19.00 ON 16 NOV VISA 1.2594 FINAL GBP AMOUNT INCLUDES NON-STERLING TRANS FEE £0.45 ', 'currency': 'GBP', 'amount': -1554, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:10:08 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:10:08 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6535, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6535, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:10:08 [INFO] Transaction 2024-11-18 00:00:00:
2025-02-11 04:10:08 [INFO] Account Code: ERROR
2025-02-11 04:10:08 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:10:08 [INFO] --------------------------------------------------
2025-02-11 04:10:08 [INFO] Processing transaction 2024-11-18 00:00:00
2025-02-11 04:10:08 [INFO] Processing transaction: {'creditorName': nan, 'remittanceInformationUnstructured': 'NOTEMACHINE Notemachine 16NOV 13.49 ATM', 'currency': 'GBP', 'amount': -6000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:10:10 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:10:10 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:10:10 [INFO] Transaction 2024-11-18 00:00:00:
2025-02-11 04:10:10 [INFO] Account Code: ERROR
2025-02-11 04:10:10 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:10:10 [INFO] --------------------------------------------------
2025-02-11 04:10:10 [INFO] Processing transaction 2024-11-08 00:00:00
2025-02-11 04:10:10 [INFO] Processing transaction: {'creditorName': 'Barclaycard', 'remittanceInformationUnstructured': 'MR MICHAEL ALI  4929158426786006 BBP', 'currency': 'GBP', 'amount': -18002, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:10:12 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:10:12 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:10:12 [INFO] Transaction 2024-11-08 00:00:00:
2025-02-11 04:10:12 [INFO] Account Code: ERROR
2025-02-11 04:10:12 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:10:12 [INFO] --------------------------------------------------
2025-02-11 04:10:12 [INFO] Processing transaction 2024-11-08 00:00:00
2025-02-11 04:10:12 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -50000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:10:14 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:10:14 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:10:14 [INFO] Transaction 2024-11-08 00:00:00:
2025-02-11 04:10:14 [INFO] Account Code: ERROR
2025-02-11 04:10:14 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:10:14 [INFO] --------------------------------------------------
2025-02-11 04:10:14 [INFO] Processing transaction 2024-11-07 00:00:00
2025-02-11 04:10:14 [INFO] Processing transaction: {'creditorName': 'Vodafone', 'remittanceInformationUnstructured': 'VODAFONE LTD  7082364969-1001 DDR', 'currency': 'GBP', 'amount': -1163, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:10:16 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:10:16 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:10:16 [INFO] Transaction 2024-11-07 00:00:00:
2025-02-11 04:10:16 [INFO] Account Code: ERROR
2025-02-11 04:10:16 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:10:16 [INFO] --------------------------------------------------
2025-02-11 04:10:16 [INFO] Processing transaction 2024-11-04 00:00:00
2025-02-11 04:10:16 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -30000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:10:18 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:10:18 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:10:18 [INFO] Transaction 2024-11-04 00:00:00:
2025-02-11 04:10:18 [INFO] Account Code: ERROR
2025-02-11 04:10:18 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:10:18 [INFO] --------------------------------------------------
2025-02-11 04:10:18 [INFO] Processing transaction 2024-11-01 00:00:00
2025-02-11 04:10:18 [INFO] Processing transaction: {'creditorName': 'DVLA', 'remittanceInformationUnstructured': 'DVLA-LN61PYJ  000000000056504532 DDR', 'currency': 'GBP', 'amount': -1312, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:10:20 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:10:20 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:10:20 [INFO] Transaction 2024-11-01 00:00:00:
2025-02-11 04:10:20 [INFO] Account Code: ERROR
2025-02-11 04:10:20 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:10:20 [INFO] --------------------------------------------------
2025-02-11 04:10:20 [INFO] Processing transaction 2024-10-21 00:00:00
2025-02-11 04:10:20 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB847450840043831', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB847450840043831 FT', 'currency': 'GBP', 'amount': -599, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:10:22 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:10:22 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6515, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:10:22 [INFO] Transaction 2024-10-21 00:00:00:
2025-02-11 04:10:22 [INFO] Account Code: ERROR
2025-02-11 04:10:22 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:10:22 [INFO] --------------------------------------------------
2025-02-11 04:10:22 [INFO] Processing transaction 2024-10-21 00:00:00
2025-02-11 04:10:22 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB170732208294537', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB170732208294537 FT', 'currency': 'GBP', 'amount': -44918, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:10:24 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:10:24 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6516, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:10:24 [INFO] Transaction 2024-10-21 00:00:00:
2025-02-11 04:10:24 [INFO] Account Code: ERROR
2025-02-11 04:10:24 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:10:24 [INFO] --------------------------------------------------
2025-02-11 04:10:24 [INFO] Processing transaction 2024-10-18 00:00:00
2025-02-11 04:10:24 [INFO] Processing transaction: {'creditorName': 'AA', 'remittanceInformationUnstructured': 'AA MEMBERSHIP  6356011528897956 DDR', 'currency': 'GBP', 'amount': -1641, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:10:26 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:10:26 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6506, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6506, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:10:26 [INFO] Transaction 2024-10-18 00:00:00:
2025-02-11 04:10:26 [INFO] Account Code: ERROR
2025-02-11 04:10:26 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:10:26 [INFO] --------------------------------------------------
2025-02-11 04:10:26 [INFO] Processing transaction 2024-10-17 00:00:00
2025-02-11 04:10:26 [INFO] Processing transaction: {'creditorName': 'Admiral', 'remittanceInformationUnstructured': 'ADMIRAL INSURANCE  P73244643010000009 DDR', 'currency': 'GBP', 'amount': -11313, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:10:28 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:10:28 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6509, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:10:28 [INFO] Transaction 2024-10-17 00:00:00:
2025-02-11 04:10:28 [INFO] Account Code: ERROR
2025-02-11 04:10:28 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:10:28 [INFO] --------------------------------------------------
2025-02-11 04:10:28 [INFO] Processing transaction 2024-10-09 00:00:00
2025-02-11 04:10:28 [INFO] Processing transaction: {'creditorName': 'Vodafone', 'remittanceInformationUnstructured': 'VODAFONE LTD  7082364969-1001 DDR', 'currency': 'GBP', 'amount': -1295, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:10:30 [INFO] HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 413 Payload Too Large"
2025-02-11 04:10:30 [ERROR] Classification failed: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3825566685.py", line 73, in classify_transaction
    response = chat_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_groq/chat_models.py", line 480, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/resources/chat/completions.py", line 322, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 958, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/groq/_base_client.py", line 1061, in _request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-specdec` in organization `org_01j7b5kwz6esgtrd5ddk4eyxge` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6507, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
2025-02-11 04:10:30 [INFO] Transaction 2024-10-09 00:00:00:
2025-02-11 04:10:30 [INFO] Account Code: ERROR
2025-02-11 04:10:30 [INFO] Reason Preview: Classification failed: Error code: 413 - {'error': {'message': 'Request too...
2025-02-11 04:10:30 [INFO] --------------------------------------------------
2025-02-11 04:10:30 [INFO] Finished processing all transactions
2025-02-11 04:15:10 [INFO] Starting to process 37 transactions
2025-02-11 04:15:10 [INFO] Initializing TransactionClassifier
2025-02-11 04:15:10 [INFO] Processing transaction 2025-01-02 00:00:00
2025-02-11 04:15:10 [INFO] Processing transaction: {'creditorName': 'DVLA', 'remittanceInformationUnstructured': 'DVLA-LN61PYJ  000000000056504532 DDR', 'currency': 'GBP', 'amount': -1312, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:15:10 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-02-11 04:15:10 [ERROR] Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/825659701.py", line 76, in classify_transaction
    response = openai_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py", line 790, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 863, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1283, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 960, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1064, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-02-11 04:15:10 [INFO] Transaction 2025-01-02 00:00:00:
2025-02-11 04:15:10 [INFO] Account Code: ERROR
2025-02-11 04:15:10 [INFO] Reason Preview: Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API...
2025-02-11 04:15:10 [INFO] --------------------------------------------------
2025-02-11 04:15:10 [INFO] Processing transaction 2025-01-02 00:00:00
2025-02-11 04:15:10 [INFO] Processing transaction: {'creditorName': 'SUMUP *NEW LOOK BA', 'remittanceInformationUnstructured': 'SUMUP *NEW LOOK BA  ON 31 DEC BCC', 'currency': 'GBP', 'amount': -6000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:15:12 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-02-11 04:15:12 [ERROR] Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/825659701.py", line 76, in classify_transaction
    response = openai_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py", line 790, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 863, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1283, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 960, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1064, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-02-11 04:15:12 [INFO] Transaction 2025-01-02 00:00:00:
2025-02-11 04:15:12 [INFO] Account Code: ERROR
2025-02-11 04:15:12 [INFO] Reason Preview: Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API...
2025-02-11 04:15:12 [INFO] --------------------------------------------------
2025-02-11 04:15:12 [INFO] Processing transaction 2025-01-02 00:00:00
2025-02-11 04:15:12 [INFO] Processing transaction: {'creditorName': nan, 'remittanceInformationUnstructured': 'SAINSBURYS BANK Sainsburys Bank 31DEC 21.41 ATM', 'currency': 'GBP', 'amount': -5000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:15:14 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-02-11 04:15:14 [ERROR] Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/825659701.py", line 76, in classify_transaction
    response = openai_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py", line 790, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 863, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1283, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 960, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1064, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-02-11 04:15:14 [INFO] Transaction 2025-01-02 00:00:00:
2025-02-11 04:15:14 [INFO] Account Code: ERROR
2025-02-11 04:15:14 [INFO] Reason Preview: Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API...
2025-02-11 04:15:14 [INFO] --------------------------------------------------
2025-02-11 04:15:14 [INFO] Processing transaction 2024-12-27 00:00:00
2025-02-11 04:15:14 [INFO] Processing transaction: {'creditorName': 'KLARNA*ZOOM COMMUN', 'remittanceInformationUnstructured': 'KLARNA*ZOOM COMMUN  ON 26 DEC BCC', 'currency': 'GBP', 'amount': -1559, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:15:16 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-02-11 04:15:16 [ERROR] Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/825659701.py", line 76, in classify_transaction
    response = openai_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py", line 790, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 863, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1283, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 960, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1064, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-02-11 04:15:16 [INFO] Transaction 2024-12-27 00:00:00:
2025-02-11 04:15:16 [INFO] Account Code: ERROR
2025-02-11 04:15:16 [INFO] Reason Preview: Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API...
2025-02-11 04:15:16 [INFO] --------------------------------------------------
2025-02-11 04:15:16 [INFO] Processing transaction 2024-12-27 00:00:00
2025-02-11 04:15:16 [INFO] Processing transaction: {'creditorName': 'THE HUB DENTAL PRA', 'remittanceInformationUnstructured': 'THE HUB DENTAL PRA  ON 24 DEC BCC', 'currency': 'GBP', 'amount': -5000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:15:18 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-02-11 04:15:18 [ERROR] Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/825659701.py", line 76, in classify_transaction
    response = openai_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py", line 790, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 863, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1283, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 960, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1064, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-02-11 04:15:18 [INFO] Transaction 2024-12-27 00:00:00:
2025-02-11 04:15:18 [INFO] Account Code: ERROR
2025-02-11 04:15:18 [INFO] Reason Preview: Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API...
2025-02-11 04:15:18 [INFO] --------------------------------------------------
2025-02-11 04:15:18 [INFO] Processing transaction 2024-12-24 00:00:00
2025-02-11 04:15:18 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB166643916980508', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB166643916980508 FT', 'currency': 'GBP', 'amount': -599, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:15:20 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-02-11 04:15:20 [ERROR] Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/825659701.py", line 76, in classify_transaction
    response = openai_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py", line 790, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 863, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1283, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 960, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1064, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-02-11 04:15:20 [INFO] Transaction 2024-12-24 00:00:00:
2025-02-11 04:15:20 [INFO] Account Code: ERROR
2025-02-11 04:15:20 [INFO] Reason Preview: Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API...
2025-02-11 04:15:20 [INFO] --------------------------------------------------
2025-02-11 04:15:20 [INFO] Processing transaction 2024-12-24 00:00:00
2025-02-11 04:15:20 [INFO] Processing transaction: {'creditorName': 'AMERICAN EXP 3773  PB313981532278988', 'remittanceInformationUnstructured': 'AMERICAN EXP 3773  PB313981532278988 FT', 'currency': 'GBP', 'amount': -21320, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:15:22 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-02-11 04:15:22 [ERROR] Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/825659701.py", line 76, in classify_transaction
    response = openai_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py", line 790, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 863, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1283, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 960, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1064, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-02-11 04:15:22 [INFO] Transaction 2024-12-24 00:00:00:
2025-02-11 04:15:22 [INFO] Account Code: ERROR
2025-02-11 04:15:22 [INFO] Reason Preview: Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API...
2025-02-11 04:15:22 [INFO] --------------------------------------------------
2025-02-11 04:15:22 [INFO] Processing transaction 2024-12-24 00:00:00
2025-02-11 04:15:22 [INFO] Processing transaction: {'creditorName': 'CRESCENT ADVISORS  MICHAEL ALI DLA', 'remittanceInformationUnstructured': 'CRESCENT ADVISORS  MICHAEL ALI DLA FT', 'currency': 'GBP', 'amount': -30000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:15:24 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-02-11 04:15:24 [ERROR] Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/825659701.py", line 76, in classify_transaction
    response = openai_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py", line 790, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 863, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1283, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 960, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1064, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-02-11 04:15:24 [INFO] Transaction 2024-12-24 00:00:00:
2025-02-11 04:15:24 [INFO] Account Code: ERROR
2025-02-11 04:15:24 [INFO] Reason Preview: Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API...
2025-02-11 04:15:24 [INFO] --------------------------------------------------
2025-02-11 04:15:24 [INFO] Processing transaction 2024-12-24 00:00:00
2025-02-11 04:15:24 [INFO] Processing transaction: {'creditorName': 'Barclaycard', 'remittanceInformationUnstructured': 'MR MICHAEL ALI  4929158426786006 BBP', 'currency': 'GBP', 'amount': -14538, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:15:26 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-02-11 04:15:26 [ERROR] Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/825659701.py", line 76, in classify_transaction
    response = openai_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py", line 790, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 863, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1283, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 960, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1064, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-02-11 04:15:26 [INFO] Transaction 2024-12-24 00:00:00:
2025-02-11 04:15:26 [INFO] Account Code: ERROR
2025-02-11 04:15:26 [INFO] Reason Preview: Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API...
2025-02-11 04:15:26 [INFO] --------------------------------------------------
2025-02-11 04:15:26 [INFO] Processing transaction 2024-12-18 00:00:00
2025-02-11 04:15:26 [INFO] Processing transaction: {'creditorName': 'AA', 'remittanceInformationUnstructured': 'AA MEMBERSHIP  6356011528897956 DDR', 'currency': 'GBP', 'amount': -1641, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:15:28 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-02-11 04:15:28 [ERROR] Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/825659701.py", line 76, in classify_transaction
    response = openai_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py", line 790, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 863, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1283, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 960, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1064, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-02-11 04:15:28 [INFO] Transaction 2024-12-18 00:00:00:
2025-02-11 04:15:28 [INFO] Account Code: ERROR
2025-02-11 04:15:28 [INFO] Reason Preview: Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API...
2025-02-11 04:15:28 [INFO] --------------------------------------------------
2025-02-11 04:15:28 [INFO] Processing transaction 2024-12-17 00:00:00
2025-02-11 04:15:28 [INFO] Processing transaction: {'creditorName': 'FIRECRAWL.DEV USA AMOUNT IN USD 19.00', 'remittanceInformationUnstructured': 'FIRECRAWL.DEV USA AMOUNT IN USD 19.00 ON 16 DEC VISA 1.2605 FINAL GBP AMOUNT INCLUDES NON-STERLING TRANS FEE £0.45 ', 'currency': 'GBP', 'amount': -1552, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:15:30 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-02-11 04:15:30 [ERROR] Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/825659701.py", line 76, in classify_transaction
    response = openai_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py", line 790, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 863, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1283, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 960, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1064, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-02-11 04:15:30 [INFO] Transaction 2024-12-17 00:00:00:
2025-02-11 04:15:30 [INFO] Account Code: ERROR
2025-02-11 04:15:30 [INFO] Reason Preview: Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API...
2025-02-11 04:15:30 [INFO] --------------------------------------------------
2025-02-11 04:15:30 [INFO] Processing transaction 2024-12-17 00:00:00
2025-02-11 04:15:30 [INFO] Processing transaction: {'creditorName': 'Admiral', 'remittanceInformationUnstructured': 'ADMIRAL INSURANCE  P73244643010000011 DDR', 'currency': 'GBP', 'amount': -11313, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:16:14 [INFO] Starting to process 37 transactions
2025-02-11 04:16:14 [INFO] Initializing TransactionClassifier
2025-02-11 04:16:14 [INFO] Processing transaction 2025-01-02 00:00:00
2025-02-11 04:16:14 [INFO] Processing transaction: {'creditorName': 'DVLA', 'remittanceInformationUnstructured': 'DVLA-LN61PYJ  000000000056504532 DDR', 'currency': 'GBP', 'amount': -1312, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:16:15 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-02-11 04:16:15 [ERROR] Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/2311476061.py", line 76, in classify_transaction
    response = openai_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py", line 790, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 863, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1283, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 960, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1064, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-02-11 04:16:15 [INFO] Transaction 2025-01-02 00:00:00:
2025-02-11 04:16:15 [INFO] Account Code: ERROR
2025-02-11 04:16:15 [INFO] Reason Preview: Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API...
2025-02-11 04:16:15 [INFO] --------------------------------------------------
2025-02-11 04:16:15 [INFO] Processing transaction 2025-01-02 00:00:00
2025-02-11 04:16:15 [INFO] Processing transaction: {'creditorName': 'SUMUP *NEW LOOK BA', 'remittanceInformationUnstructured': 'SUMUP *NEW LOOK BA  ON 31 DEC BCC', 'currency': 'GBP', 'amount': -6000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:16:16 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-02-11 04:16:16 [ERROR] Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/2311476061.py", line 76, in classify_transaction
    response = openai_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py", line 790, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 863, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1283, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 960, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1064, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-02-11 04:16:16 [INFO] Transaction 2025-01-02 00:00:00:
2025-02-11 04:16:16 [INFO] Account Code: ERROR
2025-02-11 04:16:16 [INFO] Reason Preview: Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API...
2025-02-11 04:16:16 [INFO] --------------------------------------------------
2025-02-11 04:16:16 [INFO] Processing transaction 2025-01-02 00:00:00
2025-02-11 04:16:16 [INFO] Processing transaction: {'creditorName': nan, 'remittanceInformationUnstructured': 'SAINSBURYS BANK Sainsburys Bank 31DEC 21.41 ATM', 'currency': 'GBP', 'amount': -5000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:16:18 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-02-11 04:16:18 [ERROR] Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/2311476061.py", line 76, in classify_transaction
    response = openai_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py", line 790, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 863, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1283, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 960, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1064, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-02-11 04:16:18 [INFO] Transaction 2025-01-02 00:00:00:
2025-02-11 04:16:18 [INFO] Account Code: ERROR
2025-02-11 04:16:18 [INFO] Reason Preview: Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API...
2025-02-11 04:16:18 [INFO] --------------------------------------------------
2025-02-11 04:16:18 [INFO] Processing transaction 2024-12-27 00:00:00
2025-02-11 04:16:18 [INFO] Processing transaction: {'creditorName': 'KLARNA*ZOOM COMMUN', 'remittanceInformationUnstructured': 'KLARNA*ZOOM COMMUN  ON 26 DEC BCC', 'currency': 'GBP', 'amount': -1559, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:16:20 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-02-11 04:16:20 [ERROR] Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/2311476061.py", line 76, in classify_transaction
    response = openai_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py", line 790, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 863, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1283, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 960, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1064, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-02-11 04:16:20 [INFO] Transaction 2024-12-27 00:00:00:
2025-02-11 04:16:20 [INFO] Account Code: ERROR
2025-02-11 04:16:20 [INFO] Reason Preview: Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API...
2025-02-11 04:16:20 [INFO] --------------------------------------------------
2025-02-11 04:16:20 [INFO] Processing transaction 2024-12-27 00:00:00
2025-02-11 04:16:20 [INFO] Processing transaction: {'creditorName': 'THE HUB DENTAL PRA', 'remittanceInformationUnstructured': 'THE HUB DENTAL PRA  ON 24 DEC BCC', 'currency': 'GBP', 'amount': -5000, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:16:46 [INFO] Starting to process 37 transactions
2025-02-11 04:16:46 [INFO] Initializing TransactionClassifier
2025-02-11 04:16:46 [INFO] Processing transaction 2025-01-02 00:00:00
2025-02-11 04:16:46 [INFO] Processing transaction: {'creditorName': 'DVLA', 'remittanceInformationUnstructured': 'DVLA-LN61PYJ  000000000056504532 DDR', 'currency': 'GBP', 'amount': -1312, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:16:46 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-02-11 04:16:46 [ERROR] Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1388853697.py", line 76, in classify_transaction
    response = openai_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py", line 790, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 863, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1283, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 960, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1064, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-02-11 04:16:46 [INFO] Transaction 2025-01-02 00:00:00:
2025-02-11 04:16:46 [INFO] Account Code: ERROR
2025-02-11 04:16:46 [INFO] Reason Preview: Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API...
2025-02-11 04:16:46 [INFO] --------------------------------------------------
2025-02-11 04:16:46 [INFO] Finished processing all transactions
2025-02-11 04:17:30 [INFO] Starting to process 37 transactions
2025-02-11 04:17:30 [INFO] Initializing TransactionClassifier
2025-02-11 04:17:30 [INFO] Processing transaction 2025-01-02 00:00:00
2025-02-11 04:17:30 [INFO] Processing transaction: {'creditorName': 'DVLA', 'remittanceInformationUnstructured': 'DVLA-LN61PYJ  000000000056504532 DDR', 'currency': 'GBP', 'amount': -1312, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:17:30 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-02-11 04:17:30 [ERROR] Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1355351156.py", line 76, in classify_transaction
    response = openai_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py", line 790, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 863, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1283, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 960, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1064, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-02-11 04:17:30 [INFO] Transaction 2025-01-02 00:00:00:
2025-02-11 04:17:30 [INFO] Account Code: ERROR
2025-02-11 04:17:30 [INFO] Reason Preview: Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API...
2025-02-11 04:17:30 [INFO] --------------------------------------------------
2025-02-11 04:17:30 [INFO] Finished processing all transactions
2025-02-11 04:18:05 [INFO] Starting to process 37 transactions
2025-02-11 04:18:05 [INFO] Initializing TransactionClassifier
2025-02-11 04:18:05 [INFO] Processing transaction 2025-01-02 00:00:00
2025-02-11 04:18:05 [INFO] Processing transaction: {'creditorName': 'DVLA', 'remittanceInformationUnstructured': 'DVLA-LN61PYJ  000000000056504532 DDR', 'currency': 'GBP', 'amount': -1312, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:18:05 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-02-11 04:18:05 [ERROR] Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/3536272148.py", line 75, in classify_transaction
    response = openai_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py", line 790, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 863, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1283, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 960, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1064, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-02-11 04:18:05 [INFO] Transaction 2025-01-02 00:00:00:
2025-02-11 04:18:05 [INFO] Account Code: ERROR
2025-02-11 04:18:05 [INFO] Reason Preview: Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API...
2025-02-11 04:18:05 [INFO] --------------------------------------------------
2025-02-11 04:18:05 [INFO] Finished processing all transactions
2025-02-11 04:19:14 [INFO] Starting to process 37 transactions
2025-02-11 04:19:14 [INFO] Initializing TransactionClassifier
2025-02-11 04:19:14 [INFO] Processing transaction 2025-01-02 00:00:00
2025-02-11 04:19:14 [INFO] Processing transaction: {'creditorName': 'DVLA', 'remittanceInformationUnstructured': 'DVLA-LN61PYJ  000000000056504532 DDR', 'currency': 'GBP', 'amount': -1312, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:19:15 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-02-11 04:19:15 [ERROR] Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/1355351156.py", line 76, in classify_transaction
    response = openai_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py", line 790, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 863, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1283, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 960, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1064, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-02-11 04:19:15 [INFO] Transaction 2025-01-02 00:00:00:
2025-02-11 04:19:15 [INFO] Account Code: ERROR
2025-02-11 04:19:15 [INFO] Reason Preview: Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API...
2025-02-11 04:19:15 [INFO] --------------------------------------------------
2025-02-11 04:19:15 [INFO] Finished processing all transactions
2025-02-11 04:21:25 [INFO] Starting to process 37 transactions
2025-02-11 04:21:25 [INFO] Initializing TransactionClassifier
2025-02-11 04:21:25 [INFO] Processing transaction 2025-01-02 00:00:00
2025-02-11 04:21:25 [INFO] Processing transaction: {'creditorName': 'DVLA', 'remittanceInformationUnstructured': 'DVLA-LN61PYJ  000000000056504532 DDR', 'currency': 'GBP', 'amount': -1312, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 04:21:25 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-02-11 04:21:25 [ERROR] Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "/var/folders/_y/xmd2krqs2lb6gkj69wkq3vp00000gn/T/ipykernel_42477/2108133928.py", line 78, in classify_transaction
    response = openai_model.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 284, in invoke
    self.generate_prompt(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 860, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 690, in generate
    self._generate_with_cache(
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py", line 925, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py", line 790, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 863, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1283, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 960, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Users/michaelali/Repos/bankstream.io/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1064, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************fIcA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
2025-02-11 04:21:25 [INFO] Transaction 2025-01-02 00:00:00:
2025-02-11 04:21:25 [INFO] Account Code: ERROR
2025-02-11 04:21:25 [INFO] Reason Preview: Classification failed: Error code: 401 - {'error': {'message': 'Incorrect API...
2025-02-11 04:21:25 [INFO] --------------------------------------------------
2025-02-11 04:21:25 [INFO] Finished processing all transactions
2025-02-11 04:22:30 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-02-11 04:24:03 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-02-11 04:24:22 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-02-11 16:29:31 [INFO] Starting to process 37 transactions
2025-02-11 16:29:31 [INFO] Initializing TransactionClassifier
2025-02-11 16:29:31 [INFO] Processing transaction 2025-01-02 00:00:00
2025-02-11 16:29:31 [INFO] Processing transaction: {'creditorName': 'DVLA', 'remittanceInformationUnstructured': 'DVLA-LN61PYJ  000000000056504532 DDR', 'currency': 'GBP', 'amount': -13.12, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 16:49:57 [INFO] Starting to process 37 transactions
2025-02-11 16:49:57 [INFO] Initializing TransactionClassifier
2025-02-11 16:49:57 [INFO] Processing transaction 2025-01-02 00:00:00
2025-02-11 16:49:57 [INFO] Processing transaction: {'creditorName': 'DVLA', 'remittanceInformationUnstructured': 'DVLA-LN61PYJ  000000000056504532 DDR', 'currency': 'GBP', 'amount': -13.12, 'coa_agent': None, 'coa_reason': None, 'coa_confidence': None}
2025-02-11 16:50:00 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-02-11 16:50:00 [WARNING] Direct JSON parsing failed, attempting to extract JSON from response
2025-02-11 16:50:00 [INFO] Successfully extracted and parsed JSON from response
2025-02-11 16:50:00 [INFO] Classification successful: b04da64d-1580-4dd4-9de4-ac77130087d0
2025-02-11 16:50:00 [INFO] Transaction 2025-01-02 00:00:00:
2025-02-11 16:50:00 [INFO] Account Code: b04da64d-1580-4dd4-9de4-ac77130087d0
2025-02-11 16:50:00 [INFO] Reason Preview: The transaction is associated with DVLA, which typically indicates a...
2025-02-11 16:50:00 [INFO] --------------------------------------------------
2025-02-11 16:50:00 [INFO] Number of processed transactions: 1
2025-02-11 16:50:00 [INFO] Finished processing all transactions
