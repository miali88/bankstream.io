{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simi Search with CoA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:app.services.vectorise_data:Requesting embedding from Jina AI\n",
      "DEBUG:app.services.vectorise_data:Successfully received embedding from Jina AI\n"
     ]
    }
   ],
   "source": [
    "from app.services.vectorise_data import get_embedding\n",
    "\n",
    "response = await get_embedding(\"salary payment to developers\", 'retrieval.query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import voyageai\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from typing import Literal\n",
    "load_dotenv()\n",
    "\n",
    "voyage_api_key = os.getenv(\"VOYAGE_API_KEY\")\n",
    "\n",
    "client = voyageai.Client(api_key=voyage_api_key)\n",
    "input_type = Literal[\"document\", \"query\"]\n",
    "\n",
    "response = client.embed(\n",
    "    texts=\"salary payment to developers\",\n",
    "    model=\"voyage-finance-2\",\n",
    "    input_type=\"document\"\n",
    ")\n",
    "response.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.services.xero import vectorise_coa\n",
    "\n",
    "await vectorise_coa(\"ALL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import bank data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandasai as pai\n",
    "import pandas as pd\n",
    "import ast\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# pai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# if not pai_api_key:\n",
    "#     raise ValueError(\"PAI_API_KEY is not set\")\n",
    "# else:\n",
    "#     print(f\"OPENAI_API_KEY is set\")\n",
    "\n",
    "df = pd.read_csv(\"bank_data_23-25/barclays_072.csv\")\n",
    "\n",
    "# # Sample DataFrame\n",
    "# df = pai.DataFrame(df)\n",
    "\n",
    "# pai.api_key.set(pai_api_key)\n",
    "\n",
    "\n",
    "### extract currency and amount\n",
    "df['currency'] = df['transactionAmount'].apply(lambda x: ast.literal_eval(x)['currency'])\n",
    "df['amount'] = df['transactionAmount'].apply(lambda x: float(ast.literal_eval(x)['amount']))\n",
    "df['amount'] = (df['amount'] * 100).astype(int)\n",
    "df = df.drop('transactionAmount', axis=1)\n",
    "\n",
    "## setting datetimeindex\n",
    "df['bookingDate'] = pd.to_datetime(df['bookingDate'])\n",
    "df.set_index('bookingDate', inplace=True)\n",
    "df = df.drop(['valueDate', 'bookingDateTime', 'valueDateTime', 'internalTransactionId'], axis=1)\n",
    "df = df.rename(columns={'remittanceInformationUnstructured': 'remittanceInfo'})\n",
    "\n",
    "# df.chat(\"What are the total expenses for 2024?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Reconciliation Time\n",
    "\n",
    "- Add new columns \"coa_agent\", \"coa_reason\", \"coa_agent_confidence\" for LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from typing import Dict, Tuple\n",
    "import logging\n",
    "import json\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the ChatGroq model\n",
    "groq_model = ChatGroq(model_name=\"Llama-3.3-70b-Specdec\")\n",
    "openai_model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "\n",
    "class TransactionClassifier:\n",
    "    def __init__(self):\n",
    "        logger.info(\"Initializing TransactionClassifier\")\n",
    "        self.system_prompt = \"\"\"\n",
    "        You are a helpful financial expert responsible for classifying transactions.\n",
    "        You will be given some transactions and a list of chart of accounts to reconcile. \n",
    "\n",
    "        For each transaction, you must provide:\n",
    "        1. The most appropriate chart of account\n",
    "        2. A brief explanation of why this classification was chosen. If confidence score is low, explain why. Take hints from the description of the transaction, look out for entity names, and infer the purpose of the transaction.\n",
    "        3. A confidence score between 0 and 1 (e.g., 0.95 for high confidence)\n",
    "\n",
    "        # IMPORTANT: Instructions for your response:\n",
    "        You must respond with valid JSON in the following format only:\n",
    "        {\n",
    "            \"account\": \"string\",\n",
    "            \"reasoning\": \"string\",\n",
    "            \"confidence\": float\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.last_request_time = 0\n",
    "        self.rate_limit_delay = 2  # 2 seconds between requests (30 requests/minute)\n",
    "\n",
    "    def _rate_limit(self):\n",
    "        \"\"\"Implement rate limiting\"\"\"\n",
    "        current_time = time.time()\n",
    "        time_since_last_request = current_time - self.last_request_time\n",
    "        if time_since_last_request < self.rate_limit_delay:\n",
    "            delay = self.rate_limit_delay - time_since_last_request\n",
    "            logger.debug(f\"Rate limiting: waiting {delay:.2f} seconds\")\n",
    "            time.sleep(delay)\n",
    "        self.last_request_time = time.time() \n",
    "\n",
    "    def classify_transaction(self, transaction: Dict, chart_of_accounts: list) -> Tuple[str, str, float]:\n",
    "        \"\"\"Classify a single transaction using the LLM\"\"\"\n",
    "        logger.info(f\"Processing transaction: {transaction}\")\n",
    "        self._rate_limit()\n",
    "\n",
    "        # Format the transaction details for the LLM\n",
    "        transaction_prompt = f\"\"\"\n",
    "        Please classify the following transaction:\n",
    "        Transaction: {transaction}\n",
    "        \n",
    "        chart of accounts: {chart_of_accounts}\n",
    "        \"\"\"\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=transaction_prompt)\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            logger.debug(\"Sending request to LLM\")\n",
    "            # Add more visible print statements\n",
    "            print(\"\\n=== Sending request to LLM ===\")\n",
    "            response = openai_model.invoke(messages)\n",
    "            print(\"\\n=== Raw LLM Response ===\")\n",
    "            print(response.content)\n",
    "            logger.debug(f\"Raw LLM response: {response.content}\")\n",
    "\n",
    "            # Try to parse the response as JSON\n",
    "            try:\n",
    "                # First, try direct JSON parsing\n",
    "                result = json.loads(response.content)\n",
    "                logger.info(\"Successfully parsed JSON response\")\n",
    "            except json.JSONDecodeError:\n",
    "                # If direct parsing fails, try to extract JSON from the response\n",
    "                logger.warning(\"Direct JSON parsing failed, attempting to extract JSON from response\")\n",
    "                # Look for JSON-like structure in the response\n",
    "                import re\n",
    "                json_match = re.search(r'\\{.*\\}', response.content, re.DOTALL)\n",
    "                if json_match:\n",
    "                    result = json.loads(json_match.group())\n",
    "                    logger.info(\"Successfully extracted and parsed JSON from response\")\n",
    "                else:\n",
    "                    raise ValueError(\"No JSON structure found in response\")\n",
    "\n",
    "            # Validate the response structure\n",
    "            required_keys = {'account', 'reasoning', 'confidence'}\n",
    "            if not all(key in result for key in required_keys):\n",
    "                missing_keys = required_keys - result.keys()\n",
    "                raise ValueError(f\"Missing required keys in response: {missing_keys}\")\n",
    "\n",
    "            logger.info(f\"Classification successful: {result['account']}\")\n",
    "            return (\n",
    "                result['account'],\n",
    "                result['reasoning'],\n",
    "                result['confidence']\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Classification failed: {str(e)}\", exc_info=True)\n",
    "            return (\n",
    "                \"ERROR\",\n",
    "                f\"Classification failed: {str(e)}\",\n",
    "                0.0\n",
    "            )\n",
    "\n",
    "\n",
    "def process_transactions(df: pd.DataFrame, chart_of_accounts: list) -> pd.DataFrame:\n",
    "    \"\"\"Process transactions in DataFrame and add classifications directly\"\"\"\n",
    "    logger.info(f\"Starting to process {len(df)} transactions\")\n",
    "    classifier = TransactionClassifier()\n",
    "\n",
    "    # Create a fresh copy of the DataFrame\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Explicitly clear any previous results\n",
    "    df['coa_agent'] = None\n",
    "    df['coa_reason'] = None\n",
    "    df['coa_confidence'] = None\n",
    "\n",
    "    for i, (idx, row) in enumerate(df.iterrows()):\n",
    "        print(\"row\", row)\n",
    "        logger.info(f\"Processing transaction {idx}\")\n",
    "        transaction = row.to_dict()\n",
    "        transaction['amount'] = transaction['amount']/100\n",
    "        account, reasoning, confidence = classifier.classify_transaction(transaction, chart_of_accounts)\n",
    "        \n",
    "        # Update DataFrame using iloc instead of at\n",
    "        df.iloc[i, df.columns.get_loc('coa_agent')] = account\n",
    "        df.iloc[i, df.columns.get_loc('coa_reason')] = reasoning\n",
    "        df.iloc[i, df.columns.get_loc('coa_confidence')] = confidence\n",
    "        \n",
    "        # Print summary of classification\n",
    "        reason_preview = ' '.join(reasoning.split()[:10]) + '...' if reasoning else 'No reason provided'\n",
    "        logger.info(f\"Transaction {idx}:\")\n",
    "        logger.info(f\"Account: {account}\")\n",
    "        logger.info(f\"Reason Preview: {reason_preview}\")\n",
    "        logger.info(\"-\" * 50)\n",
    "\n",
    "    \n",
    "    # Add verification logging\n",
    "    processed_count = df[df['coa_agent'].notna()].shape[0]\n",
    "    logger.info(f\"Number of processed transactions: {processed_count}\")\n",
    "    \n",
    "    logger.info(\"Finished processing all transactions\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\"\"\" Fetch and store chart of accounts into parsed_accounts \"\"\"\n",
    "import json\n",
    "\n",
    "# Read the JSON file\n",
    "with open('coa.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract only the required fields from each account\n",
    "parsed_accounts = []\n",
    "for account in data['Accounts']:\n",
    "    if account.get('Status') == 'ACTIVE':\n",
    "        parsed_account = {\n",
    "            'code': account.get('Code', ''),\n",
    "            'name': account.get('Name', ''),\n",
    "            'type': account.get('Type', ''),\n",
    "            'description': account.get('Description', ''),  # Note: Not present in sample but included as requested\n",
    "            'class': account.get('Class', ''),\n",
    "    }\n",
    "    parsed_accounts.append(parsed_account)\n",
    "\n",
    "transactions = []  # Create empty list to store dictionaries\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    transaction = row.to_dict()\n",
    "    transactions.append(transaction)\n",
    "\n",
    "df_processed = process_transactions(df.copy(), parsed_accounts)\n",
    "\n",
    "df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "\n",
    "from app.services.supabase import get_supabase\n",
    "supabase = await get_supabase()\n",
    "\n",
    "class TransactionToLLM(BaseModel):\n",
    "    entity_name : str\n",
    "    amount : float\n",
    "    remittance_info : str\n",
    "    ntropy_enrich : bool\n",
    "    ntropy_entity : str\n",
    "    ntropy_category : str\n",
    "\n",
    "\n",
    "\n",
    "# Query both tables\n",
    "ntropy_response = await supabase.table('ntropy_transactions').select('*').execute()\n",
    "gocardless_response = await supabase.table('gocardless_transactions').select('*').execute()\n",
    "\n",
    "# Convert to DataFrames\n",
    "ntropy_df = pd.DataFrame(ntropy_response.data)\n",
    "gocardless_df = pd.DataFrame(gocardless_response.data)\n",
    "\n",
    "# Merge DataFrames on the specified keys\n",
    "merged_df = pd.merge(\n",
    "    gocardless_df,\n",
    "    ntropy_df,\n",
    "    how='left',\n",
    "    left_on='id',\n",
    "    right_on='ntropy_id'\n",
    ")\n",
    "\n",
    "# Create a new DataFrame with selected columns\n",
    "result_df = pd.DataFrame({\n",
    "    'entity_name': merged_df['enriched_data'].apply(lambda x: x['entities']['counterparty']['name'] if pd.notnull(x) else None),\n",
    "    'amount': merged_df['amount']/100,\n",
    "    'remittance_info': merged_df['remittance_info'],\n",
    "    'ntropy_enrich': merged_df['enriched_data'].notnull(),\n",
    "    'ntropy_entity': merged_df['enriched_data'].apply(lambda x: x['entities']['counterparty']['name'] if pd.notnull(x) else None),\n",
    "    'ntropy_category': merged_df['enriched_data'].apply(lambda x: x['categories']['general'] if pd.notnull(x) else None)\n",
    "})\n",
    "\n",
    "# Print or return the result\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" NTROPY \"\n",
    "import requests\n",
    "\n",
    "ntropy_api_key = os.getenv(\"NTROPY_API_KEY\")\n",
    "if not ntropy_api_key:\n",
    "    raise ValueError(\"NTROPY_API_KEY is not set\")\n",
    "else:\n",
    "    print(f\"NTROPY_API_KEY is set\")\n",
    "\n",
    "\"\"\" CREATE NEW ACCOUNT HOLDER \"\"\"\n",
    "url = \"https://api.ntropy.com/v3/account_holders\"\n",
    "headers = {\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"X-API-KEY\": ntropy_api_key,\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"id\": \"35b927b6-6fda-40aa-93b8-95b47c2b2cad\",\n",
    "    \"type\": \"business\",\n",
    "    \"name\": \"Michael Ali\",\n",
    "    \"website\": \"https://flowon.ai\",\n",
    "    \"industry\": \"ai software\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" NTROPY BATCH PROCESS TRANSACTIONS\"\"\"\n",
    "import uuid\n",
    "\n",
    "url = \"https://api.ntropy.com/v3/batches/\"\n",
    "\n",
    "\n",
    "data = {\n",
    "        \"operation\": \"POST /v3/transactions\",\n",
    "        \"data\": transformed_data\n",
    "    }\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" NOW CHECK TRANSACTION STATUS \"\"\"\n",
    "batch_id = \"1a2bc613-111b-49b1-b35c-77e9b1d7a2fc\"\n",
    "\n",
    "url = f\"https://api.ntropy.com/v3/batches/{batch_id}/results\"\n",
    "\n",
    "\n",
    "get_batch = requests.get(url, headers=headers)\n",
    "\n",
    "get_batch.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_batch.json()['results'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transaction_id = \"1177539c-b570-4588-9953-d76ae4647afb\"\n",
    "\n",
    "url = f\"https://api.ntropy.com/v3/transactions/{transaction_id}\"\n",
    "\n",
    "get_transaction = requests.get(url, headers=headers)\n",
    "\n",
    "get_transaction.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "\"\"\" DF TO JSON FIELDS \"\"\"\n",
    "# Convert DataFrame to JSON\n",
    "def prepare_df_for_frontend(df):\n",
    "    # Reset index to make bookingDate a column\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # Convert datetime to ISO format string\n",
    "    df['bookingDate'] = df['bookingDate'].dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    \n",
    "    # Convert to JSON records format (this gives us a string)\n",
    "    json_string = df.to_json(orient='records', date_format='iso')\n",
    "    \n",
    "    # Parse the JSON string into Python objects (list of dictionaries)\n",
    "    json_data = json.loads(json_string)\n",
    "    \n",
    "    return json_data\n",
    "\n",
    "json_data = prepare_df_for_frontend(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid \n",
    "\"\"\"  \"\"\"\n",
    "def transform_transaction(transaction, account_holder_id):\n",
    "    # Transform a single transaction\n",
    "    return {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"description\": transaction[\"remittanceInfo\"],\n",
    "        \"date\": transaction[\"bookingDate\"].split(\"T\")[0],\n",
    "        \"amount\": abs(transaction[\"amount\"]/100),  # Make amount positive\n",
    "        \"entry_type\": \"outgoing\" if transaction[\"amount\"] < 0 else \"incoming\",\n",
    "        \"currency\": transaction[\"currency\"],\n",
    "        \"account_holder_id\": account_holder_id,\n",
    "        \"location\": {\n",
    "            \"country\": \"GB\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "account_holder_id = \"35b927b6-6fda-40aa-93b8-95b47c2b2cad\"\n",
    "# Transform all transactions using list comprehension\n",
    "transformed_data = [\n",
    "    transform_transaction(transaction, account_holder_id) \n",
    "    for transaction in json_data\n",
    "]\n",
    "\n",
    "transformed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To evaluate reponses of LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.to_csv(\"transactions_ai_reconciled.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
