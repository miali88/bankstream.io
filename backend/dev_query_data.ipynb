{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simi Search with CoA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.services.vectorise_data import get_embedding\n",
    "\n",
    "response = await get_embedding(\"salary payment to developers\", 'retrieval.query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import voyageai\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from typing import Literal\n",
    "load_dotenv()\n",
    "\n",
    "voyage_api_key = os.getenv(\"VOYAGE_API_KEY\")\n",
    "\n",
    "client = voyageai.Client(api_key=voyage_api_key)\n",
    "input_type = Literal[\"document\", \"query\"]\n",
    "\n",
    "response = client.embed(\n",
    "    texts=\"salary payment to developers\",\n",
    "    model=\"voyage-finance-2\",\n",
    "    input_type=\"document\"\n",
    ")\n",
    "response.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.services.xero import vectorise_coa\n",
    "\n",
    "await vectorise_coa(\"ALL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import bank data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandasai as pai\n",
    "import pandas as pd\n",
    "import ast\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# pai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# if not pai_api_key:\n",
    "#     raise ValueError(\"PAI_API_KEY is not set\")\n",
    "# else:\n",
    "#     print(f\"OPENAI_API_KEY is set\")\n",
    "\n",
    "df = pd.read_csv(\"bank_data_23-25/barclays_072.csv\")\n",
    "\n",
    "# # Sample DataFrame\n",
    "# df = pai.DataFrame(df)\n",
    "\n",
    "# pai.api_key.set(pai_api_key)\n",
    "\n",
    "\n",
    "### extract currency and amount\n",
    "df['currency'] = df['transactionAmount'].apply(lambda x: ast.literal_eval(x)['currency'])\n",
    "df['amount'] = df['transactionAmount'].apply(lambda x: float(ast.literal_eval(x)['amount']))\n",
    "df['amount'] = (df['amount'] * 100).astype(int)\n",
    "df = df.drop('transactionAmount', axis=1)\n",
    "\n",
    "## setting datetimeindex\n",
    "df['bookingDate'] = pd.to_datetime(df['bookingDate'])\n",
    "df.set_index('bookingDate', inplace=True)\n",
    "df = df.drop(['valueDate', 'bookingDateTime', 'valueDateTime', 'internalTransactionId'], axis=1)\n",
    "df = df.rename(columns={'remittanceInformationUnstructured': 'remittanceInfo'})\n",
    "\n",
    "# df.chat(\"What are the total expenses for 2024?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Reconciliation Time\n",
    "\n",
    "- Add new columns \"coa_agent\", \"coa_reason\", \"coa_agent_confidence\" for LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" classifiy transaction belonging to TransactionClassifier \"\"\"\n",
    "def classify_transaction(self, transaction: Dict, chart_of_accounts: list) -> Tuple[str, str, float]:\n",
    "    \"\"\"Classify a single transaction using the LLM\"\"\"\n",
    "    logger.info(f\"Processing transaction: {transaction}\")\n",
    "    self._rate_limit()\n",
    "\n",
    "    # Format the transaction details for the LLM\n",
    "    transaction_prompt = f\"\"\"\n",
    "    Please classify the following transaction:\n",
    "    Transaction: {transaction}\n",
    "    \n",
    "    chart of accounts: {chart_of_accounts}\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=self.system_prompt),\n",
    "        HumanMessage(content=transaction_prompt)\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        logger.debug(\"Sending request to LLM\")\n",
    "        # Add more visible print statements\n",
    "        print(\"\\n=== Sending request to LLM ===\")\n",
    "        response = openai_model.invoke(messages)\n",
    "        print(\"\\n=== Raw LLM Response ===\")\n",
    "        print(response.content)\n",
    "        logger.debug(f\"Raw LLM response: {response.content}\")\n",
    "\n",
    "        # Try to parse the response as JSON\n",
    "        try:\n",
    "            # First, try direct JSON parsing\n",
    "            result = json.loads(response.content)\n",
    "            logger.info(\"Successfully parsed JSON response\")\n",
    "        except json.JSONDecodeError:\n",
    "            # If direct parsing fails, try to extract JSON from the response\n",
    "            logger.warning(\"Direct JSON parsing failed, attempting to extract JSON from response\")\n",
    "            # Look for JSON-like structure in the response\n",
    "            import re\n",
    "            json_match = re.search(r'\\{.*\\}', response.content, re.DOTALL)\n",
    "            if json_match:\n",
    "                result = json.loads(json_match.group())\n",
    "                logger.info(\"Successfully extracted and parsed JSON from response\")\n",
    "            else:\n",
    "                raise ValueError(\"No JSON structure found in response\")\n",
    "\n",
    "        # Validate the response structure\n",
    "        required_keys = {'account', 'reasoning', 'confidence'}\n",
    "        if not all(key in result for key in required_keys):\n",
    "            missing_keys = required_keys - result.keys()\n",
    "            raise ValueError(f\"Missing required keys in response: {missing_keys}\")\n",
    "\n",
    "        logger.info(f\"Classification successful: {result['account']}\")\n",
    "        return (\n",
    "            result['account'],\n",
    "            result['reasoning'],\n",
    "            result['confidence']\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Classification failed: {str(e)}\", exc_info=True)\n",
    "        return (\n",
    "            \"ERROR\",\n",
    "            f\"Classification failed: {str(e)}\",\n",
    "            0.0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 15:56:34,675 - INFO - HTTP Request: GET https://lvhbpccylcxeehgxtrwv.supabase.co/rest/v1/ntropy_transactions?select=%2A \"HTTP/2 200 OK\"\n",
      "2025-02-18 15:56:34,803 - INFO - HTTP Request: GET https://lvhbpccylcxeehgxtrwv.supabase.co/rest/v1/gocardless_transactions?select=%2A \"HTTP/2 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "\"\"\" fetch transactions + ntropy enrich from supabase \"\"\"\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "\n",
    "from app.services.supabase import get_supabase\n",
    "supabase = await get_supabase()\n",
    "\n",
    "class TransactionToLLM(BaseModel):\n",
    "    entity_name : str\n",
    "    amount : float\n",
    "    remittance_info : str\n",
    "    ntropy_enrich : bool\n",
    "    ntropy_entity : str\n",
    "    ntropy_category : str\n",
    "\n",
    "# Query both tables\n",
    "ntropy_response = await supabase.table('ntropy_transactions').select('*').execute()\n",
    "gocardless_response = await supabase.table('gocardless_transactions').select('*').execute()\n",
    "\n",
    "# Convert to DataFrames\n",
    "ntropy_df = pd.DataFrame(ntropy_response.data)\n",
    "gocardless_df = pd.DataFrame(gocardless_response.data)\n",
    "\n",
    "# Merge DataFrames on the specified keys\n",
    "merged_df = pd.merge(\n",
    "    gocardless_df,\n",
    "    ntropy_df,\n",
    "    how='left',\n",
    "    left_on='id',\n",
    "    right_on='ntropy_id'\n",
    ")\n",
    "\n",
    "# Create a new DataFrame with selected columns\n",
    "result_df = pd.DataFrame({\n",
    "    'id': merged_df['id'],\n",
    "    'entity_name': merged_df.apply(\n",
    "        lambda row: row['creditor_name'] if pd.notnull(row['creditor_name'])\n",
    "        else row['debtor_name'] if pd.notnull(row['debtor_name'])\n",
    "        else row['enriched_data']['entities']['counterparty']['name'] if pd.notnull(row['enriched_data'])\n",
    "        else None,\n",
    "        axis=1\n",
    "    ),\n",
    "    'amount': merged_df['amount']/100,\n",
    "    'remittance_info': merged_df['remittance_info'],\n",
    "    'ntropy_enrich': merged_df['enriched_data'].notnull(),\n",
    "    'ntropy_entity': merged_df['enriched_data'].apply(lambda x: x['entities']['counterparty']['name'] if pd.notnull(x) else None),\n",
    "    'ntropy_category': merged_df['enriched_data'].apply(lambda x: x['categories']['general'] if pd.notnull(x) else None)\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 16:32:09,204 - INFO - Starting reconciliation process\n",
      "2025-02-18 16:32:09,205 - INFO - Loading chart of accounts from coa.json\n",
      "2025-02-18 16:32:09,206 - INFO - Loaded 83 active accounts from CoA\n",
      "2025-02-18 16:32:09,206 - INFO - Fetching and preparing transactions\n",
      "2025-02-18 16:32:09,207 - INFO - Starting to fetch transactions from Supabase\n",
      "2025-02-18 16:32:09,207 - INFO - Successfully connected to Supabase\n",
      "2025-02-18 16:32:09,207 - INFO - Querying Supabase tables...\n",
      "2025-02-18 16:32:09,301 - INFO - HTTP Request: GET https://lvhbpccylcxeehgxtrwv.supabase.co/rest/v1/ntropy_transactions?select=%2A \"HTTP/2 200 OK\"\n",
      "2025-02-18 16:32:09,370 - INFO - HTTP Request: GET https://lvhbpccylcxeehgxtrwv.supabase.co/rest/v1/gocardless_transactions?select=%2A \"HTTP/2 200 OK\"\n",
      "2025-02-18 16:32:09,372 - INFO - Retrieved 10 ntropy transactions\n",
      "2025-02-18 16:32:09,373 - INFO - Retrieved 126 gocardless transactions\n",
      "2025-02-18 16:32:09,380 - INFO - Final prepared DataFrame contains 126 rows\n",
      "2025-02-18 16:32:09,381 - INFO - Retrieved 126 transactions to process\n",
      "2025-02-18 16:32:09,381 - INFO - Starting transaction processing\n",
      "2025-02-18 16:32:09,381 - INFO - Starting to process 126 transactions\n",
      "2025-02-18 16:32:09,381 - INFO - Initializing TransactionClassifier\n",
      "2025-02-18 16:32:09,382 - INFO - Processing batch 1/42 (rows 0 to 2)\n",
      "2025-02-18 16:32:09,383 - INFO - Starting classification of batch with 3 transactions\n",
      "2025-02-18 16:32:09,383 - INFO - Sending request to LLM\n",
      "2025-02-18 16:32:13,548 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-18 16:32:13,552 - INFO - Successfully parsed 3 classifications\n",
      "2025-02-18 16:32:13,553 - INFO - Transaction 0 classified:\n",
      "2025-02-18 16:32:13,553 - INFO - Account: 260\n",
      "2025-02-18 16:32:13,553 - INFO - Confidence: 0.8\n",
      "2025-02-18 16:32:13,554 - INFO - Reason Preview: This transaction indicates a payment received with a positive amount,...\n",
      "2025-02-18 16:32:13,554 - INFO - --------------------------------------------------\n",
      "2025-02-18 16:32:13,555 - INFO - Transaction 1 classified:\n",
      "2025-02-18 16:32:13,555 - INFO - Account: 420\n",
      "2025-02-18 16:32:13,555 - INFO - Confidence: 0.7\n",
      "2025-02-18 16:32:13,556 - INFO - Reason Preview: The transaction is a payment to 'SOHO HOUSE WHITE CITY',...\n",
      "2025-02-18 16:32:13,556 - INFO - --------------------------------------------------\n",
      "2025-02-18 16:32:13,556 - INFO - Transaction 2 classified:\n",
      "2025-02-18 16:32:13,556 - INFO - Account: 449\n",
      "2025-02-18 16:32:13,557 - INFO - Confidence: 0.85\n",
      "2025-02-18 16:32:13,557 - INFO - Reason Preview: This transaction involves a payment for a car park at...\n",
      "2025-02-18 16:32:13,557 - INFO - --------------------------------------------------\n",
      "2025-02-18 16:32:13,558 - INFO - Processing batch 2/42 (rows 3 to 5)\n",
      "2025-02-18 16:32:13,558 - INFO - Starting classification of batch with 3 transactions\n",
      "2025-02-18 16:32:13,559 - INFO - Sending request to LLM\n",
      "2025-02-18 16:32:18,468 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-18 16:32:18,472 - INFO - Successfully parsed 3 classifications\n",
      "2025-02-18 16:32:18,473 - INFO - Transaction 3 classified:\n",
      "2025-02-18 16:32:18,474 - INFO - Account: 485\n",
      "2025-02-18 16:32:18,474 - INFO - Confidence: 0.95\n",
      "2025-02-18 16:32:18,474 - INFO - Reason Preview: The transaction is identified as a payment to 'AUDIBLE UK'...\n",
      "2025-02-18 16:32:18,475 - INFO - --------------------------------------------------\n",
      "2025-02-18 16:32:18,475 - INFO - Transaction 4 classified:\n",
      "2025-02-18 16:32:18,475 - INFO - Account: 463\n",
      "2025-02-18 16:32:18,476 - INFO - Confidence: 0.9\n",
      "2025-02-18 16:32:18,476 - INFO - Reason Preview: The transaction is a payment to 'METRICOOL', indicating an outflow...\n",
      "2025-02-18 16:32:18,477 - INFO - --------------------------------------------------\n",
      "2025-02-18 16:32:18,478 - INFO - Transaction 5 classified:\n",
      "2025-02-18 16:32:18,478 - INFO - Account: 429\n",
      "2025-02-18 16:32:18,479 - INFO - Confidence: 0.75\n",
      "2025-02-18 16:32:18,479 - INFO - Reason Preview: This transaction is listed as a payment to 'AMAZON MARKETPLACE...\n",
      "2025-02-18 16:32:18,480 - INFO - --------------------------------------------------\n",
      "2025-02-18 16:32:18,480 - INFO - Processing batch 3/42 (rows 6 to 8)\n",
      "2025-02-18 16:32:18,481 - INFO - Starting classification of batch with 3 transactions\n",
      "2025-02-18 16:32:18,482 - INFO - Sending request to LLM\n",
      "2025-02-18 16:32:23,366 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-18 16:32:23,369 - INFO - Successfully parsed 3 classifications\n",
      "2025-02-18 16:32:23,371 - INFO - Transaction 6 classified:\n",
      "2025-02-18 16:32:23,371 - INFO - Account: 429\n",
      "2025-02-18 16:32:23,372 - INFO - Confidence: 0.7\n",
      "2025-02-18 16:32:23,372 - INFO - Reason Preview: The entity 'AMAZON MARKETPLACE UK APUK' suggests a purchase through...\n",
      "2025-02-18 16:32:23,372 - INFO - --------------------------------------------------\n",
      "2025-02-18 16:32:23,373 - INFO - Transaction 7 classified:\n",
      "2025-02-18 16:32:23,373 - INFO - Account: 485\n",
      "2025-02-18 16:32:23,374 - INFO - Confidence: 0.85\n",
      "2025-02-18 16:32:23,374 - INFO - Reason Preview: The entity 'METRICOOL' implies a subscription service, likely for analytics...\n",
      "2025-02-18 16:32:23,374 - INFO - --------------------------------------------------\n",
      "2025-02-18 16:32:23,375 - INFO - Transaction 8 classified:\n",
      "2025-02-18 16:32:23,375 - INFO - Account: 401\n",
      "2025-02-18 16:32:23,375 - INFO - Confidence: 0.8\n",
      "2025-02-18 16:32:23,376 - INFO - Reason Preview: The entity 'QUALITY FORMATIONS' refers to a service provider for...\n",
      "2025-02-18 16:32:23,376 - INFO - --------------------------------------------------\n",
      "2025-02-18 16:32:23,377 - INFO - Processing batch 4/42 (rows 9 to 11)\n",
      "2025-02-18 16:32:23,378 - INFO - Starting classification of batch with 3 transactions\n",
      "2025-02-18 16:32:23,379 - INFO - Sending request to LLM\n",
      "2025-02-18 16:32:27,459 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-18 16:32:27,463 - INFO - Successfully parsed 3 classifications\n",
      "2025-02-18 16:32:27,463 - INFO - Transaction 9 classified:\n",
      "2025-02-18 16:32:27,464 - INFO - Account: 463\n",
      "2025-02-18 16:32:27,464 - INFO - Confidence: 0.85\n",
      "2025-02-18 16:32:27,464 - INFO - Reason Preview: This transaction appears to be a small expense related to...\n",
      "2025-02-18 16:32:27,464 - INFO - --------------------------------------------------\n",
      "2025-02-18 16:32:27,465 - INFO - Transaction 10 classified:\n",
      "2025-02-18 16:32:27,465 - INFO - Account: 260\n",
      "2025-02-18 16:32:27,465 - INFO - Confidence: 0.9\n",
      "2025-02-18 16:32:27,465 - INFO - Reason Preview: The transaction is described as a 'Cashback Rebate', which suggests...\n",
      "2025-02-18 16:32:27,466 - INFO - --------------------------------------------------\n",
      "2025-02-18 16:32:27,466 - INFO - Transaction 11 classified:\n",
      "2025-02-18 16:32:27,466 - INFO - Account: 429\n",
      "2025-02-18 16:32:27,466 - INFO - Confidence: 0.8\n",
      "2025-02-18 16:32:27,467 - INFO - Reason Preview: This transaction is a purchase from Amazon Marketplace, which could...\n",
      "2025-02-18 16:32:27,467 - INFO - --------------------------------------------------\n",
      "2025-02-18 16:32:27,467 - INFO - Processing batch 5/42 (rows 12 to 14)\n",
      "2025-02-18 16:32:27,468 - INFO - Starting classification of batch with 3 transactions\n",
      "2025-02-18 16:32:27,469 - INFO - Sending request to LLM\n",
      "2025-02-18 16:32:31,558 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-18 16:32:31,563 - INFO - Successfully parsed 3 classifications\n",
      "2025-02-18 16:32:31,564 - INFO - Transaction 12 classified:\n",
      "2025-02-18 16:32:31,564 - INFO - Account: 463\n",
      "2025-02-18 16:32:31,565 - INFO - Confidence: 0.7\n",
      "2025-02-18 16:32:31,565 - INFO - Reason Preview: The transaction involves a minimal charge from FLOWON.AI, a company...\n",
      "2025-02-18 16:32:31,566 - INFO - --------------------------------------------------\n",
      "2025-02-18 16:32:31,566 - INFO - Transaction 13 classified:\n",
      "2025-02-18 16:32:31,567 - INFO - Account: 429\n",
      "2025-02-18 16:32:31,567 - INFO - Confidence: 0.8\n",
      "2025-02-18 16:32:31,567 - INFO - Reason Preview: This transaction is a vendor payment made to a dental...\n",
      "2025-02-18 16:32:31,568 - INFO - --------------------------------------------------\n",
      "2025-02-18 16:32:31,568 - INFO - Transaction 14 classified:\n",
      "2025-02-18 16:32:31,569 - INFO - Account: 485\n",
      "2025-02-18 16:32:31,569 - INFO - Confidence: 0.9\n",
      "2025-02-18 16:32:31,570 - INFO - Reason Preview: The Amazon Prime UK transaction suggests a subscription or media...\n",
      "2025-02-18 16:32:31,570 - INFO - --------------------------------------------------\n",
      "2025-02-18 16:32:31,570 - INFO - Processing batch 6/42 (rows 15 to 17)\n",
      "2025-02-18 16:32:31,571 - INFO - Starting classification of batch with 3 transactions\n",
      "2025-02-18 16:32:31,572 - INFO - Sending request to LLM\n",
      "2025-02-18 16:32:36,567 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-18 16:32:36,570 - INFO - Successfully parsed 3 classifications\n",
      "2025-02-18 16:32:36,571 - INFO - Transaction 15 classified:\n",
      "2025-02-18 16:32:36,572 - INFO - Account: 493\n",
      "2025-02-18 16:32:36,572 - INFO - Confidence: 0.85\n",
      "2025-02-18 16:32:36,572 - INFO - Reason Preview: The transaction relates to a travel charge for TFL (Transport...\n",
      "2025-02-18 16:32:36,572 - INFO - --------------------------------------------------\n",
      "2025-02-18 16:32:36,573 - INFO - Transaction 16 classified:\n",
      "2025-02-18 16:32:36,573 - INFO - Account: 424\n",
      "2025-02-18 16:32:36,574 - INFO - Confidence: 0.75\n",
      "2025-02-18 16:32:36,574 - INFO - Reason Preview: The transaction involves a purchase at McDonald's, which is generally...\n",
      "2025-02-18 16:32:36,574 - INFO - --------------------------------------------------\n",
      "2025-02-18 16:32:36,575 - INFO - Transaction 17 classified:\n",
      "2025-02-18 16:32:36,575 - INFO - Account: 429\n",
      "2025-02-18 16:32:36,576 - INFO - Confidence: 0.8\n",
      "2025-02-18 16:32:36,576 - INFO - Reason Preview: The transaction is a payment to a dental center. This...\n",
      "2025-02-18 16:32:36,576 - INFO - --------------------------------------------------\n",
      "2025-02-18 16:32:36,576 - INFO - Finished processing all transactions\n",
      "2025-02-18 16:32:36,577 - INFO - Completed reconciliation process\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entity_name</th>\n",
       "      <th>amount</th>\n",
       "      <th>remittance_info</th>\n",
       "      <th>ntropy_enrich</th>\n",
       "      <th>ntropy_entity</th>\n",
       "      <th>ntropy_category</th>\n",
       "      <th>coa_agent</th>\n",
       "      <th>coa_reason</th>\n",
       "      <th>coa_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2c37580c-5bd5-45a2-a2bb-2f4371b1199c9ef65f8b-e...</td>\n",
       "      <td>None</td>\n",
       "      <td>172.37</td>\n",
       "      <td>PAYMENT RECEIVED - THANK YOU</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>260</td>\n",
       "      <td>This transaction indicates a payment received ...</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99e0c076-c6c3-490b-85a2-3687ac9c79551b84f6b4-2...</td>\n",
       "      <td>SOHO HOUSE WHITE CITY</td>\n",
       "      <td>-46.60</td>\n",
       "      <td>3CPAYMENT*WHITE CITY HO LONDON</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>420</td>\n",
       "      <td>The transaction is a payment to 'SOHO HOUSE WH...</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>daa2a72e-9e19-41dd-92ac-b57eb0fbc3c75d494b7c-2...</td>\n",
       "      <td>WESTFIELD LONDON - CAR PARK</td>\n",
       "      <td>-8.50</td>\n",
       "      <td>WESTFIELD LONDON - CAR  LONDON</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>449</td>\n",
       "      <td>This transaction involves a payment for a car ...</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9bc38ce4-c82a-473b-8498-7fe5a49ffcf7f5c10402-4...</td>\n",
       "      <td>AUDIBLE</td>\n",
       "      <td>-7.99</td>\n",
       "      <td>AUDIBLE UK              ADBL.CO/PYMT</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>485</td>\n",
       "      <td>The transaction is identified as a payment to ...</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c928468a-1500-439a-a6a1-3500daf43476bba85042-7...</td>\n",
       "      <td>METRICOOL</td>\n",
       "      <td>-18.39</td>\n",
       "      <td>METRICOOL.COM           MADRID</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>463</td>\n",
       "      <td>The transaction is a payment to 'METRICOOL', i...</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>8db6dbd2-862e-40c5-8140-d8e11b63cfc062e2e853-9...</td>\n",
       "      <td>FLOWON.AI</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>FLOWON.AI               LONDON</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>36760625-422b-4d79-93a2-cdb844da08be92e37836-6...</td>\n",
       "      <td>METRICOOL</td>\n",
       "      <td>-18.29</td>\n",
       "      <td>METRICOOL.COM           MADRID</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>42b5a4d9-bf37-417a-a716-138718236d17af9852a7-3...</td>\n",
       "      <td>AUDIBLE</td>\n",
       "      <td>-7.99</td>\n",
       "      <td>AUDIBLE UK              ADBL.CO/PYMT</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>bbf5786a-75f9-4b4c-93d5-550cdd6cd73f98707b70-8...</td>\n",
       "      <td>HEATHROW NORTH SERVICE STATION</td>\n",
       "      <td>-25.26</td>\n",
       "      <td>EUG00840 HEATHROW NORTH Hayes</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0678d0e5-29b3-4c73-b3d9-929b2ba428de3ae2256b-9...</td>\n",
       "      <td>SAINSBURYS</td>\n",
       "      <td>-22.65</td>\n",
       "      <td>SAINSBURY'S WEST DRAYTO WEST DRAYTON</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    id  \\\n",
       "0    2c37580c-5bd5-45a2-a2bb-2f4371b1199c9ef65f8b-e...   \n",
       "1    99e0c076-c6c3-490b-85a2-3687ac9c79551b84f6b4-2...   \n",
       "2    daa2a72e-9e19-41dd-92ac-b57eb0fbc3c75d494b7c-2...   \n",
       "3    9bc38ce4-c82a-473b-8498-7fe5a49ffcf7f5c10402-4...   \n",
       "4    c928468a-1500-439a-a6a1-3500daf43476bba85042-7...   \n",
       "..                                                 ...   \n",
       "121  8db6dbd2-862e-40c5-8140-d8e11b63cfc062e2e853-9...   \n",
       "122  36760625-422b-4d79-93a2-cdb844da08be92e37836-6...   \n",
       "123  42b5a4d9-bf37-417a-a716-138718236d17af9852a7-3...   \n",
       "124  bbf5786a-75f9-4b4c-93d5-550cdd6cd73f98707b70-8...   \n",
       "125  0678d0e5-29b3-4c73-b3d9-929b2ba428de3ae2256b-9...   \n",
       "\n",
       "                        entity_name  amount  \\\n",
       "0                              None  172.37   \n",
       "1             SOHO HOUSE WHITE CITY  -46.60   \n",
       "2       WESTFIELD LONDON - CAR PARK   -8.50   \n",
       "3                           AUDIBLE   -7.99   \n",
       "4                         METRICOOL  -18.39   \n",
       "..                              ...     ...   \n",
       "121                       FLOWON.AI   -0.99   \n",
       "122                       METRICOOL  -18.29   \n",
       "123                         AUDIBLE   -7.99   \n",
       "124  HEATHROW NORTH SERVICE STATION  -25.26   \n",
       "125                      SAINSBURYS  -22.65   \n",
       "\n",
       "                          remittance_info  ntropy_enrich ntropy_entity  \\\n",
       "0            PAYMENT RECEIVED - THANK YOU          False          None   \n",
       "1          3CPAYMENT*WHITE CITY HO LONDON          False          None   \n",
       "2          WESTFIELD LONDON - CAR  LONDON          False          None   \n",
       "3    AUDIBLE UK              ADBL.CO/PYMT          False          None   \n",
       "4          METRICOOL.COM           MADRID          False          None   \n",
       "..                                    ...            ...           ...   \n",
       "121        FLOWON.AI               LONDON          False          None   \n",
       "122        METRICOOL.COM           MADRID          False          None   \n",
       "123  AUDIBLE UK              ADBL.CO/PYMT          False          None   \n",
       "124         EUG00840 HEATHROW NORTH Hayes          False          None   \n",
       "125  SAINSBURY'S WEST DRAYTO WEST DRAYTON          False          None   \n",
       "\n",
       "    ntropy_category coa_agent  \\\n",
       "0              None       260   \n",
       "1              None       420   \n",
       "2              None       449   \n",
       "3              None       485   \n",
       "4              None       463   \n",
       "..              ...       ...   \n",
       "121            None      None   \n",
       "122            None      None   \n",
       "123            None      None   \n",
       "124            None      None   \n",
       "125            None      None   \n",
       "\n",
       "                                            coa_reason coa_confidence  \n",
       "0    This transaction indicates a payment received ...            0.8  \n",
       "1    The transaction is a payment to 'SOHO HOUSE WH...            0.7  \n",
       "2    This transaction involves a payment for a car ...           0.85  \n",
       "3    The transaction is identified as a payment to ...           0.95  \n",
       "4    The transaction is a payment to 'METRICOOL', i...            0.9  \n",
       "..                                                 ...            ...  \n",
       "121                                               None           None  \n",
       "122                                               None           None  \n",
       "123                                               None           None  \n",
       "124                                               None           None  \n",
       "125                                               None           None  \n",
       "\n",
       "[126 rows x 10 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" LLM inference for reconciliation\"\"\"\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from typing import Dict, Tuple, List\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the ChatGroq model\n",
    "groq_model = ChatGroq(model_name=\"Llama-3.3-70b-Specdec\")\n",
    "openai_model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "\n",
    "class TransactionClassifier:\n",
    "    def __init__(self):\n",
    "        logger.info(\"Initializing TransactionClassifier\")\n",
    "        self.system_prompt = \"\"\"\n",
    "        You are a helpful financial expert responsible for classifying transactions.\n",
    "        You will be given multiple transactions and a list of chart of accounts to reconcile. \n",
    "\n",
    "        For each transaction, you must provide:\n",
    "        1. A brief thought process and explanation of which chart of account is appropriate for this transaction\n",
    "        2. The code for the most appropriate chart of account\n",
    "        3. A confidence score between 0 and 1 (e.g., 0.95 for high confidence, 0.40 for low confidence)\n",
    "\n",
    "        # IMPORTANT: Instructions for your response:\n",
    "        You must respond with valid JSON in the following format only:\n",
    "        {\n",
    "          \"classifications\": [\n",
    "            {\n",
    "              \"transaction_index\": 0,\n",
    "              \"reasoning\": \"string\",\n",
    "              \"account\": \"string\",\n",
    "              \"confidence\": float\n",
    "            },\n",
    "            {\n",
    "              \"transaction_index\": 1,\n",
    "              \"reasoning\": \"string\",\n",
    "              \"account\": \"string\",\n",
    "              \"confidence\": float\n",
    "            },\n",
    "            {\n",
    "              \"transaction_index\": 2,\n",
    "              \"reasoning\": \"string\",\n",
    "              \"account\": \"string\",\n",
    "              \"confidence\": float\n",
    "            },\n",
    "          ]\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.last_request_time = 0\n",
    "        self.rate_limit_delay = 2  # 2 seconds between requests (30 requests/minute)\n",
    "\n",
    "    def _rate_limit(self):\n",
    "        \"\"\"Implement rate limiting\"\"\"\n",
    "        current_time = time.time()\n",
    "        time_since_last_request = current_time - self.last_request_time\n",
    "        if time_since_last_request < self.rate_limit_delay:\n",
    "            delay = self.rate_limit_delay - time_since_last_request\n",
    "            logger.debug(f\"Rate limiting: waiting {delay:.2f} seconds\")\n",
    "            time.sleep(delay)\n",
    "        self.last_request_time = time.time() \n",
    "\n",
    "    def classify_transactions_batch(self, transactions: List[Dict], chart_of_accounts: list) -> List[Tuple[str, str, float]]:\n",
    "        \"\"\"Classify a batch of transactions using the LLM\"\"\"\n",
    "        logger.info(f\"Starting classification of batch with {len(transactions)} transactions\")\n",
    "        logger.debug(f\"Transactions to classify: {json.dumps(transactions, indent=2)}\")\n",
    "        logger.debug(f\"Number of chart of accounts: {len(chart_of_accounts)}\")\n",
    "        \n",
    "        self._rate_limit()\n",
    "\n",
    "        # Format the transactions for the LLM\n",
    "        transactions_prompt = f\"\"\"\n",
    "        Please classify the following transactions:\n",
    "        Transactions: {transactions}\n",
    "        \n",
    "        chart of accounts: {chart_of_accounts}\n",
    "        \"\"\"\n",
    "        logger.debug(f\"Generated prompt: {transactions_prompt}\")\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=transactions_prompt)\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Sending request to LLM\")\n",
    "            response = openai_model.invoke(messages)\n",
    "            logger.debug(f\"Raw LLM response: {response.content}\")\n",
    "\n",
    "            try:\n",
    "                import re\n",
    "                json_match = re.search(r'\\{.*\\}', response.content, re.DOTALL)\n",
    "                if json_match:\n",
    "                    logger.debug(\"Found JSON structure in response\")\n",
    "                    result = json.loads(json_match.group())\n",
    "                    classifications = result.get('classifications', [])\n",
    "                    logger.info(f\"Successfully parsed {len(classifications)} classifications\")\n",
    "                    logger.debug(f\"Classifications: {json.dumps(classifications, indent=2)}\")\n",
    "                    \n",
    "                    # Convert to list of tuples\n",
    "                    return [\n",
    "                        (c['account'], c['reasoning'], c['confidence'])\n",
    "                        for c in sorted(classifications, key=lambda x: x['transaction_index'])\n",
    "                    ]\n",
    "                else:\n",
    "                    logger.error(\"No JSON structure found in response\")\n",
    "                    return [(f\"ERROR: No JSON found\", \"\", 0.0)] * len(transactions)\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                logger.error(f\"Failed to parse JSON response: {e}\")\n",
    "                logger.debug(f\"Problematic response content: {response.content}\")\n",
    "                return [(f\"ERROR: {str(e)}\", \"\", 0.0)] * len(transactions)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Batch classification failed: {str(e)}\", exc_info=True)\n",
    "            return [(f\"ERROR: {str(e)}\", \"\", 0.0)] * len(transactions)\n",
    "\n",
    "\"\"\" fetch transactions + ntropy enrich from supabase \"\"\"\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "\n",
    "from app.services.supabase import get_supabase\n",
    "\n",
    "class TransactionToLLM(BaseModel):\n",
    "    entity_name : str\n",
    "    amount : float\n",
    "    remittance_info : str\n",
    "    ntropy_enrich : bool\n",
    "    ntropy_entity : str\n",
    "    ntropy_category : str\n",
    "\n",
    "def process_transactions(df: pd.DataFrame, chart_of_accounts: list) -> pd.DataFrame:\n",
    "    \"\"\"Process transactions grouped by remittance_info in batches of 3 groups\"\"\"\n",
    "    logger.info(f\"Starting to process {len(df)} transactions\")\n",
    "    logger.debug(f\"Input DataFrame shape: {df.shape}\")\n",
    "    \n",
    "    classifier = TransactionClassifier()\n",
    "\n",
    "    # Create copy of DataFrame with new columns\n",
    "    df = df.copy()\n",
    "    df['coa_agent'] = None\n",
    "    df['coa_reason'] = None\n",
    "    df['coa_confidence'] = None\n",
    "\n",
    "    # Group by remittance_info and get representative samples\n",
    "    grouped = df.groupby('remittance_info')\n",
    "    unique_groups = list(grouped.groups.keys())\n",
    "    \n",
    "    # Process groups in batches of 3\n",
    "    batch_size = 3\n",
    "    total_batches = (len(unique_groups) + batch_size - 1) // batch_size\n",
    "    \n",
    "    stop_counter = 0\n",
    "    for i in range(0, len(unique_groups), batch_size):\n",
    "        batch_end = min(i + batch_size, len(unique_groups))\n",
    "        current_batch = (i // batch_size) + 1\n",
    "        logger.info(f\"Processing batch {current_batch}/{total_batches} (groups {i} to {batch_end-1})\")\n",
    "        \n",
    "        # Get representative transactions from each group in this batch\n",
    "        batch_transactions = []\n",
    "        group_indices = []\n",
    "        \n",
    "        for remittance_info in unique_groups[i:batch_end]:\n",
    "            group_df = grouped.get_group(remittance_info)\n",
    "            # Take the first transaction as representative for the group\n",
    "            representative = group_df.iloc[0]\n",
    "            batch_transactions.append({\n",
    "                **representative.to_dict(),\n",
    "                'amount': representative['amount'],\n",
    "                'group_size': len(group_df)\n",
    "            })\n",
    "            group_indices.append(remittance_info)\n",
    "        \n",
    "        # Get classifications for the batch\n",
    "        classifications = classifier.classify_transactions_batch(batch_transactions, chart_of_accounts)\n",
    "        \n",
    "        # Apply classifications to all transactions in each group\n",
    "        for remittance_info, (account, reasoning, confidence) in zip(group_indices, classifications):\n",
    "            mask = df['remittance_info'] == remittance_info\n",
    "            df.loc[mask, 'coa_agent'] = account\n",
    "            df.loc[mask, 'coa_reason'] = reasoning\n",
    "            df.loc[mask, 'coa_confidence'] = confidence\n",
    "            \n",
    "            group_size = len(df[mask])\n",
    "            logger.info(f\"Applied classification to group '{remittance_info}' ({group_size} transactions):\")\n",
    "            logger.info(f\"Account: {account}\")\n",
    "            logger.info(f\"Confidence: {confidence}\")\n",
    "            logger.info(f\"Reason Preview: {' '.join(reasoning.split()[:10])}...\")\n",
    "            logger.info(\"-\" * 50)\n",
    "\n",
    "        stop_counter += 1\n",
    "        if stop_counter > 5:\n",
    "            break\n",
    "    logger.info(\"Finished processing all transaction groups\")\n",
    "    logger.debug(f\"Final DataFrame shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "async def fetch_and_prepare_transactions() -> pd.DataFrame:\n",
    "    \"\"\"Fetch transactions from Supabase and prepare DataFrame\"\"\"\n",
    "    logger.info(\"Starting to fetch transactions from Supabase\")\n",
    "    \n",
    "    try:\n",
    "        # Get Supabase client\n",
    "        supabase = await get_supabase()\n",
    "        logger.info(\"Successfully connected to Supabase\")\n",
    "        \n",
    "        # Query both tables asynchronously\n",
    "        logger.info(\"Querying Supabase tables...\")\n",
    "        ntropy_response = await supabase.table('ntropy_transactions').select('*').execute()\n",
    "        gocardless_response = await supabase.table('gocardless_transactions').select('*').execute()\n",
    "        \n",
    "        logger.info(f\"Retrieved {len(ntropy_response.data)} ntropy transactions\")\n",
    "        logger.info(f\"Retrieved {len(gocardless_response.data)} gocardless transactions\")\n",
    "        \n",
    "        # Convert to DataFrames\n",
    "        ntropy_df = pd.DataFrame(ntropy_response.data)\n",
    "        gocardless_df = pd.DataFrame(gocardless_response.data)\n",
    "        \n",
    "        # Merge DataFrames on the specified keys\n",
    "        merged_df = pd.merge(\n",
    "            gocardless_df,\n",
    "            ntropy_df,\n",
    "            how='left',\n",
    "            left_on='id',\n",
    "            right_on='ntropy_id'\n",
    "        )\n",
    "        \n",
    "        # Create a new DataFrame with selected columns\n",
    "        result_df = pd.DataFrame({\n",
    "            'id': merged_df['id'],\n",
    "            'entity_name': merged_df.apply(\n",
    "                lambda row: row['creditor_name'] if pd.notnull(row['creditor_name'])\n",
    "                else row['debtor_name'] if pd.notnull(row['debtor_name'])\n",
    "                else row['enriched_data']['entities']['counterparty']['name'] if pd.notnull(row['enriched_data'])\n",
    "                else None,\n",
    "                axis=1\n",
    "            ),\n",
    "            'amount': merged_df['amount']/100,\n",
    "            'remittance_info': merged_df['remittance_info'],\n",
    "            'ntropy_enrich': merged_df['enriched_data'].notnull(),\n",
    "            'ntropy_entity': merged_df['enriched_data'].apply(\n",
    "                lambda x: x['entities']['counterparty']['name'] if pd.notnull(x) else None\n",
    "            ),\n",
    "            'ntropy_category': merged_df['enriched_data'].apply(\n",
    "                lambda x: x['categories']['general'] if pd.notnull(x) else None\n",
    "            )\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"Final prepared DataFrame contains {len(result_df)} rows\")\n",
    "        logger.debug(f\"DataFrame columns: {result_df.columns.tolist()}\")\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in fetch_and_prepare_transactions\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main async function to orchestrate the reconciliation process\"\"\"\n",
    "    logger.info(\"Starting reconciliation process\")\n",
    "    \n",
    "    try:\n",
    "        # Load chart of accounts\n",
    "        logger.info(\"Loading chart of accounts from coa.json\")\n",
    "        with open('coa.json', 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # Extract only the required fields from each account\n",
    "        parsed_accounts = [\n",
    "            {\n",
    "                'code': account.get('Code', ''),\n",
    "                'name': account.get('Name', ''),\n",
    "                'type': account.get('Type', ''),\n",
    "                'description': account.get('Description', ''),\n",
    "                'class': account.get('Class', ''),\n",
    "            }\n",
    "            for account in data['Accounts']\n",
    "            if account.get('Status') == 'ACTIVE'\n",
    "        ]\n",
    "        logger.info(f\"Loaded {len(parsed_accounts)} active accounts from CoA\")\n",
    "        \n",
    "        # Fetch and prepare transactions\n",
    "        logger.info(\"Fetching and preparing transactions\")\n",
    "        result_df = await fetch_and_prepare_transactions()\n",
    "        logger.info(f\"Retrieved {len(result_df)} transactions to process\")\n",
    "        \n",
    "        # Process transactions\n",
    "        logger.info(\"Starting transaction processing\")\n",
    "        df_reconciled = process_transactions(result_df, parsed_accounts)\n",
    "        logger.info(\"Completed reconciliation process\")\n",
    "        \n",
    "        return df_reconciled\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in main function\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    df_reconciled = asyncio.run(main())\n",
    "    print(df_reconciled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entity_name</th>\n",
       "      <th>amount</th>\n",
       "      <th>remittance_info</th>\n",
       "      <th>ntropy_enrich</th>\n",
       "      <th>ntropy_entity</th>\n",
       "      <th>ntropy_category</th>\n",
       "      <th>coa_agent</th>\n",
       "      <th>coa_reason</th>\n",
       "      <th>coa_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2c37580c-5bd5-45a2-a2bb-2f4371b1199c9ef65f8b-e...</td>\n",
       "      <td>None</td>\n",
       "      <td>172.37</td>\n",
       "      <td>PAYMENT RECEIVED - THANK YOU</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>260</td>\n",
       "      <td>This transaction indicates a payment received ...</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99e0c076-c6c3-490b-85a2-3687ac9c79551b84f6b4-2...</td>\n",
       "      <td>SOHO HOUSE WHITE CITY</td>\n",
       "      <td>-46.60</td>\n",
       "      <td>3CPAYMENT*WHITE CITY HO LONDON</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>420</td>\n",
       "      <td>The transaction is a payment to 'SOHO HOUSE WH...</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>daa2a72e-9e19-41dd-92ac-b57eb0fbc3c75d494b7c-2...</td>\n",
       "      <td>WESTFIELD LONDON - CAR PARK</td>\n",
       "      <td>-8.50</td>\n",
       "      <td>WESTFIELD LONDON - CAR  LONDON</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>449</td>\n",
       "      <td>This transaction involves a payment for a car ...</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9bc38ce4-c82a-473b-8498-7fe5a49ffcf7f5c10402-4...</td>\n",
       "      <td>AUDIBLE</td>\n",
       "      <td>-7.99</td>\n",
       "      <td>AUDIBLE UK              ADBL.CO/PYMT</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>485</td>\n",
       "      <td>The transaction is identified as a payment to ...</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c928468a-1500-439a-a6a1-3500daf43476bba85042-7...</td>\n",
       "      <td>METRICOOL</td>\n",
       "      <td>-18.39</td>\n",
       "      <td>METRICOOL.COM           MADRID</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>463</td>\n",
       "      <td>The transaction is a payment to 'METRICOOL', i...</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>8db6dbd2-862e-40c5-8140-d8e11b63cfc062e2e853-9...</td>\n",
       "      <td>FLOWON.AI</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>FLOWON.AI               LONDON</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>36760625-422b-4d79-93a2-cdb844da08be92e37836-6...</td>\n",
       "      <td>METRICOOL</td>\n",
       "      <td>-18.29</td>\n",
       "      <td>METRICOOL.COM           MADRID</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>42b5a4d9-bf37-417a-a716-138718236d17af9852a7-3...</td>\n",
       "      <td>AUDIBLE</td>\n",
       "      <td>-7.99</td>\n",
       "      <td>AUDIBLE UK              ADBL.CO/PYMT</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>bbf5786a-75f9-4b4c-93d5-550cdd6cd73f98707b70-8...</td>\n",
       "      <td>HEATHROW NORTH SERVICE STATION</td>\n",
       "      <td>-25.26</td>\n",
       "      <td>EUG00840 HEATHROW NORTH Hayes</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0678d0e5-29b3-4c73-b3d9-929b2ba428de3ae2256b-9...</td>\n",
       "      <td>SAINSBURYS</td>\n",
       "      <td>-22.65</td>\n",
       "      <td>SAINSBURY'S WEST DRAYTO WEST DRAYTON</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    id  \\\n",
       "0    2c37580c-5bd5-45a2-a2bb-2f4371b1199c9ef65f8b-e...   \n",
       "1    99e0c076-c6c3-490b-85a2-3687ac9c79551b84f6b4-2...   \n",
       "2    daa2a72e-9e19-41dd-92ac-b57eb0fbc3c75d494b7c-2...   \n",
       "3    9bc38ce4-c82a-473b-8498-7fe5a49ffcf7f5c10402-4...   \n",
       "4    c928468a-1500-439a-a6a1-3500daf43476bba85042-7...   \n",
       "..                                                 ...   \n",
       "121  8db6dbd2-862e-40c5-8140-d8e11b63cfc062e2e853-9...   \n",
       "122  36760625-422b-4d79-93a2-cdb844da08be92e37836-6...   \n",
       "123  42b5a4d9-bf37-417a-a716-138718236d17af9852a7-3...   \n",
       "124  bbf5786a-75f9-4b4c-93d5-550cdd6cd73f98707b70-8...   \n",
       "125  0678d0e5-29b3-4c73-b3d9-929b2ba428de3ae2256b-9...   \n",
       "\n",
       "                        entity_name  amount  \\\n",
       "0                              None  172.37   \n",
       "1             SOHO HOUSE WHITE CITY  -46.60   \n",
       "2       WESTFIELD LONDON - CAR PARK   -8.50   \n",
       "3                           AUDIBLE   -7.99   \n",
       "4                         METRICOOL  -18.39   \n",
       "..                              ...     ...   \n",
       "121                       FLOWON.AI   -0.99   \n",
       "122                       METRICOOL  -18.29   \n",
       "123                         AUDIBLE   -7.99   \n",
       "124  HEATHROW NORTH SERVICE STATION  -25.26   \n",
       "125                      SAINSBURYS  -22.65   \n",
       "\n",
       "                          remittance_info  ntropy_enrich ntropy_entity  \\\n",
       "0            PAYMENT RECEIVED - THANK YOU          False          None   \n",
       "1          3CPAYMENT*WHITE CITY HO LONDON          False          None   \n",
       "2          WESTFIELD LONDON - CAR  LONDON          False          None   \n",
       "3    AUDIBLE UK              ADBL.CO/PYMT          False          None   \n",
       "4          METRICOOL.COM           MADRID          False          None   \n",
       "..                                    ...            ...           ...   \n",
       "121        FLOWON.AI               LONDON          False          None   \n",
       "122        METRICOOL.COM           MADRID          False          None   \n",
       "123  AUDIBLE UK              ADBL.CO/PYMT          False          None   \n",
       "124         EUG00840 HEATHROW NORTH Hayes          False          None   \n",
       "125  SAINSBURY'S WEST DRAYTO WEST DRAYTON          False          None   \n",
       "\n",
       "    ntropy_category coa_agent  \\\n",
       "0              None       260   \n",
       "1              None       420   \n",
       "2              None       449   \n",
       "3              None       485   \n",
       "4              None       463   \n",
       "..              ...       ...   \n",
       "121            None      None   \n",
       "122            None      None   \n",
       "123            None      None   \n",
       "124            None      None   \n",
       "125            None      None   \n",
       "\n",
       "                                            coa_reason coa_confidence  \n",
       "0    This transaction indicates a payment received ...            0.8  \n",
       "1    The transaction is a payment to 'SOHO HOUSE WH...            0.7  \n",
       "2    This transaction involves a payment for a car ...           0.85  \n",
       "3    The transaction is identified as a payment to ...           0.95  \n",
       "4    The transaction is a payment to 'METRICOOL', i...            0.9  \n",
       "..                                                 ...            ...  \n",
       "121                                               None           None  \n",
       "122                                               None           None  \n",
       "123                                               None           None  \n",
       "124                                               None           None  \n",
       "125                                               None           None  \n",
       "\n",
       "[126 rows x 10 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reconciled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" LLM inference for reconciliation\"\"\"\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from typing import Dict, Tuple, List\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the ChatGroq model\n",
    "groq_model = ChatGroq(model_name=\"Llama-3.3-70b-Specdec\")\n",
    "openai_model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "\n",
    "class TransactionClassifier:\n",
    "    def __init__(self):\n",
    "        logger.info(\"Initializing TransactionClassifier\")\n",
    "        self.system_prompt = \"\"\"\n",
    "        You are a helpful financial expert responsible for classifying transactions.\n",
    "        You will be given multiple transactions and a list of chart of accounts to reconcile. \n",
    "\n",
    "        For each transaction, you must provide:\n",
    "        1. A brief thought process and explanation of which chart of account is appropriate for this transaction\n",
    "        2. The code for the most appropriate chart of account\n",
    "        3. A confidence score between 0 and 1 (e.g., 0.95 for high confidence, 0.40 for low confidence)\n",
    "\n",
    "        # IMPORTANT: Instructions for your response:\n",
    "        You must respond with valid JSON in the following format only:\n",
    "        {\n",
    "          \"classifications\": [\n",
    "            {\n",
    "              \"transaction_index\": 0,\n",
    "              \"reasoning\": \"string\",\n",
    "              \"account\": \"string\",\n",
    "              \"confidence\": float\n",
    "            },\n",
    "            {\n",
    "              \"transaction_index\": 1,\n",
    "              \"reasoning\": \"string\",\n",
    "              \"account\": \"string\",\n",
    "              \"confidence\": float\n",
    "            },\n",
    "            {\n",
    "              \"transaction_index\": 2,\n",
    "              \"reasoning\": \"string\",\n",
    "              \"account\": \"string\",\n",
    "              \"confidence\": float\n",
    "            },\n",
    "          ]\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.last_request_time = 0\n",
    "        self.rate_limit_delay = 2  # 2 seconds between requests (30 requests/minute)\n",
    "\n",
    "    def _rate_limit(self):\n",
    "        \"\"\"Implement rate limiting\"\"\"\n",
    "        current_time = time.time()\n",
    "        time_since_last_request = current_time - self.last_request_time\n",
    "        if time_since_last_request < self.rate_limit_delay:\n",
    "            delay = self.rate_limit_delay - time_since_last_request\n",
    "            logger.debug(f\"Rate limiting: waiting {delay:.2f} seconds\")\n",
    "            time.sleep(delay)\n",
    "        self.last_request_time = time.time() \n",
    "\n",
    "    def classify_transactions_batch(self, transactions: List[Dict], chart_of_accounts: list) -> List[Tuple[str, str, float]]:\n",
    "            \"\"\"Classify a batch of transactions using the LLM\"\"\"\n",
    "            logger.info(f\"Processing batch of {len(transactions)} transactions\")\n",
    "            self._rate_limit()\n",
    "\n",
    "            # Format the transactions for the LLM\n",
    "            transactions_prompt = f\"\"\"\n",
    "            Please classify the following transactions:\n",
    "            Transactions: {transactions}\n",
    "            \n",
    "            chart of accounts: {chart_of_accounts}\n",
    "            \"\"\"\n",
    "\n",
    "            messages = [\n",
    "                SystemMessage(content=self.system_prompt),\n",
    "                HumanMessage(content=transactions_prompt)\n",
    "            ]\n",
    "\n",
    "            try:\n",
    "                print(\"\\n=== Sending batch request to LLM ===\")\n",
    "                response = openai_model.invoke(messages)\n",
    "                print(\"\\n=== Raw LLM Response ===\")\n",
    "                print(response.content)\n",
    "\n",
    "                # Improved JSON parsing logic\n",
    "                try:\n",
    "                    # First, try to extract JSON from the response\n",
    "                    import re\n",
    "                    json_match = re.search(r'\\{.*\\}', response.content, re.DOTALL)\n",
    "                    if json_match:\n",
    "                        result = json.loads(json_match.group())\n",
    "                        classifications = result.get('classifications', [])\n",
    "                        logger.info(f\"Successfully parsed {len(classifications)} classifications\")\n",
    "                        \n",
    "                        # Convert to list of tuples\n",
    "                        return [\n",
    "                            (c['account'], c['reasoning'], c['confidence'])\n",
    "                            for c in sorted(classifications, key=lambda x: x['transaction_index'])\n",
    "                        ]\n",
    "                    else:\n",
    "                        logger.error(\"No JSON structure found in response\")\n",
    "                        return [(f\"ERROR: No JSON found\", \"\", 0.0)] * len(transactions)\n",
    "                        \n",
    "                except json.JSONDecodeError as e:\n",
    "                    logger.error(f\"Failed to parse JSON response: {e}\")\n",
    "                    return [(f\"ERROR: {str(e)}\", \"\", 0.0)] * len(transactions)\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Batch classification failed: {str(e)}\", exc_info=True)\n",
    "                return [(f\"ERROR: {str(e)}\", \"\", 0.0)] * len(transactions)\n",
    "\n",
    "\n",
    "def process_transactions(df: pd.DataFrame, chart_of_accounts: list) -> pd.DataFrame:\n",
    "    \"\"\"Process transactions in DataFrame in batches of 3\"\"\"\n",
    "    logger.info(f\"Starting to process {len(df)} transactions\")\n",
    "    classifier = TransactionClassifier()\n",
    "\n",
    "    df = df.copy()\n",
    "    df['coa_agent'] = None\n",
    "    df['coa_reason'] = None\n",
    "    df['coa_confidence'] = None\n",
    "\n",
    "    # Process in batches of 3\n",
    "    batch_size = 3\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_end = min(i + batch_size, len(df))\n",
    "        batch_rows = df.iloc[i:batch_end]\n",
    "        \n",
    "        # Convert batch to list of dictionaries\n",
    "        batch_transactions = [\n",
    "            {**row.to_dict(), 'amount': row['amount']/100}\n",
    "            for _, row in batch_rows.iterrows()\n",
    "        ]\n",
    "        \n",
    "        # Get classifications for the batch\n",
    "        classifications: list = classifier.classify_transactions_batch(batch_transactions, chart_of_accounts)\n",
    "        \n",
    "        # Update DataFrame with results\n",
    "        for j, (account, reasoning, confidence) in enumerate(classifications):\n",
    "            idx = i + j\n",
    "            if idx < len(df):  # Ensure we don't go past the end of the DataFrame\n",
    "                df.iloc[idx, df.columns.get_loc('coa_agent')] = account\n",
    "                df.iloc[idx, df.columns.get_loc('coa_reason')] = reasoning\n",
    "                df.iloc[idx, df.columns.get_loc('coa_confidence')] = confidence\n",
    "                \n",
    "                # Print summary\n",
    "                reason_preview = ' '.join(reasoning.split()[:10]) + '...' if reasoning else 'No reason provided'\n",
    "                logger.info(f\"Transaction {idx}:\")\n",
    "                logger.info(f\"Account: {account}\")\n",
    "                logger.info(f\"Reason Preview: {reason_preview}\")\n",
    "                logger.info(\"-\" * 50)\n",
    "    logger.info(\"Finished processing all transactions\")\n",
    "    return df\n",
    "\n",
    "\"\"\" Fetch and store chart of accounts into parsed_accounts \"\"\"\n",
    "import json\n",
    "\n",
    "# Read the JSON file\n",
    "with open('coa.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract only the required fields from each account\n",
    "parsed_accounts = []\n",
    "for account in data['Accounts']:\n",
    "    if account.get('Status') == 'ACTIVE':\n",
    "        parsed_account = {\n",
    "            'code': account.get('Code', ''),\n",
    "            'name': account.get('Name', ''),\n",
    "            'type': account.get('Type', ''),\n",
    "            'description': account.get('Description', ''),  # Note: Not present in sample but included as requested\n",
    "            'class': account.get('Class', ''),\n",
    "    }\n",
    "    parsed_accounts.append(parsed_account)\n",
    "\n",
    "df_reconciled = process_transactions(result_df, parsed_accounts)\n",
    "\n",
    "df_reconciled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reconciled.to_csv(\"transactions_ai_reconciled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x13f979090>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0.0 (Total amount: -128.51999999999998):\n",
      "  AMZNMKTPLACE*SV3139IO5  AMAZON.CO.UK     Amount: -17.98\n",
      "  AMZNMKTPLACE*A37DG8PS5  AMAZON.CO.UK     Amount: -28.88\n",
      "  AMZNMKTPLACE*1K8C45SV5  AMAZON.CO.UK     Amount: -19.98\n",
      "  AMZNMKTPLACE*BX8LY0GQ5  AMAZON.CO.UK     Amount: -15.98\n",
      "  AMZNMKTPLACE*E50UR0V15  AMAZON.CO.UK     Amount: -32.52\n",
      "  AMZNMKTPLACE*T95QP19W4  AMAZON.CO.UK     Amount: -13.18\n",
      "\n",
      "Cluster 1.0 (Total amount: -53.940000000000005):\n",
      "  AMAZON PRIME*VV9BJ3IL5  AMZN.CO.UK/PM    Amount: -17.98\n",
      "  AMAZON PRIME*O99TT0FV5  AMZN.CO.UK/PM    Amount: -17.98\n",
      "  AMAZON PRIME*TE26Y4ZT4  AMZN.CO.UK/PM    Amount: -17.98\n",
      "\n",
      "Cluster 2.0 (Total amount: -67.78):\n",
      "  AMAZON.CO.UK*ZP8HS0JD5  AMAZON.CO.UK     Amount: -27.0\n",
      "  AMAZON.CO.UK*D54RH4I65  AMAZON.CO.UK     Amount: -40.78\n",
      "\n",
      "Cluster 3.0 (Total amount: -109.98000000000002):\n",
      "  GREGGS HEATHROW NORTH   HAYES            Amount: -9.4\n",
      "  EUG00840 HEATHROW NORTH Hayes            Amount: -100.58000000000001\n",
      "\n",
      "Cluster 4.0 (Total amount: -35.940000000000005):\n",
      "  MICROSOFT*MICROSOFT 365 MSBILL.INFO      Amount: -23.96\n",
      "  MICROSOFT               MSBILL.INFO      Amount: -11.98\n"
     ]
    }
   ],
   "source": [
    "\"\"\" fuzzy matching clustering\"\"\"\n",
    "from thefuzz import fuzz\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def calculate_similarity(str1, str2):\n",
    "    # Split strings into merchant name and location\n",
    "    parts1 = str1.strip().split()\n",
    "    parts2 = str2.strip().split()\n",
    "    \n",
    "    # Get the main merchant part (before any location)\n",
    "    merchant1 = ' '.join([p for p in parts1 if 'LONDON' not in p and 'CO.UK' not in p])\n",
    "    merchant2 = ' '.join([p for p in parts2 if 'LONDON' not in p and 'CO.UK' not in p])\n",
    "    \n",
    "    # Calculate different similarity metrics\n",
    "    ratio = fuzz.ratio(merchant1, merchant2)\n",
    "    token_sort_ratio = fuzz.token_sort_ratio(merchant1, merchant2)\n",
    "    token_set_ratio = fuzz.token_set_ratio(merchant1, merchant2)\n",
    "    \n",
    "    # Check if they share common prefixes\n",
    "    prefix_match = merchant1.split('*')[0] == merchant2.split('*')[0]\n",
    "    prefix_bonus = 20 if prefix_match else 0\n",
    "    \n",
    "    # Calculate weighted similarity\n",
    "    similarity = (ratio + token_sort_ratio + token_set_ratio) / 3 + prefix_bonus\n",
    "    \n",
    "    # Location penalty - if one has LONDON and other doesn't, reduce similarity\n",
    "    if ('LONDON' in str1) != ('LONDON' in str2):\n",
    "        similarity *= 0.5\n",
    "    \n",
    "    return max(0, min(100, similarity))  # Ensure similarity is between 0 and 100\n",
    "\n",
    "def create_distance_matrix(merchant_names):\n",
    "    n = len(merchant_names)\n",
    "    distance_matrix = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                distance_matrix[i, j] = 0\n",
    "            else:\n",
    "                similarity = calculate_similarity(merchant_names[i], merchant_names[j])\n",
    "                # Convert similarity to distance (ensure non-negative)\n",
    "                distance_matrix[i, j] = max(0, (100 - similarity) / 100)\n",
    "    \n",
    "    return distance_matrix\n",
    "\n",
    "def cluster_similar_merchants(df, eps=0.3, min_samples=2):\n",
    "    # Get unique merchant names\n",
    "    merchant_names = df['remittance_info'].unique()\n",
    "    \n",
    "    # Create distance matrix\n",
    "    distances = create_distance_matrix(merchant_names)\n",
    "    \n",
    "    # Perform DBSCAN clustering\n",
    "    clustering = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')\n",
    "    clusters = clustering.fit_predict(distances)\n",
    "    \n",
    "    # Create mapping of merchants to their cluster\n",
    "    merchant_clusters = {}\n",
    "    for merchant, cluster_id in zip(merchant_names, clusters):\n",
    "        if cluster_id != -1:  # -1 represents noise in DBSCAN\n",
    "            merchant_clusters[merchant] = cluster_id\n",
    "    \n",
    "    # Add cluster labels to original dataframe\n",
    "    df_grouped = df.copy()\n",
    "    df_grouped['cluster'] = df_grouped['remittance_info'].map(merchant_clusters)\n",
    "    \n",
    "    return df_grouped\n",
    "\n",
    "# Apply clustering\n",
    "clustered_df = cluster_similar_merchants(result_df)\n",
    "\n",
    "# Show clusters with their transactions and amounts\n",
    "for cluster in sorted(clustered_df['cluster'].unique()):\n",
    "    if cluster is not None:  # Skip unmatched transactions\n",
    "        cluster_data = clustered_df[clustered_df['cluster'] == cluster]\n",
    "        merchants = cluster_data['remittance_info'].unique()\n",
    "        \n",
    "        if len(merchants) > 1:  # Only show groups with multiple merchants\n",
    "            total_amount = cluster_data['amount'].sum()\n",
    "            print(f\"\\nCluster {cluster} (Total amount: {total_amount}):\")\n",
    "            for merchant in merchants:\n",
    "                amount = cluster_data[cluster_data['remittance_info'] == merchant]['amount'].sum()\n",
    "                print(f\"  {merchant:<40} Amount: {amount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" NTROPY \"\n",
    "import requests\n",
    "\n",
    "ntropy_api_key = os.getenv(\"NTROPY_API_KEY\")\n",
    "if not ntropy_api_key:\n",
    "    raise ValueError(\"NTROPY_API_KEY is not set\")\n",
    "else:\n",
    "    print(f\"NTROPY_API_KEY is set\")\n",
    "\n",
    "\"\"\" CREATE NEW ACCOUNT HOLDER \"\"\"\n",
    "url = \"https://api.ntropy.com/v3/account_holders\"\n",
    "headers = {\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"X-API-KEY\": ntropy_api_key,\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"id\": \"35b927b6-6fda-40aa-93b8-95b47c2b2cad\",\n",
    "    \"type\": \"business\",\n",
    "    \"name\": \"Michael Ali\",\n",
    "    \"website\": \"https://flowon.ai\",\n",
    "    \"industry\": \"ai software\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" NTROPY BATCH PROCESS TRANSACTIONS\"\"\"\n",
    "import uuid\n",
    "\n",
    "url = \"https://api.ntropy.com/v3/batches/\"\n",
    "\n",
    "\n",
    "data = {\n",
    "        \"operation\": \"POST /v3/transactions\",\n",
    "        \"data\": transformed_data\n",
    "    }\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" NOW CHECK TRANSACTION STATUS \"\"\"\n",
    "batch_id = \"1a2bc613-111b-49b1-b35c-77e9b1d7a2fc\"\n",
    "\n",
    "url = f\"https://api.ntropy.com/v3/batches/{batch_id}/results\"\n",
    "\n",
    "\n",
    "get_batch = requests.get(url, headers=headers)\n",
    "\n",
    "get_batch.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_batch.json()['results'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transaction_id = \"1177539c-b570-4588-9953-d76ae4647afb\"\n",
    "\n",
    "url = f\"https://api.ntropy.com/v3/transactions/{transaction_id}\"\n",
    "\n",
    "get_transaction = requests.get(url, headers=headers)\n",
    "\n",
    "get_transaction.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "\"\"\" DF TO JSON FIELDS \"\"\"\n",
    "# Convert DataFrame to JSON\n",
    "def prepare_df_for_frontend(df):\n",
    "    # Reset index to make bookingDate a column\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # Convert datetime to ISO format string\n",
    "    df['bookingDate'] = df['bookingDate'].dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    \n",
    "    # Convert to JSON records format (this gives us a string)\n",
    "    json_string = df.to_json(orient='records', date_format='iso')\n",
    "    \n",
    "    # Parse the JSON string into Python objects (list of dictionaries)\n",
    "    json_data = json.loads(json_string)\n",
    "    \n",
    "    return json_data\n",
    "\n",
    "json_data = prepare_df_for_frontend(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid \n",
    "\"\"\"  \"\"\"\n",
    "def transform_transaction(transaction, account_holder_id):\n",
    "    # Transform a single transaction\n",
    "    return {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"description\": transaction[\"remittanceInfo\"],\n",
    "        \"date\": transaction[\"bookingDate\"].split(\"T\")[0],\n",
    "        \"amount\": abs(transaction[\"amount\"]/100),  # Make amount positive\n",
    "        \"entry_type\": \"outgoing\" if transaction[\"amount\"] < 0 else \"incoming\",\n",
    "        \"currency\": transaction[\"currency\"],\n",
    "        \"account_holder_id\": account_holder_id,\n",
    "        \"location\": {\n",
    "            \"country\": \"GB\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "account_holder_id = \"35b927b6-6fda-40aa-93b8-95b47c2b2cad\"\n",
    "# Transform all transactions using list comprehension\n",
    "transformed_data = [\n",
    "    transform_transaction(transaction, account_holder_id) \n",
    "    for transaction in json_data\n",
    "]\n",
    "\n",
    "transformed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To evaluate reponses of LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.to_csv(\"transactions_ai_reconciled.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
