{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.services.ntropy import NtropyService\n",
    "import asyncio\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# async def process_gocardless_transactions(user_id: str):\n",
    "#     \"\"\"\n",
    "#     Process GoCardless transactions through Ntropy enrichment\n",
    "    \n",
    "#     Args:\n",
    "#         user_id (str): The user ID to process transactions for\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Initialize Ntropy service\n",
    "#         ntropy_service = NtropyService()\n",
    "        \n",
    "#         # Start the enrichment process\n",
    "#         batch_response = await ntropy_service.enrich_transactions(user_id)\n",
    "        \n",
    "#         if not batch_response:\n",
    "#             logger.warning(\"No transactions to process\")\n",
    "#             return\n",
    "            \n",
    "#         batch_id = batch_response.id\n",
    "        \n",
    "#         # Poll for results\n",
    "#         while True:\n",
    "#             status = await ntropy_service.get_batch_status(batch_id)\n",
    "            \n",
    "#             if status[\"status\"] == \"complete\":\n",
    "#                 logger.info(f\"Batch {batch_id} processing completed\")\n",
    "#                 break\n",
    "#             elif status[\"status\"] == \"error\":\n",
    "#                 logger.error(f\"Batch {batch_id} failed: {status.get('error')}\")\n",
    "#                 break\n",
    "            \n",
    "#             # Wait before next poll\n",
    "#             await asyncio.sleep(5)  # Poll every 5 seconds\n",
    "#             logger.info(f\"Progress: {status['progress']}/{status['total']}\")\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error processing transactions: {str(e)}\", exc_info=True)\n",
    "#         raise\n",
    "\n",
    "# await process_gocardless_transactions(\"user_2tEnUq7rivacYtZnsAXPlC5gi9B\")\n",
    "\n",
    "from supabase import create_client\n",
    "import os\n",
    "\n",
    "# Initialize Supabase client\n",
    "supabase_url = os.environ.get(\"SUPABASE_URL\")\n",
    "supabase_key = os.environ.get(\"SUPABASE_KEY\")\n",
    "supabase = create_client(supabase_url, supabase_key)\n",
    "\n",
    "# # Fetch all ntropy transactions\n",
    "# ntropy_transactions = supabase.table(\"ntropy_transactions\").select(\"*\").execute()\n",
    "\n",
    "# # Loop through each ntropy transaction\n",
    "# for ntropy_tx in ntropy_transactions.data:\n",
    "#     ntropy_id = ntropy_tx['ntropy_id']\n",
    "    \n",
    "#     # Extract the required fields from the entities and categories\n",
    "#     counterparty = ntropy_tx['enriched_data']['entities']['counterparty']['name'] if ntropy_tx['enriched_data']['entities'] and ntropy_tx['enriched_data']['entities']['counterparty'] else None\n",
    "#     category = ntropy_tx['enriched_data']['categories'] if ntropy_tx['enriched_data']['categories'] else None\n",
    "    \n",
    "#     # Find and update the corresponding GoCardless transaction\n",
    "#     # Note: ntropy_id is stored with underscore in gocardless_transactions\n",
    "#     gocardless_ntropy_id = ntropy_id\n",
    "    \n",
    "#     try:\n",
    "#         # Update the GoCardless transaction with the new data\n",
    "#         supabase.table(\"gocardless_transactions\").update({\n",
    "#             \"entity\": counterparty,\n",
    "#             \"category\": category\n",
    "#         }).eq(\"id\", gocardless_ntropy_id).execute()\n",
    "        \n",
    "#         print(f\"Updated transaction with ntropy_id: {gocardless_ntropy_id}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error updating transaction {gocardless_ntropy_id}: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions saved to transactions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert transactions list to DataFrame\n",
    "df = pd.DataFrame(tx['transactions']['booked'])\n",
    "\n",
    "# Save to CSV\n",
    "csv_filename = 'transactions.csv'\n",
    "df.to_csv(csv_filename, index=False)\n",
    "print(f\"Transactions saved to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error fetching transactions: 429 Client Error: Too Many Requests for url: https://bankaccountdata.gocardless.com/api/v2/accounts/f9ee4378-ecc9-45c6-aedc-d982a7329072/transactions/\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://bankaccountdata.gocardless.com/api/v2/accounts/f9ee4378-ecc9-45c6-aedc-d982a7329072/transactions/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m supabase \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m get_supabase()\n\u001b[1;32m     11\u001b[0m access_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m get_access_token()\n\u001b[0;32m---> 12\u001b[0m tx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m get_transactions(accounts\u001b[38;5;241m=\u001b[39m[barclays_accounts[\u001b[38;5;241m1\u001b[39m]], access_token\u001b[38;5;241m=\u001b[39maccess_token[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccess\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# for i in tx['transactions']['booked']:\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#     try:\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#         if i.get('bookingDate'):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# response.data\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# tx\u001b[39;00m\n",
      "File \u001b[0;32m~/Repos/bankstream.io/backend/app/services/gocardless.py:333\u001b[0m, in \u001b[0;36mget_transactions\u001b[0;34m(accounts, access_token)\u001b[0m\n\u001b[1;32m    328\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccept\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccess_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m }\n\u001b[1;32m    332\u001b[0m accounts_transactions \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m--> 333\u001b[0m \u001b[43maccounts_transactions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m accounts_transactions: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m accounts_transactions\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# Aggregate transactions from this account\u001b[39;00m\n",
      "File \u001b[0;32m~/Repos/bankstream.io/venv/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://bankaccountdata.gocardless.com/api/v2/accounts/f9ee4378-ecc9-45c6-aedc-d982a7329072/transactions/"
     ]
    }
   ],
   "source": [
    "from app.core.supabase_client import get_supabase\n",
    "from app.services.gocardless import get_transactions\n",
    "from app.services.gocardless import get_access_token\n",
    "\n",
    "barclays_accounts = ['5abbe86b-f321-4271-9db1-19f6fe644262',\n",
    "    'f9ee4378-ecc9-45c6-aedc-d982a7329072']\n",
    "\n",
    "# accounts = ['6612b912-7ff6-4948-8ed6-fbc8c0c6e539']\n",
    "# accounts = ['81451d40-c0fb-4524-9ef0-7b36f7b2cc50'] ## natwest crescent advisors \n",
    "supabase = await get_supabase()\n",
    "access_token = await get_access_token()\n",
    "tx = await get_transactions(accounts=[barclays_accounts[1]], access_token=access_token['access'])\n",
    "\n",
    "# for i in tx['transactions']['booked']:\n",
    "#     try:\n",
    "#         if i.get('bookingDate'):\n",
    "#             entity = i[\"creditorName\"] if float(i['transactionAmount']['amount']) < 0 else i[\"debtorName\"]\n",
    "#             print(entity)\n",
    "#             await supabase.table(\"gocardless_transactions\")\\\n",
    "#                 .update({\"entity\": entity})\\\n",
    "#                 .eq(\"transaction_id\", i[\"transactionId\"])\\\n",
    "#                 .execute()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error updating transaction {i['transactionId']}: {str(e)}\")\n",
    "#         continue\n",
    "\n",
    "\n",
    "# response = await supabase.table(\"gocardless_transactions\").select(\"*\").execute()\n",
    "\n",
    "\n",
    "# response.data\n",
    "# tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.app.core.supabase_client import get_supabase\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "supabase = await get_supabase()\n",
    "\n",
    "# Get all transactions\n",
    "logger.info(\"Fetching all transactions...\")\n",
    "response = await supabase.table(\"gocardless_transactions\").select(\"*\").execute()\n",
    "logger.info(f\"Found {len(response.data)} transactions to update\")\n",
    "\n",
    "# Update chart_of_accounts to None for each transaction\n",
    "column_to_clear = \"chart_of_accounts\"\n",
    "for transaction in response.data:\n",
    "    logger.info(f\"Updating transaction {transaction['id']}\")\n",
    "    await supabase.table(\"gocardless_transactions\")\\\n",
    "        .update({column_to_clear: None})\\\n",
    "        .eq(\"id\", transaction[\"id\"])\\\n",
    "        .execute()\n",
    "\n",
    "# Get updated data to verify\n",
    "logger.info(\"Fetching updated transactions...\")\n",
    "response = await supabase.table(\"gocardless_transactions\").select(\"*\").execute()\n",
    "logger.info(\"Update complete\")\n",
    "\n",
    "# Display the results\n",
    "response.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simi Search with CoA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" embedding invoices table \"\"\"\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "\n",
    "from app.services.etl.vectorise_data import get_embedding, JinaTask, kb_item_to_chunks\n",
    "from backend.app.core.supabase_client import get_supabase\n",
    "\n",
    "supabase = await get_supabase()\n",
    "\n",
    "response = await supabase.table('invoices').select('*').execute()\n",
    "\n",
    "for i in response.data:\n",
    "    print(i['results'])\n",
    "    results_str = json.dumps(i['results'])\n",
    "    await kb_item_to_chunks(data_id=i['id'], \n",
    "                            data_content=results_str, \n",
    "                            user_id='ALL', \n",
    "                            title=i['results']['invoice_from']['name'],\n",
    "                            source_table='invoices',\n",
    "                            is_tabular=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import bank data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandasai as pai\n",
    "import pandas as pd\n",
    "import ast\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "pai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not pai_api_key:\n",
    "    raise ValueError(\"PAI_API_KEY is not set\")\n",
    "else:\n",
    "    print(f\"OPENAI_API_KEY is set\")\n",
    "\n",
    "df = pd.read_csv(\"bank_data_23-25/barclays_072.csv\")\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pai.DataFrame(df)\n",
    "\n",
    "pai.api_key.set(pai_api_key)\n",
    "\n",
    "\n",
    "### extract currency and amount\n",
    "df['currency'] = df['transactionAmount'].apply(lambda x: ast.literal_eval(x)['currency'])\n",
    "df['amount'] = df['transactionAmount'].apply(lambda x: float(ast.literal_eval(x)['amount']))\n",
    "df['amount'] = (df['amount'] * 100).astype(int)\n",
    "df = df.drop('transactionAmount', axis=1)\n",
    "\n",
    "## setting datetimeindex\n",
    "df['bookingDate'] = pd.to_datetime(df['bookingDate'])\n",
    "df.set_index('bookingDate', inplace=True)\n",
    "df = df.drop(['valueDate', 'bookingDateTime', 'valueDateTime', 'internalTransactionId'], axis=1)\n",
    "df = df.rename(columns={'remittanceInformationUnstructured': 'remittanceInfo'})\n",
    "\n",
    "# df.chat(\"What are the total expenses for 2024?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Reconciliation Time\n",
    "\n",
    "- Add new columns \"coa_agent\", \"coa_reason\", \"coa_agent_confidence\" for LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from typing import Dict, Tuple\n",
    "import logging\n",
    "import json\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the ChatGroq model\n",
    "groq_model = ChatGroq(model_name=\"Llama-3.3-70b-Specdec\")\n",
    "openai_model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "\n",
    "class TransactionClassifier:\n",
    "    def __init__(self):\n",
    "        logger.info(\"Initializing TransactionClassifier\")\n",
    "        self.system_prompt = \"\"\"\n",
    "        You are a financial expert responsible for classifying transactions into appropriate chart of accounts.\n",
    "        For each transaction, you must provide:\n",
    "        1. The most specific appropriate chart of account\n",
    "        2. A brief explanation of why this classification was chosen. If confidence score is low, explain why.\n",
    "        3. A confidence score between 0 and 1 (e.g., 0.95 for high confidence)\n",
    "\n",
    "        You must respond with valid JSON in the following format only:\n",
    "        {\n",
    "            \"account\": \"string\",\n",
    "            \"reasoning\": \"string\",\n",
    "            \"confidence\": float\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.last_request_time = 0\n",
    "        self.rate_limit_delay = 2  # 2 seconds between requests (30 requests/minute)\n",
    "\n",
    "    def _rate_limit(self):\n",
    "        \"\"\"Implement rate limiting\"\"\"\n",
    "        current_time = time.time()\n",
    "        time_since_last_request = current_time - self.last_request_time\n",
    "        if time_since_last_request < self.rate_limit_delay:\n",
    "            delay = self.rate_limit_delay - time_since_last_request\n",
    "            logger.debug(f\"Rate limiting: waiting {delay:.2f} seconds\")\n",
    "            time.sleep(delay)\n",
    "        self.last_request_time = time.time() \n",
    "\n",
    "    def classify_transaction(self, transaction: Dict, chart_of_accounts: list) -> Tuple[str, str, float]:\n",
    "        \"\"\"Classify a single transaction using the LLM\"\"\"\n",
    "        logger.info(f\"Processing transaction: {transaction}\")\n",
    "        self._rate_limit()\n",
    "\n",
    "        # Format the transaction details for the LLM\n",
    "        transaction_prompt = f\"\"\"\n",
    "        Please classify the following transaction:\n",
    "        Transaction: {transaction}\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=transaction_prompt)\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            logger.debug(\"Sending request to LLM\")\n",
    "            # Add more visible print statements\n",
    "            print(\"\\n=== Sending request to LLM ===\")\n",
    "            response = openai_model.invoke(messages)\n",
    "            print(\"\\n=== Raw LLM Response ===\")\n",
    "            print(response.content)\n",
    "            logger.debug(f\"Raw LLM response: {response.content}\")\n",
    "\n",
    "            # Try to parse the response as JSON\n",
    "            try:\n",
    "                # First, try direct JSON parsing\n",
    "                result = json.loads(response.content)\n",
    "                logger.info(\"Successfully parsed JSON response\")\n",
    "            except json.JSONDecodeError:\n",
    "                # If direct parsing fails, try to extract JSON from the response\n",
    "                logger.warning(\"Direct JSON parsing failed, attempting to extract JSON from response\")\n",
    "                # Look for JSON-like structure in the response\n",
    "                import re\n",
    "                json_match = re.search(r'\\{.*\\}', response.content, re.DOTALL)\n",
    "                if json_match:\n",
    "                    result = json.loads(json_match.group())\n",
    "                    logger.info(\"Successfully extracted and parsed JSON from response\")\n",
    "                else:\n",
    "                    raise ValueError(\"No JSON structure found in response\")\n",
    "\n",
    "            # Validate the response structure\n",
    "            required_keys = {'account', 'reasoning', 'confidence'}\n",
    "            if not all(key in result for key in required_keys):\n",
    "                missing_keys = required_keys - result.keys()\n",
    "                raise ValueError(f\"Missing required keys in response: {missing_keys}\")\n",
    "\n",
    "            logger.info(f\"Classification successful: {result['account']}\")\n",
    "            return (\n",
    "                result['account'],\n",
    "                result['reasoning'],\n",
    "                result['confidence']\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Classification failed: {str(e)}\", exc_info=True)\n",
    "            return (\n",
    "                \"ERROR\",\n",
    "                f\"Classification failed: {str(e)}\",\n",
    "                0.0\n",
    "            )\n",
    "\n",
    "\n",
    "def process_transactions(df: pd.DataFrame, chart_of_accounts: list) -> pd.DataFrame:\n",
    "    \"\"\"Process transactions in DataFrame and add classifications directly\"\"\"\n",
    "    logger.info(f\"Starting to process {len(df)} transactions\")\n",
    "    classifier = TransactionClassifier()\n",
    "\n",
    "    # Create a fresh copy of the DataFrame\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Explicitly clear any previous results\n",
    "    df['coa_agent'] = None\n",
    "    df['coa_reason'] = None\n",
    "    df['coa_confidence'] = None\n",
    "\n",
    "    for i, (idx, row) in enumerate(df.iterrows()):\n",
    "        print(\"row\", row)\n",
    "        logger.info(f\"Processing transaction {idx}\")\n",
    "        transaction = row.to_dict()\n",
    "        transaction['amount'] = transaction['amount']/100\n",
    "        account, reasoning, confidence = classifier.classify_transaction(transaction, chart_of_accounts)\n",
    "        \n",
    "        # Update DataFrame using iloc instead of at\n",
    "        df.iloc[i, df.columns.get_loc('coa_agent')] = account\n",
    "        df.iloc[i, df.columns.get_loc('coa_reason')] = reasoning\n",
    "        df.iloc[i, df.columns.get_loc('coa_confidence')] = confidence\n",
    "        \n",
    "        # Print summary of classification\n",
    "        reason_preview = ' '.join(reasoning.split()[:10]) + '...' if reasoning else 'No reason provided'\n",
    "        logger.info(f\"Transaction {idx}:\")\n",
    "        logger.info(f\"Account: {account}\")\n",
    "        logger.info(f\"Reason Preview: {reason_preview}\")\n",
    "        logger.info(\"-\" * 50)\n",
    "\n",
    "    \n",
    "    # Add verification logging\n",
    "    processed_count = df[df['coa_agent'].notna()].shape[0]\n",
    "    logger.info(f\"Number of processed transactions: {processed_count}\")\n",
    "    \n",
    "    logger.info(\"Finished processing all transactions\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\"\"\" Fetch and store chart of accounts into parsed_accounts \"\"\"\n",
    "import json\n",
    "\n",
    "# Read the JSON file\n",
    "with open('coa.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract only the required fields from each account\n",
    "parsed_accounts = []\n",
    "for account in data['Accounts']:\n",
    "    parsed_account = {\n",
    "        'code': account.get('AccountID', ''),\n",
    "        'name': account.get('Name', ''),\n",
    "        'status': account.get('Status', ''),\n",
    "        'type': account.get('Type', ''),\n",
    "        'taxtype': account.get('TaxType', ''),\n",
    "        'description': account.get('Description', ''),  # Note: Not present in sample but included as requested\n",
    "        'class': account.get('Class', ''),\n",
    "        'reportingcode': account.get('ReportingCode', '')\n",
    "    }\n",
    "    parsed_accounts.append(parsed_account)\n",
    "\n",
    "transactions = []  # Create empty list to store dictionaries\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    transaction = row.to_dict()\n",
    "    transactions.append(transaction)\n",
    "\n",
    "df_processed = process_transactions(df.copy(), parsed_accounts)\n",
    "\n",
    "df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "feeding per line of transaction\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ntropy_sdk import SDK\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ntropy_api_key = os.getenv(\"NTROPY_API_KEY\")\n",
    "\n",
    "sdk = SDK(ntropy_api_key)\n",
    "\n",
    "data = [{\n",
    "    \"id\": \"4yp49x3tbj9mD8DB4fM8DDY6Yxbx8YP14g565Xketw3tFmn\",\n",
    "    \"description\": \"AMAZON WEB SERVICES\",\n",
    "    \"entry_type\": \"outgoing\",\n",
    "    \"amount\": 12042.37,\n",
    "    \"currency\": \"USD\",\n",
    "    \"date\": \"2021-11-01\",\n",
    "    \"account_holder_id\": \"35b927b6-6fda-40aa-93b8-95b47c2b2cad\",  # Add this required field\n",
    "    \"location\": {\n",
    "        \"country\": \"US\"\n",
    "    }\n",
    "}]\n",
    "\n",
    "r = sdk.batches.create(\n",
    "    operation=\"POST /v3/transactions\",\n",
    "    data=data\n",
    ")\n",
    "\n",
    "\n",
    "vars(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(vars(sdk.batches.results(id=r.id))['results'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(sdk.batches.get(id=r.id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdk.batches.create(\n",
    "    operation=\"POST /v3/transactions\",\n",
    "    data=transactions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" NTROPY \"\n",
    "import requests\n",
    "\n",
    "ntropy_api_key = os.getenv(\"NTROPY_API_KEY\")\n",
    "if not ntropy_api_key:\n",
    "    raise ValueError(\"NTROPY_API_KEY is not set\")\n",
    "else:\n",
    "    print(f\"NTROPY_API_KEY is set\")\n",
    "\n",
    "\"\"\" CREATE NEW ACCOUNT HOLDER \"\"\"\n",
    "url = \"https://api.ntropy.com/v3/account_holders\"\n",
    "headers = {\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"X-API-KEY\": ntropy_api_key,\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"id\": \"35b927b6-6fda-40aa-93b8-95b47c2b2cad\",\n",
    "    \"type\": \"business\",\n",
    "    \"name\": \"Michael Ali\",\n",
    "    \"website\": \"https://flowon.ai\",\n",
    "    \"industry\": \"ai software\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" NTROPY BATCH PROCESS TRANSACTIONS\"\"\"\n",
    "import uuid\n",
    "\n",
    "url = \"https://api.ntropy.com/v3/batches/\"\n",
    "\n",
    "\n",
    "data = {\n",
    "        \"operation\": \"POST /v3/transactions\",\n",
    "        \"data\": transformed_data\n",
    "    }\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" NOW CHECK TRANSACTION STATUS \"\"\"\n",
    "batch_id = \"1a2bc613-111b-49b1-b35c-77e9b1d7a2fc\"\n",
    "\n",
    "url = f\"https://api.ntropy.com/v3/batches/{batch_id}/results\"\n",
    "\n",
    "\n",
    "get_batch = requests.get(url, headers=headers)\n",
    "\n",
    "get_batch.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_batch.json()['results'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transaction_id = \"1177539c-b570-4588-9953-d76ae4647afb\"\n",
    "\n",
    "url = f\"https://api.ntropy.com/v3/transactions/{transaction_id}\"\n",
    "\n",
    "get_transaction = requests.get(url, headers=headers)\n",
    "\n",
    "get_transaction.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "\"\"\" DF TO JSON FIELDS \"\"\"\n",
    "# Convert DataFrame to JSON\n",
    "def prepare_df_for_frontend(df):\n",
    "    # Reset index to make bookingDate a column\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # Convert datetime to ISO format string\n",
    "    df['bookingDate'] = df['bookingDate'].dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    \n",
    "    # Convert to JSON records format (this gives us a string)\n",
    "    json_string = df.to_json(orient='records', date_format='iso')\n",
    "    \n",
    "    # Parse the JSON string into Python objects (list of dictionaries)\n",
    "    json_data = json.loads(json_string)\n",
    "    \n",
    "    return json_data\n",
    "\n",
    "json_data = prepare_df_for_frontend(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid \n",
    "\"\"\"  \"\"\"\n",
    "def transform_transaction(transaction, account_holder_id):\n",
    "    # Transform a single transaction\n",
    "    return {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"description\": transaction[\"remittanceInfo\"],\n",
    "        \"date\": transaction[\"bookingDate\"].split(\"T\")[0],\n",
    "        \"amount\": abs(transaction[\"amount\"]/100),  # Make amount positive\n",
    "        \"entry_type\": \"outgoing\" if transaction[\"amount\"] < 0 else \"incoming\",\n",
    "        \"currency\": transaction[\"currency\"],\n",
    "        \"account_holder_id\": account_holder_id,\n",
    "        \"location\": {\n",
    "            \"country\": \"GB\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "account_holder_id = \"35b927b6-6fda-40aa-93b8-95b47c2b2cad\"\n",
    "# Transform all transactions using list comprehension\n",
    "transformed_data = [\n",
    "    transform_transaction(transaction, account_holder_id) \n",
    "    for transaction in json_data\n",
    "]\n",
    "\n",
    "transformed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To evaluate reponses of LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.to_csv(\"transactions_ai_reconciled.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
